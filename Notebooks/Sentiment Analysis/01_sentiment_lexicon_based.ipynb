{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3187,
     "status": "ok",
     "timestamp": 1743000348879,
     "user": {
      "displayName": "Caleb Ong",
      "userId": "09148710192798478088"
     },
     "user_tz": -480
    },
    "id": "b_d8DYUUXUug",
    "outputId": "d51a9e26-d485-4827-c13e-a21a57abd2f4"
   },
   "outputs": [],
   "source": [
    "# Block 1: Import Libraries\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, mean_squared_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 394,
     "status": "ok",
     "timestamp": 1743000349271,
     "user": {
      "displayName": "Caleb Ong",
      "userId": "09148710192798478088"
     },
     "user_tz": -480
    },
    "id": "lRhUOgMZXVDu",
    "outputId": "c36cdb92-944a-415c-e5a9-46ce41591630"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: ../../\n",
      "Looking for lexicon data in: ../../Data\\Historical Reddit\\Lexicon_Data\n",
      "Using golden dataset from: ../../Data\\Historical Reddit\\golden_dataset_sentiment.csv\n",
      "Saving outputs to: ../../outputs\\sentiment_analysis\\lexicon_based\n"
     ]
    }
   ],
   "source": [
    "# Block 2: Define Paths for Data and Output\n",
    "\n",
    "# Define the root path of your project on Google Drive\n",
    "project_root = '../../'\n",
    "\n",
    "# Define the path to your processed lexicon data\n",
    "processed_dir = os.path.join(project_root, 'Data', 'Historical Reddit', 'Lexicon_Data')\n",
    "\n",
    "# Define the path to the golden dataset\n",
    "golden_path = os.path.join(project_root, 'Data', 'Historical Reddit', 'golden_dataset_sentiment.csv')\n",
    "\n",
    "# Define the base output directory structure (this seems fine)\n",
    "base_output_dir = os.path.join(project_root, 'outputs', 'sentiment_analysis', 'lexicon_based')\n",
    "results_output_dir = os.path.join(base_output_dir, 'results') # For per-subreddit CSVs\n",
    "# evaluation_output_dir = os.path.join(base_output_dir, 'evaluations') # Optional: If you save evaluations later\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Looking for lexicon data in: {processed_dir}\")\n",
    "print(f\"Using golden dataset from: {golden_path}\")\n",
    "print(f\"Saving outputs to: {base_output_dir}\")\n",
    "\n",
    "# Create all necessary output directories if they don't exist\n",
    "os.makedirs(results_output_dir, exist_ok=True)\n",
    "# os.makedirs(evaluation_output_dir, exist_ok=True) # Uncomment if needed later\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1743000349280,
     "user": {
      "displayName": "Caleb Ong",
      "userId": "09148710192798478088"
     },
     "user_tz": -480
    },
    "id": "D7i5jeHXXW_1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of negative words: 2369\n",
      "Number of positive words: 370\n"
     ]
    }
   ],
   "source": [
    "# Block 3: Load the Loughran-McDonald Master Dictionary and Prepare Lexicons\n",
    "lexicon_path = os.path.join(project_root, 'Documents', 'Dictionaries', 'Loughran-McDonald_MasterDictionary_1993-2024.csv')\n",
    "lexicon_df = pd.read_csv(lexicon_path)\n",
    "\n",
    "# Create binary flags for sentiment (flag as 1 if count > 0)\n",
    "lexicon_df['Negative_flag'] = lexicon_df['Negative'].apply(lambda x: 1 if pd.notnull(x) and float(x) > 0 else 0)\n",
    "lexicon_df['Positive_flag'] = lexicon_df['Positive'].apply(lambda x: 1 if pd.notnull(x) and float(x) > 0 else 0)\n",
    "\n",
    "# Build sets of negative and positive words (lowercase for consistency)\n",
    "negative_words = set(lexicon_df.loc[lexicon_df['Negative_flag'] == 1, 'Word'].str.lower())\n",
    "positive_words = set(lexicon_df.loc[lexicon_df['Positive_flag'] == 1, 'Word'].str.lower())\n",
    "\n",
    "postive_jargons = [\n",
    "    # ðŸš€ Bullish Sentiment / Positive\n",
    "    \"diamond_hands\", \"hodl\", \"hodler\", \"wagmi\", \"ape_in\", \"btfd\", \"buy_the_dip\",\n",
    "    \"go_long\", \"long_it\", \"going_long\", \"bull_run\", \"bullish\", \"long_call\",\n",
    "    \"long_position\", \"long_stock\", \"to_the_moon\", \"moonshot\", \"ath\", \"stonks\",\n",
    "    \"dca\", \"flippening\", \"wen_lambo\", \"bear_trap\"\n",
    "]\n",
    "\n",
    "negative_jargons = [\n",
    "    # ðŸ˜¬ Bearish Sentiment / Negative\n",
    "    \"paper_hands\", \"bagholder\", \"ngmi\", \"rekt\", \"gn\", \"bearish\", \"long_put\",\n",
    "    \"exit_scam\", \"shill\", \"rug_pull\", \"pump_and_dump\", \"margin_call\", \"liquidated\",\n",
    "    \"overleveraged\", \"capitulate\", \"bull_trap\", \"short_position\", \"short_sell\",\n",
    "    \"shorting\", \"shorts\", \"shorted\", \"short_interest\", \"short_call\", \"short_it\",\n",
    "    \"short_selling\"\n",
    "]\n",
    "\n",
    "positive_words.update(postive_jargons)\n",
    "negative_words.update(negative_jargons)\n",
    "\n",
    "print(\"Number of negative words:\", len(negative_words))\n",
    "print(\"Number of positive words:\", len(positive_words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1743000349293,
     "user": {
      "displayName": "Caleb Ong",
      "userId": "09148710192798478088"
     },
     "user_tz": -480
    },
    "id": "NsHhhwETXYN9"
   },
   "outputs": [],
   "source": [
    "# Block 4: Define Functions for Lexicon-Based Sentiment Analysis with Calibration\n",
    "\n",
    "negation_words = {\"not\", \"no\", \"never\", \"without\", \"against\", \"cannot\", \"don't\", \"isn't\", \"wasn't\", \"shouldn't\", \"wouldn't\", \"couldn't\", \"won't\", \"shan't\", \"haven't\", \"hasn't\", \"hadn't\", \"doesn't\", \"didn't\"}\n",
    "\n",
    "def raw_lexicon_score(tokens, pos_set, neg_set):\n",
    "    pos_count = sum(1 for token in tokens if token.lower() in pos_set)\n",
    "    neg_count = sum(1 for token in tokens if token.lower() in neg_set)\n",
    "    return pos_count - neg_count\n",
    "\n",
    "def enhanced_raw_lexicon_score(tokens, pos_set, neg_set, negation_words):\n",
    "    \"\"\"\n",
    "    Calculates a raw sentiment score, checking for negation words\n",
    "    within a small window (up to 3 words) preceding a sentiment word.\n",
    "    \"\"\"\n",
    "    score = 0\n",
    "    negation_window = 3 # How many words back to check for negation\n",
    "\n",
    "    for i, token in enumerate(tokens):\n",
    "        lower_token = token.lower()\n",
    "        is_negation_word_present = False\n",
    "\n",
    "        # Check the window before the current token for negation words\n",
    "        # Start checking from index i-1 down to max(0, i-negation_window)\n",
    "        for j in range(max(0, i - negation_window), i):\n",
    "            if tokens[j].lower() in negation_words:\n",
    "                is_negation_word_present = not is_negation_word_present # Flip negation state for each negation word found (simple double negative handling)\n",
    "\n",
    "        # Calculate score based on current token and negation status\n",
    "        if lower_token in pos_set:\n",
    "            score += -1 if is_negation_word_present else 1\n",
    "        elif lower_token in neg_set:\n",
    "            score += 1 if is_negation_word_present else -1 # Flip score if negated\n",
    "\n",
    "        # Note: This version doesn't explicitly track consecutive negation words\n",
    "        # or complex grammatical structures, but expands the negation check window.\n",
    "\n",
    "    return score\n",
    "\n",
    "def lexicon_sentiment(tokens, pos_set, neg_set, negation_words, min_score, max_score):\n",
    "        # Get the raw sentiment score using the enhanced function.\n",
    "        raw_score = enhanced_raw_lexicon_score(tokens, pos_set, neg_set, negation_words)\n",
    "\n",
    "        # Clamp the raw score within the provided calibration range.\n",
    "        clamped_score = max(min(raw_score, max_score), min_score)\n",
    "\n",
    "        # Handle the case where min_score equals max_score to avoid division by zero\n",
    "        if max_score == min_score:\n",
    "             normalized = 0.5 # Assign neutral value or handle as appropriate\n",
    "        else:\n",
    "             normalized = (clamped_score - min_score) / (max_score - min_score)\n",
    "\n",
    "        # Map the normalized value to a 1-5 rating.\n",
    "        rating = round(normalized * 4) + 1\n",
    "        return rating, raw_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 93911,
     "status": "ok",
     "timestamp": 1743000443205,
     "user": {
      "displayName": "Caleb Ong",
      "userId": "09148710192798478088"
     },
     "user_tz": -480
    },
    "id": "34XEUYmYXaEc",
    "outputId": "c20a87af-0e4a-4aee-e620-420e748d1e65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best lower percentile: 7%\n",
      "Best upper percentile: 95%\n",
      "Best weighted F1 Score: 0.3234\n",
      "Calibration Parameters:\n",
      "Min Score (calibrated): -6.0000\n",
      "Max Score (calibrated): 5.0000\n",
      "\n",
      "Lexicon-Based Sentiment Analysis Evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.21      0.31      0.25      3613\n",
      "           2       0.45      0.26      0.33     14914\n",
      "           3       0.25      0.46      0.32      9968\n",
      "           4       0.40      0.33      0.36     15114\n",
      "           5       0.30      0.21      0.25      5566\n",
      "\n",
      "    accuracy                           0.32     49175\n",
      "   macro avg       0.32      0.32      0.30     49175\n",
      "weighted avg       0.36      0.32      0.32     49175\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1117 1092 1019  341   44]\n",
      " [2495 3927 5730 2370  392]\n",
      " [ 768 1516 4608 2622  454]\n",
      " [ 895 1824 5556 4980 1859]\n",
      " [ 155  419 1635 2172 1185]]\n",
      "\n",
      "Mean Squared Error: 1.5643924758515506\n"
     ]
    }
   ],
   "source": [
    "# Block 5: Evaluation on Golden Dataset and Tuning Calibration Hyperparameters\n",
    "\n",
    "# Load the golden dataset with 5-class sentiment labels\n",
    "golden = pd.read_csv(golden_path)\n",
    "\n",
    "# Load and combine all relevant processed JSON files\n",
    "dfs = []\n",
    "for fname in os.listdir(processed_dir):\n",
    "    if fname.startswith('lexicon_r_') and fname.endswith('.json'):\n",
    "        file_path = os.path.join(processed_dir, fname)\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                content = json.load(f)\n",
    "                if isinstance(content, list) and len(content) > 0:\n",
    "                    df = pd.DataFrame(content)\n",
    "                    if 'id' in df.columns and 'processed_tokens_lexicon' in df.columns:\n",
    "                        dfs.append(df)\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "if not dfs:\n",
    "    raise ValueError(\"No valid processed dataframes found.\")\n",
    "\n",
    "all_posts = pd.concat(dfs, ignore_index=True)\n",
    "all_posts['id'] = all_posts['id'].astype(str)\n",
    "golden['id'] = golden['id'].astype(str)\n",
    "\n",
    "# Merge with golden sentiment labels\n",
    "data = all_posts.merge(golden[['id', 'sentiment']], on='id', how='inner')\n",
    "data['processed_tokens_lexicon'] = data['processed_tokens_lexicon'].apply(lambda x: x if isinstance(x, list) else [])\n",
    "\n",
    "# Compute raw lexicon score using enhanced scoring function\n",
    "data['raw_score'] = data['processed_tokens_lexicon'].apply(\n",
    "    lambda toks: enhanced_raw_lexicon_score(toks, positive_words, negative_words, negation_words)\n",
    ")\n",
    "\n",
    "def tune_calibration_percentiles(data, pos_set, neg_set, negation_words, lowers, uppers):\n",
    "    \"\"\"\n",
    "    Grid search to find the best lower and upper percentiles for calibration.\n",
    "    Returns best_lower, best_upper, best_f1.\n",
    "    \"\"\"\n",
    "    best_f1 = -1\n",
    "    best_lower = best_upper = None\n",
    "\n",
    "    for lower in lowers:\n",
    "        for upper in uppers:\n",
    "            if lower >= upper:\n",
    "                continue\n",
    "            min_val = data['raw_score'].quantile(lower / 100)\n",
    "            max_val = data['raw_score'].quantile(upper / 100)\n",
    "\n",
    "            preds = data['processed_tokens_lexicon'].apply(\n",
    "                lambda toks: lexicon_sentiment(toks, pos_set, neg_set, negation_words, min_val, max_val)[0]\n",
    "            )\n",
    "\n",
    "            f1 = f1_score(data['sentiment'], preds, average='weighted')\n",
    "            if f1 > best_f1:\n",
    "                best_f1, best_lower, best_upper = f1, lower, upper\n",
    "\n",
    "    return best_lower, best_upper, best_f1\n",
    "\n",
    "candidate_lowers = [1, 3, 5, 7, 10]\n",
    "candidate_uppers = [90, 93, 95, 97, 99]\n",
    "\n",
    "best_lower, best_upper, best_f1 = tune_calibration_percentiles(\n",
    "    data, positive_words, negative_words, negation_words, candidate_lowers, candidate_uppers\n",
    ")\n",
    "\n",
    "if best_lower is None or best_upper is None:\n",
    "    print(\"Tuning failed. Using fallback percentiles: 5% and 95%\")\n",
    "    best_lower, best_upper = 5, 95\n",
    "    min_val = data['raw_score'].quantile(best_lower / 100)\n",
    "    max_val = data['raw_score'].quantile(best_upper / 100)\n",
    "    preds = data['processed_tokens_lexicon'].apply(\n",
    "        lambda toks: lexicon_sentiment(toks, positive_words, negative_words, negation_words, min_val, max_val)[0]\n",
    "    )\n",
    "    best_f1 = f1_score(data['sentiment'], preds, average='weighted')\n",
    "\n",
    "print(f\"Best lower percentile: {best_lower}%\")\n",
    "print(f\"Best upper percentile: {best_upper}%\")\n",
    "print(f\"Best weighted F1 Score: {best_f1:.4f}\")\n",
    "\n",
    "min_score_calib = data['raw_score'].quantile(best_lower / 100)\n",
    "max_score_calib = data['raw_score'].quantile(best_upper / 100)\n",
    "print(\"Calibration Parameters:\")\n",
    "print(f\"Min Score (calibrated): {min_score_calib:.4f}\")\n",
    "print(f\"Max Score (calibrated): {max_score_calib:.4f}\")\n",
    "\n",
    "def apply_lexicon_sentiment(row, min_score, max_score):\n",
    "    tokens = row['processed_tokens_lexicon'] if isinstance(row['processed_tokens_lexicon'], list) else []\n",
    "    rating, raw_score = lexicon_sentiment(tokens, positive_words, negative_words, negation_words, min_score, max_score)\n",
    "    return pd.Series({'lex_sentiment': rating, 'lex_raw_score': raw_score})\n",
    "\n",
    "data[['lex_sentiment', 'lex_raw_score']] = data.apply(\n",
    "    lambda row: apply_lexicon_sentiment(row, min_score_calib, max_score_calib), axis=1\n",
    ")\n",
    "\n",
    "# Final evaluation\n",
    "print(\"\\nLexicon-Based Sentiment Analysis Evaluation:\")\n",
    "print(classification_report(data['sentiment'], data['lex_sentiment']))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(data['sentiment'], data['lex_sentiment']))\n",
    "print(\"\\nMean Squared Error:\", mean_squared_error(data['sentiment'], data['lex_sentiment']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 52769,
     "status": "ok",
     "timestamp": 1743000495976,
     "user": {
      "displayName": "Caleb Ong",
      "userId": "09148710192798478088"
     },
     "user_tz": -480
    },
    "id": "EuI3Jq0pXcHn",
    "outputId": "17c8e2c1-3113-4e5f-e874-e24ae4608116"
   },
   "outputs": [],
   "source": [
    "# # Block 6: Process Each JSON File and Save Calibrated Lexicon Sentiment Results\n",
    "\n",
    "# for fname in os.listdir(processed_dir):\n",
    "#     if fname.startswith('processed_r_') and fname.endswith('.json'):\n",
    "#         file_path = os.path.join(processed_dir, fname)\n",
    "#         with open(file_path, 'r', encoding='utf-8') as infile:\n",
    "#             df = pd.DataFrame(json.load(infile))\n",
    "#         if 'processed_tokens_lexicon' not in df.columns:\n",
    "#             continue\n",
    "\n",
    "#         # Apply the calibrated lexicon sentiment function to each row.\n",
    "#         df[['lex_sentiment', 'lex_raw_score']] = df.apply(lambda row: apply_lexicon_sentiment(row, min_score_calib, max_score_calib), axis=1)\n",
    "\n",
    "#         # --- Use the new output directory ---\n",
    "#         out_file = fname.replace('.json', '_lexicon_sentiment.csv')\n",
    "#         out_path = os.path.join(results_output_dir, out_file) # Use results_output_dir defined in Block 3\n",
    "#         df.to_csv(out_path, index=False)\n",
    "#         print(f\"Saved: {out_file} to {results_output_dir}\")\n",
    "\n",
    "# print(\"\\nFinished saving lexicon sentiment results per subreddit.\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
