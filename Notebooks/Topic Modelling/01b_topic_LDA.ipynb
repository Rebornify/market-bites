{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 1: Import Libraries\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "import time\n",
    "\n",
    "# Gensim imports\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.models import LdaMulticore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 2: Define Paths for Data and Output\n",
    "project_root = '../../'\n",
    "processed_dir = os.path.join(project_root, 'Data', 'Historical Reddit', 'LDA_NMF_Data')\n",
    "base_output_dir = os.path.join(project_root, 'outputs', 'topic_modeling', 'lda')\n",
    "model_output_dir = os.path.join(base_output_dir, 'models')\n",
    "evaluation_output_dir = os.path.join(base_output_dir, 'evaluations')\n",
    "viz_output_dir = os.path.join(base_output_dir, 'visualizations', 'wordclouds')\n",
    "# doc_topics_output_dir = os.path.join(base_output_dir, 'document_topics')\n",
    "\n",
    "os.makedirs(model_output_dir, exist_ok=True)\n",
    "os.makedirs(evaluation_output_dir, exist_ok=True)\n",
    "os.makedirs(viz_output_dir, exist_ok=True)\n",
    "# os.makedirs(doc_topics_output_dir, exist_ok=True)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 3: Load and Merge Data from JSON Files\n",
    "dfs = []\n",
    "for fname in os.listdir(processed_dir):\n",
    "    if fname.startswith('lda_nmf_r_') and fname.endswith('.json'):\n",
    "        file_path = os.path.join(processed_dir, fname)\n",
    "        subreddit_name = fname.replace('lda_nmf_r_', '').replace('.json', '')\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            data_json = json.load(f)\n",
    "        df = pd.DataFrame(data_json)\n",
    "        df['subreddit'] = subreddit_name\n",
    "        dfs.append(df)\n",
    "all_posts = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "print(f\"Total number of posts: {len(all_posts)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 4: Data Preparation for Topic Modeling (Gensim Only)\n",
    "print(\"Preparing data for Gensim LDA...\")\n",
    "texts = all_posts['processed_tokens_lda_nmf'].tolist()\n",
    "\n",
    "print(\"Creating Gensim dictionary and corpus...\")\n",
    "dictionary = Dictionary(texts)\n",
    "\n",
    "# Filter extremes:\n",
    "# - Remove tokens that appear in less than 15 documents (no_below)\n",
    "# - Remove tokens that appear in more than 50% of documents (no_above)\n",
    "# - Keep only the top \"None\" most frequent tokens (keep_n)\n",
    "print(f\"Original dictionary size: {len(dictionary)}\")\n",
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=None)\n",
    "print(f\"Filtered dictionary size: {len(dictionary)}\")\n",
    "\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "print(f\"Number of documents in corpus: {len(corpus)}\")\n",
    "print(f\"Number of unique tokens in filtered dictionary: {len(dictionary)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 5: Comprehensive Hyperparameter Tuning for LDA Model\n",
    "\n",
    "# --- Define Hyperparameter Grid ---\n",
    "search_params = {\n",
    "    'num_topics': [10, 15, 20],     # Reduced range for k\n",
    "    'eta': [None, 'auto', 0.1]    # Reduced eta options\n",
    "}\n",
    "num_topics_list = search_params['num_topics']\n",
    "eta_settings = search_params['eta']\n",
    "\n",
    "best_score = -np.inf\n",
    "best_params = {} # Store the best combination found\n",
    "coherence_scores = {} # Store scores for all combinations\n",
    "\n",
    "start_time = time.time()\n",
    "# Use one less than the total number of cores, but at least 1\n",
    "num_workers = os.cpu_count() - 1 if os.cpu_count() and os.cpu_count() > 1 else 1\n",
    "\n",
    "# --- Grid Search ---\n",
    "print(f\"Starting comprehensive hyperparameter tuning with k = {num_topics_list}...\")\n",
    "for k in num_topics_list:\n",
    "    for eta_setting in eta_settings:\n",
    "        # Construct a unique key for logging and parameter tracking\n",
    "        # Use 'symmetric_default' for None eta for clarity in logs\n",
    "        eta_key = eta_setting if eta_setting is not None else 'symmetric_default'\n",
    "        param_key = f\"k={k}, alpha=symmetric, eta={eta_key}\"\n",
    "        print(f\"  Testing {param_key}...\")\n",
    "\n",
    "        try:\n",
    "            # Initialize and train the temporary LDA model\n",
    "            lda_temp = LdaMulticore(\n",
    "                corpus=corpus,\n",
    "                id2word=dictionary,\n",
    "                num_topics=k,\n",
    "                random_state=42, # Ensure reproducibility\n",
    "                chunksize=100,  # Process 100 documents at a time\n",
    "                passes=10,      # Moderate passes for faster tuning\n",
    "                alpha='symmetric', # Set alpha directly\n",
    "                eta=eta_setting, # Note: 'auto' eta IS supported by LdaMulticore\n",
    "                workers=num_workers\n",
    "            )\n",
    "\n",
    "            # Calculate Coherence Score (NPMI)\n",
    "            coherence_model = CoherenceModel(\n",
    "                model=lda_temp,\n",
    "                texts=texts,        # Use the original tokenized texts\n",
    "                dictionary=dictionary,\n",
    "                coherence='c_npmi', # Use NPMI for coherence calculation\n",
    "                processes=num_workers # Use multiple processes if available\n",
    "            )\n",
    "            coherence_score = coherence_model.get_coherence()\n",
    "            coherence_scores[param_key] = coherence_score\n",
    "            print(f\"    NPMI Coherence: {coherence_score:.4f}\")\n",
    "\n",
    "            # Update Best Score and Parameters if current model is better\n",
    "            if coherence_score > best_score:\n",
    "                best_score = coherence_score\n",
    "                # Store the parameters that yielded the best score\n",
    "                best_params = {\n",
    "                    'num_topics': k,\n",
    "                    'alpha': 'symmetric', # Store the fixed alpha value\n",
    "                    'eta': eta_setting # Store the actual eta value (could be None)\n",
    "                }\n",
    "\n",
    "        except Exception as e:\n",
    "             # Log errors for specific parameter combinations\n",
    "             print(f\"    Failed for {param_key}: {e}\")\n",
    "             coherence_scores[param_key] = np.nan # Record failure with NaN\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"\\nTuning finished in {end_time - start_time:.2f} seconds.\")\n",
    "print(f\"\\nAll Coherence Scores:\")\n",
    "# Print scores sorted by coherence value (highest first), handling potential NaNs\n",
    "sorted_scores = sorted(coherence_scores.items(), key=lambda item: item[1] if not np.isnan(item[1]) else -np.inf, reverse=True)\n",
    "for params, score in sorted_scores:\n",
    "    # Format NaN scores appropriately for printing\n",
    "    score_str = f\"{score:.4f}\" if not np.isnan(score) else \"Failed\"\n",
    "    print(f\"  {params}: {score_str}\")\n",
    "\n",
    "\n",
    "# --- Select Best Parameters ---\n",
    "# Check if any run succeeded and found parameters\n",
    "if not best_params:\n",
    "    print(\"\\nError: All tuning runs failed or no valid best parameters found. Using fallback.\")\n",
    "    # Define fallback parameters if all runs fail\n",
    "    best_lda_params = {'num_topics': 20, 'alpha': 'symmetric', 'eta': None}\n",
    "else:\n",
    "    print(f\"\\nBest NPMI Score achieved during tuning: {best_score:.4f}\")\n",
    "    print(f\"Best LDA Parameters found: {best_params}\")\n",
    "    # Use the dictionary containing the best combination found during tuning\n",
    "    best_lda_params = best_params\n",
    "\n",
    "# Ensure the best_lda_params dictionary is ready for Block 6\n",
    "print(f\"\\nParameters selected for final model training in Block 6: {best_lda_params}\")\n",
    "\n",
    "# Note: Block 6 will automatically use 'best_lda_params' when run next.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 6: Train Final LDA Model with Best Parameters\n",
    "\n",
    "print(\"\\nTraining final Gensim LDA model using best parameters found during tuning...\")\n",
    "# Use best_lda_params dictionary determined in Block 5\n",
    "final_lda = LdaMulticore(\n",
    "    corpus=corpus,\n",
    "    id2word=dictionary,\n",
    "    num_topics=best_lda_params['num_topics'], # Get k from best_params\n",
    "    random_state=42,\n",
    "    chunksize=100,\n",
    "    passes=20, # Use more passes for the final model\n",
    "    alpha=best_lda_params['alpha'], # Get alpha from best_params\n",
    "    eta=best_lda_params['eta'],     # Get eta from best_params\n",
    "    workers=num_workers # num_workers should still be defined from Block 5\n",
    ")\n",
    "\n",
    "print(\"\\nFinal Model Configuration:\")\n",
    "print(final_lda)\n",
    "# The script will now proceed automatically to Block 7 (Save Model), Block 8 (Evaluate), etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 7: Save Trained Gensim LDA Model and Dictionary\n",
    "print(\"Saving Gensim LDA model and dictionary...\")\n",
    "\n",
    "model_path = os.path.join(model_output_dir, 'gensim_lda.model')\n",
    "final_lda.save(model_path)\n",
    "\n",
    "dict_path = os.path.join(model_output_dir, 'gensim_dictionary.dict')\n",
    "dictionary.save(dict_path)\n",
    "\n",
    "print(f\"Gensim LDA model saved to {model_path}\")\n",
    "print(f\"Gensim dictionary saved to {dict_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 8: Model Evaluation and Coherence Analysis\n",
    "\n",
    "# Get topics as list of lists of words for diversity calculation\n",
    "topics_for_diversity = final_lda.show_topics(\n",
    "    num_topics=final_lda.num_topics,\n",
    "    num_words=15, # Consistent number of words\n",
    "    formatted=False\n",
    ")\n",
    "all_topic_words = [word for topic_id, topic in topics_for_diversity for word, prob in topic]\n",
    "topic_diversity = len(set(all_topic_words)) / (final_lda.num_topics * 15)\n",
    "\n",
    "\n",
    "print(\"Calculating coherence for the final model...\")\n",
    "# Calculate C_NPMI Coherence\n",
    "coherence_model_npmi = CoherenceModel(\n",
    "    model=final_lda,\n",
    "    texts=texts,\n",
    "    corpus=corpus, # Providing corpus might improve some coherence measures\n",
    "    dictionary=dictionary,\n",
    "    coherence='c_npmi',\n",
    "    processes=num_workers\n",
    ")\n",
    "lda_coherence_npmi = coherence_model_npmi.get_coherence()\n",
    "\n",
    "# Calculate C_V Coherence\n",
    "print(\"Calculating C_V coherence...\")\n",
    "coherence_model_cv = CoherenceModel(\n",
    "    model=final_lda,\n",
    "    texts=texts, # C_V requires the original texts\n",
    "    dictionary=dictionary,\n",
    "    coherence='c_v',\n",
    "    processes=num_workers\n",
    ")\n",
    "lda_coherence_cv = coherence_model_cv.get_coherence()\n",
    "\n",
    "\n",
    "# Store the integer seed value instead of the RandomState object\n",
    "# Assuming the seed was set using np.random.seed(42) and passed as random_state=42\n",
    "# If the model's internal random state was derived differently, this might need adjustment,\n",
    "# but for this code, 42 is the effective seed.\n",
    "used_random_seed = 42 # The seed we specified for reproducibility\n",
    "\n",
    "evaluation_results = {\n",
    "    'model_type': 'Gensim LDA',\n",
    "    'coherence_score_npmi': lda_coherence_npmi, # Renamed for clarity\n",
    "    'coherence_score_cv': lda_coherence_cv,     # Added C_V score\n",
    "    'topic_diversity': topic_diversity,\n",
    "    'n_topics': final_lda.num_topics,\n",
    "    'parameters': {\n",
    "        'num_topics': final_lda.num_topics,\n",
    "        'passes': final_lda.passes,\n",
    "        'chunksize': final_lda.chunksize,\n",
    "        # 'alpha': final_lda.alpha, # Record learned alpha if 'auto' was used\n",
    "        # 'eta': final_lda.eta,   # Record learned eta if 'auto' was used\n",
    "        'random_state_seed': used_random_seed # Store the seed integer\n",
    "    }\n",
    "}\n",
    "\n",
    "eval_path = os.path.join(evaluation_output_dir, 'gensim_lda_evaluation.json')\n",
    "# Saving to JSON should now work\n",
    "try:\n",
    "    with open(eval_path, 'w') as f:\n",
    "        json.dump(evaluation_results, f, indent=4)\n",
    "    print(f\"Evaluation results saved to {eval_path}\")\n",
    "except TypeError as e:\n",
    "    # This block should hopefully not be reached now\n",
    "    print(f\"Error saving evaluation results to JSON: {e}\")\n",
    "    print(\"Evaluation Results Dictionary:\", evaluation_results)\n",
    "\n",
    "\n",
    "print(\"\\nModel Evaluation Results:\")\n",
    "print(f\"NPMI Coherence Score: {lda_coherence_npmi:.4f}\") # Updated variable name\n",
    "print(f\"C_V Coherence Score: {lda_coherence_cv:.4f}\")   # Added C_V output\n",
    "print(f\"Topic Diversity: {topic_diversity:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 9: Function to Display Top Words for each Topic\n",
    "\n",
    "def display_topics(model, n_top_words=15):\n",
    "    \"\"\"Displays topics from a Gensim LDA model and returns them as a 0-indexed dictionary.\"\"\"\n",
    "    shown_topics = model.show_topics(num_topics=model.num_topics, num_words=n_top_words, formatted=False)\n",
    "    topics_dict = {}\n",
    "    print(\"\\nGensim LDA Topics:\")\n",
    "    for topic_id, topic_words_probs in shown_topics:\n",
    "        topic_words = [word for word, prob in topic_words_probs]\n",
    "        topics_dict[topic_id] = topic_words # 0-based index for internal use\n",
    "        print(f\"Topic {topic_id + 1}: {' '.join(topic_words)}\") # Display as 1-based\n",
    "    return topics_dict\n",
    "\n",
    "lda_topics = display_topics(final_lda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 10: Create Word Clouds for Visualization\n",
    "\n",
    "def create_wordcloud(topics_dict, model_name):\n",
    "    \"\"\"Creates and saves a single figure with word clouds for each topic.\"\"\"\n",
    "    num_topics = len(topics_dict)\n",
    "    n_cols = 5 # Adjust grid layout as needed\n",
    "    n_rows = (num_topics + n_cols - 1) // n_cols\n",
    "    plt.figure(figsize=(n_cols * 5, n_rows * 3))\n",
    "\n",
    "    for topic_idx, words in topics_dict.items(): # topic_idx is 0-based\n",
    "        word_freq = {word: 1 for word in words}\n",
    "        wordcloud = WordCloud(\n",
    "            width=400, height=200, background_color='white', max_words=100\n",
    "        ).generate_from_frequencies(word_freq)\n",
    "\n",
    "        plt.subplot(n_rows, n_cols, topic_idx + 1) # subplot requires 1-based index\n",
    "        plt.imshow(wordcloud, interpolation='bilinear')\n",
    "        plt.axis('off')\n",
    "        plt.title(f'{model_name} Topic {topic_idx + 1}') # Display 1-based index\n",
    "\n",
    "    plt.tight_layout(pad=2.0)\n",
    "    out_path_all = os.path.join(viz_output_dir, f'{model_name}_all_topics_wordclouds.png')\n",
    "    plt.savefig(out_path_all, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Saved combined word cloud figure to {out_path_all}\")\n",
    "\n",
    "# Use the 0-indexed dictionary from display_topics\n",
    "create_wordcloud(lda_topics, 'Gensim_LDA')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 11: Assign Topics to Documents\n",
    "\n",
    "print(\"Assigning topics to documents using Gensim LDA model...\")\n",
    "\n",
    "def get_dominant_topic_gensim(doc_topics):\n",
    "    \"\"\"Gets the dominant topic (0-based index) and its probability.\"\"\"\n",
    "    if not doc_topics:\n",
    "        return -1, 0.0 # Indicate no topic assigned above threshold\n",
    "    dominant_topic = max(doc_topics, key=lambda item: item[1])\n",
    "    return dominant_topic[0], dominant_topic[1]\n",
    "\n",
    "# Get topic distribution per document (list of tuples (topic_id, probability))\n",
    "# Use minimum_probability=0.0 to get all assignments, can increase to filter low-prob assignments\n",
    "doc_topic_tuples = [get_dominant_topic_gensim(final_lda.get_document_topics(bow, minimum_probability=0.0)) for bow in corpus]\n",
    "\n",
    "dominant_topics, topic_confidences = zip(*doc_topic_tuples)\n",
    "all_posts['lda_dominant_topic'] = dominant_topics\n",
    "all_posts['lda_topic_confidence'] = topic_confidences\n",
    "\n",
    "# Convert to 1-based index for analysis/display consistency\n",
    "all_posts['lda_dominant_topic'] = all_posts['lda_dominant_topic'] + 1\n",
    "\n",
    "# Optional: Handle docs where no topic was assigned (dominant_topic == -1 -> 0 after +1)\n",
    "# print(f\"Number of documents with no assigned topic: {sum(all_posts['lda_dominant_topic'] == 0)}\")\n",
    "# all_posts.loc[all_posts['lda_dominant_topic'] == 0, ['lda_dominant_topic', 'lda_topic_confidence']] = [np.nan, np.nan]\n",
    "\n",
    "print(\"Sample of assigned topics (1-based index):\")\n",
    "print(all_posts[['lda_dominant_topic', 'lda_topic_confidence']].head(10))\n",
    "\n",
    "# Use nullable integer type for topic index in case of NaNs\n",
    "all_posts['lda_dominant_topic'] = all_posts['lda_dominant_topic'].astype('Int64')\n",
    "\n",
    "print(\"\\nData types after assignment:\")\n",
    "print(all_posts[['lda_dominant_topic', 'lda_topic_confidence']].dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 12: Analyze Topic Distribution by Subreddit\n",
    "print(\"Analyzing topic distribution by subreddit...\")\n",
    "\n",
    "# Use dropna() in case NaNs were introduced in Block 11\n",
    "lda_topic_by_subreddit = pd.crosstab(\n",
    "    all_posts['subreddit'],\n",
    "    all_posts['lda_dominant_topic'].dropna(),\n",
    "    normalize='index'\n",
    ") * 100\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(lda_topic_by_subreddit, annot=True, cmap='YlGnBu', fmt='.1f')\n",
    "plt.title('Gensim LDA Topic Distribution by Subreddit (%)')\n",
    "plt.xlabel('Topic (1-based)')\n",
    "plt.ylabel('Subreddit')\n",
    "plt.savefig(os.path.join(evaluation_output_dir, 'gensim_lda_topic_by_subreddit.png'))\n",
    "plt.close()\n",
    "\n",
    "print(\"Topic distribution analysis by subreddit completed and saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 13: Analyze Overall Topic Distribution\n",
    "print(\"\\nAnalyzing overall topic distribution...\")\n",
    "# Note: Assignment probability/confidence values are model-specific.\n",
    "\n",
    "topic_distribution = all_posts['lda_dominant_topic'].value_counts(normalize=True) * 100\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "topic_distribution.sort_index().plot(kind='bar')\n",
    "plt.title('Gensim LDA: Distribution of Topics Across All Posts')\n",
    "plt.xlabel('Topic Number (1-based)')\n",
    "plt.ylabel('Percentage of Posts (%)')\n",
    "plt.xticks(rotation=0)\n",
    "plt.grid(True, axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(evaluation_output_dir, 'gensim_lda_topic_distribution.png'))\n",
    "plt.close()\n",
    "\n",
    "print(\"\\nTopic Distribution Statistics (%):\")\n",
    "print(topic_distribution.sort_index())\n",
    "\n",
    "# Analyze assignment probability (confidence) WITHIN this Gensim model\n",
    "avg_confidence = all_posts['lda_topic_confidence'].mean()\n",
    "print(f\"\\nAverage Gensim LDA Topic Assignment Probability: {avg_confidence:.3f}\")\n",
    "\n",
    "# Ensure NaNs are handled if necessary before grouping\n",
    "topic_confidence = all_posts.dropna(subset=['lda_dominant_topic', 'lda_topic_confidence']).groupby('lda_dominant_topic')['lda_topic_confidence'].mean()\n",
    "\n",
    "print(\"\\nTop 5 Topics with Highest Average Assignment Probability:\")\n",
    "print(topic_confidence.nlargest(5))\n",
    "print(\"\\nTop 5 Topics with Lowest Average Assignment Probability:\")\n",
    "print(topic_confidence.nsmallest(5))\n",
    "\n",
    "print(\"\\nRemember: Prioritize coherence, diversity, and interpretability over assignment probability for model comparison.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 14: Process Each Subreddit Separately and Save Results (Commented Out)\n",
    "# def process_subreddit(file_path, dictionary, model):\n",
    "#      with open(file_path, 'r', encoding='utf-8') as f:\n",
    "#          df = pd.DataFrame(json.load(f))\n",
    "#      if 'processed_tokens_lda_nmf' not in df.columns: # Check correct column name\n",
    "#          return None, None\n",
    "#\n",
    "#      texts_subreddit = df['processed_tokens_lda_nmf'].tolist()\n",
    "#      corpus_subreddit = [dictionary.doc2bow(text) for text in texts_subreddit]\n",
    "#\n",
    "#      doc_topic_tuples = [get_dominant_topic_gensim(model.get_document_topics(bow, minimum_probability=0.0))\n",
    "#                          for bow in corpus_subreddit]\n",
    "#\n",
    "#      dominant_topics, topic_confidences = zip(*doc_topic_tuples)\n",
    "#      df['lda_dominant_topic'] = dominant_topics\n",
    "#      df['lda_topic_confidence'] = topic_confidences\n",
    "#      df['lda_dominant_topic'] = df['lda_dominant_topic'] + 1 # 1-based index\n",
    "#\n",
    "#      # Handle NaNs if necessary\n",
    "#      df['lda_dominant_topic'] = df['lda_dominant_topic'].astype('Int64')\n",
    "#\n",
    "#      return df[['id', 'title', 'lda_dominant_topic', 'lda_topic_confidence']]\n",
    "#\n",
    "# # Ensure doc_topics_output_dir is defined and created in Block 2 if uncommenting\n",
    "# # os.makedirs(doc_topics_output_dir, exist_ok=True)\n",
    "#\n",
    "# print(\"\\nProcessing subreddits individually...\")\n",
    "# for fname in os.listdir(processed_dir):\n",
    "#      if fname.startswith('lda_nmf_r_') and fname.endswith('.json'): # Use correct prefix\n",
    "#          print(f\"Processing {fname}...\")\n",
    "#          file_path = os.path.join(processed_dir, fname)\n",
    "#          results_df = process_subreddit(file_path, dictionary, final_lda)\n",
    "#\n",
    "#          if results_df is not None:\n",
    "#              out_file = fname.replace('.json', '_gensim_lda_topics.csv') # Use new suffix\n",
    "#              out_path = os.path.join(doc_topics_output_dir, out_file)\n",
    "#              results_df.to_csv(out_path, index=False)\n",
    "#              print(f\"  Saved: {out_file}\")\n",
    "#          else:\n",
    "#              print(f\"  Skipped {fname} (missing required columns or empty)\")\n",
    "# print(\"Finished processing individual subreddits.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameter search space\n",
    "def optuna_hp_space(trial):\n",
    "    return {\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-6, 1e-4, log=True),\n",
    "        \"num_train_epochs\": trial.suggest_int(\"num_train_epochs\", 2, 5),\n",
    "        \"per_device_train_batch_size\": trial.suggest_categorical(\"per_device_train_batch_size\", [8, 16]),\n",
    "    }\n",
    "\n",
    "# Run hyperparameter search\n",
    "best_trial = trainer.hyperparameter_search(\n",
    "    direction=\"maximize\",\n",
    "    backend=\"optuna\",\n",
    "    hp_space=optuna_hp_space,\n",
    "    n_trials=5\n",
    ")\n",
    "\n",
    "# Apply best hyperparameters\n",
    "for param, value in best_trial.hyperparameters.items():\n",
    "    setattr(training_args, param, value)\n",
    "\n",
    "# Train final model\n",
    "trainer = Trainer(\n",
    "    model=model_init(),\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
