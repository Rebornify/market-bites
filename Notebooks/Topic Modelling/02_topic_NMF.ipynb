{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 1: Import Libraries\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "import time # Optional: To time the tuning process\n",
    "\n",
    "# Add gensim imports\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.models import Nmf\n",
    "from gensim.models import TfidfModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 2: Define Paths for Data and Output\n",
    "project_root = '../../'\n",
    "processed_dir = os.path.join(project_root, 'Data', 'Historical Reddit', 'LDA_NMF_Data')\n",
    "base_output_dir = os.path.join(project_root, 'outputs', 'topic_modeling', 'nmf')\n",
    "model_output_dir = os.path.join(base_output_dir, 'models')\n",
    "evaluation_output_dir = os.path.join(base_output_dir, 'evaluations')\n",
    "viz_output_dir = os.path.join(base_output_dir, 'visualizations', 'wordclouds')\n",
    "# doc_topics_output_dir = os.path.join(base_output_dir, 'document_topics')\n",
    "\n",
    "os.makedirs(model_output_dir, exist_ok=True)\n",
    "os.makedirs(evaluation_output_dir, exist_ok=True)\n",
    "os.makedirs(viz_output_dir, exist_ok=True)\n",
    "# os.makedirs(doc_topics_output_dir, exist_ok=True)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 3: Load and Merge Data from JSON Files\n",
    "dfs = []\n",
    "for fname in os.listdir(processed_dir):\n",
    "    if fname.startswith('lda_nmf_r_') and fname.endswith('.json'):\n",
    "        file_path = os.path.join(processed_dir, fname)\n",
    "        subreddit_name = fname.replace('lda_nmf_r_', '').replace('.json', '')\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            data_json = json.load(f)\n",
    "        df = pd.DataFrame(data_json)\n",
    "        df['subreddit'] = subreddit_name\n",
    "        dfs.append(df)\n",
    "all_posts = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "print(f\"Total number of posts: {len(all_posts)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 4: Data Preparation for Topic Modeling (Gensim NMF with TF-IDF)\n",
    "print(\"Preparing data for Gensim NMF...\")\n",
    "texts = all_posts['processed_tokens_lda_nmf'].tolist() # Use correct token column\n",
    "\n",
    "print(\"Creating Gensim dictionary and corpus (BoW)...\")\n",
    "dictionary = Dictionary(texts)\n",
    "print(f\"Original dictionary size: {len(dictionary)}\")\n",
    "# Apply filtering\n",
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=None) # Same filtering as before\n",
    "print(f\"Filtered dictionary size: {len(dictionary)}\")\n",
    "\n",
    "# Create Bag-of-Words corpus (still needed for TF-IDF model and coherence)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "print(f\"Number of documents in BoW corpus: {len(corpus)}\")\n",
    "print(f\"Number of unique tokens in filtered dictionary: {len(dictionary)}\")\n",
    "\n",
    "# Create TF-IDF representation using Gensim\n",
    "print(\"Creating TF-IDF representation using Gensim...\")\n",
    "tfidf_model = TfidfModel(corpus, id2word=dictionary) # Train TF-IDF model on BoW corpus\n",
    "corpus_tfidf = tfidf_model[corpus] # Apply transformation\n",
    "print(\"TF-IDF corpus created.\")\n",
    "# Note: corpus_tfidf will be used as input for NMF model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 5: Hyperparameter Tuning for NMF Model using NPMI Coherence\n",
    "\n",
    "# --- Helper Function ---\n",
    "def get_gensim_nmf_topics(model, topn=15):\n",
    "    topics_list = []\n",
    "    topic_term_matrix = model.get_topics()\n",
    "    for topic_idx in range(model.num_topics):\n",
    "        top_words_indices = np.argsort(topic_term_matrix[topic_idx, :])[::-1][:topn]\n",
    "        top_words = [model.id2word[idx] for idx in top_words_indices]\n",
    "        topics_list.append(top_words)\n",
    "    return topics_list\n",
    "\n",
    "# --- Define Hyperparameter Grid ---\n",
    "search_params_nmf = {\n",
    "    # 'num_topics': [10, 15, 20, 25, 30]\n",
    "    'num_topics': [18, 19, 20, 21, 22] # Include the current best (20) and neighbors\n",
    "}\n",
    "num_topics_list = search_params_nmf['num_topics']\n",
    "\n",
    "best_score = -np.inf\n",
    "best_params = {}\n",
    "coherence_scores = {}\n",
    "start_time = time.time()\n",
    "num_workers = os.cpu_count() - 1 if os.cpu_count() and os.cpu_count() > 1 else 1\n",
    "\n",
    "# --- Grid Search ---\n",
    "print(f\"Starting NMF hyperparameter tuning with k = {num_topics_list} (using TF-IDF input)...\")\n",
    "for k in num_topics_list:\n",
    "    param_key = f\"k={k}\"\n",
    "    print(f\"  Testing {param_key}...\")\n",
    "\n",
    "    try:\n",
    "        # Initialize and train the temporary NMF model USING TF-IDF CORPUS\n",
    "        nmf_temp = Nmf(\n",
    "            corpus=corpus_tfidf, # *** Use TF-IDF corpus ***\n",
    "            id2word=dictionary,\n",
    "            num_topics=k,\n",
    "            random_state=42,\n",
    "            chunksize=100,\n",
    "            passes=10,\n",
    "            eval_every=None,\n",
    "            normalize=True # Often recommended with TF-IDF input for NMF\n",
    "        )\n",
    "\n",
    "        # Extract topics for coherence calculation\n",
    "        topics = get_gensim_nmf_topics(nmf_temp, 15)\n",
    "\n",
    "        # Calculate Coherence Score (uses BoW corpus and texts)\n",
    "        coherence_model = CoherenceModel(\n",
    "            topics=topics,\n",
    "            texts=texts,\n",
    "            dictionary=dictionary,\n",
    "            corpus=corpus, # *** Coherence uses BoW corpus ***\n",
    "            coherence='c_npmi',\n",
    "            processes=num_workers\n",
    "        )\n",
    "        coherence_score = coherence_model.get_coherence()\n",
    "        coherence_scores[param_key] = coherence_score\n",
    "        print(f\"    NPMI Coherence: {coherence_score:.4f}\")\n",
    "\n",
    "        # Update Best Score and Parameters\n",
    "        if coherence_score > best_score:\n",
    "            best_score = coherence_score\n",
    "            best_params = {'num_topics': k}\n",
    "\n",
    "    except Exception as e:\n",
    "         print(f\"    Failed for {param_key}: {e}\")\n",
    "         coherence_scores[param_key] = np.nan\n",
    "\n",
    "# --- Post-tuning steps (remain the same logic, just ensure best_nmf_params is set) ---\n",
    "end_time = time.time()\n",
    "print(f\"\\nTuning finished in {end_time - start_time:.2f} seconds.\")\n",
    "# ... (print sorted scores) ...\n",
    "sorted_scores = sorted(coherence_scores.items(), key=lambda item: item[1] if not np.isnan(item[1]) else -np.inf, reverse=True)\n",
    "print(f\"\\nAll Coherence Scores:\")\n",
    "for params, score in sorted_scores:\n",
    "    score_str = f\"{score:.4f}\" if not np.isnan(score) else \"Failed\"\n",
    "    print(f\"  {params}: {score_str}\")\n",
    "\n",
    "if not best_params:\n",
    "    print(\"\\nError: All NMF tuning runs failed. Using fallback.\")\n",
    "    best_nmf_params = {'num_topics': 15}\n",
    "else:\n",
    "    print(f\"\\nBest NPMI Score achieved during NMF tuning: {best_score:.4f}\")\n",
    "    print(f\"Best NMF Parameters found: {best_params}\")\n",
    "    best_nmf_params = best_params\n",
    "\n",
    "print(f\"\\nParameters selected for final NMF model training in Block 6: {best_nmf_params}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 6: Train Final NMF Model with Best Parameters\n",
    "\n",
    "print(\"\\nTraining final Gensim NMF model using best parameters (with TF-IDF input)...\")\n",
    "final_nmf = Nmf(\n",
    "    corpus=corpus_tfidf, # *** Use TF-IDF corpus ***\n",
    "    id2word=dictionary,\n",
    "    num_topics=best_nmf_params['num_topics'],\n",
    "    random_state=42,\n",
    "    chunksize=100,\n",
    "    passes=20, # More passes for final model\n",
    "    eval_every=None,\n",
    "    normalize=True # Keep consistent with tuning\n",
    ")\n",
    "\n",
    "print(\"\\nFinal NMF Model Configuration:\")\n",
    "print(final_nmf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 7: Save Trained Gensim NMF Model, Dictionary, and TF-IDF Model\n",
    "\n",
    "print(\"\\nSaving Gensim NMF model, dictionary, and TF-IDF model...\")\n",
    "\n",
    "# Save NMF model\n",
    "model_path = os.path.join(model_output_dir, 'gensim_nmf_tfidf.model') # Indicate TF-IDF input\n",
    "final_nmf.save(model_path)\n",
    "print(f\"Gensim NMF model saved to {model_path}\")\n",
    "\n",
    "# Save Dictionary\n",
    "dict_path = os.path.join(model_output_dir, 'gensim_dictionary.dict')\n",
    "dictionary.save(dict_path)\n",
    "print(f\"Gensim dictionary saved to {dict_path}\")\n",
    "\n",
    "# Save TF-IDF Model (needed for applying NMF to new data later)\n",
    "tfidf_model_path = os.path.join(model_output_dir, 'gensim_tfidf.model')\n",
    "tfidf_model.save(tfidf_model_path)\n",
    "print(f\"Gensim TF-IDF model saved to {tfidf_model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 8: Model Evaluation and Coherence Analysis\n",
    "\n",
    "print(\"\\nEvaluating the final Gensim NMF model (trained on TF-IDF)...\")\n",
    "\n",
    "# Extract topics (helper function remains the same)\n",
    "nmf_topics = get_gensim_nmf_topics(final_nmf, 15)\n",
    "\n",
    "# Calculate topic diversity (remains the same)\n",
    "all_topic_words = [word for topic in nmf_topics for word in topic]\n",
    "topic_diversity = len(set(all_topic_words)) / (final_nmf.num_topics * 15) if final_nmf.num_topics > 0 else 0\n",
    "\n",
    "# Calculate Coherence Scores for the final model\n",
    "print(\"Calculating coherence for the final model...\")\n",
    "# Calculate C_NPMI Coherence (uses BoW corpus)\n",
    "coherence_model_npmi = CoherenceModel(\n",
    "    topics=nmf_topics,\n",
    "    texts=texts,\n",
    "    corpus=corpus, # *** Coherence uses BoW corpus ***\n",
    "    dictionary=dictionary,\n",
    "    coherence='c_npmi',\n",
    "    processes=num_workers\n",
    ")\n",
    "nmf_coherence_npmi = coherence_model_npmi.get_coherence()\n",
    "\n",
    "# Calculate C_V Coherence (uses texts)\n",
    "print(\"Calculating C_V coherence...\")\n",
    "coherence_model_cv = CoherenceModel(\n",
    "    topics=nmf_topics,\n",
    "    texts=texts, # C_V uses the original texts\n",
    "    dictionary=dictionary,\n",
    "    coherence='c_v',\n",
    "    processes=num_workers\n",
    ")\n",
    "nmf_coherence_cv = coherence_model_cv.get_coherence()\n",
    "\n",
    "\n",
    "# Store evaluation results\n",
    "used_random_seed = 42\n",
    "evaluation_results = {\n",
    "    'model_type': 'Gensim NMF (TF-IDF Input)', # Specify input type\n",
    "    'coherence_score_npmi': nmf_coherence_npmi, # Renamed for clarity\n",
    "    'coherence_score_cv': nmf_coherence_cv,     # Added C_V score\n",
    "    'topic_diversity': topic_diversity,\n",
    "    'n_topics': final_nmf.num_topics,\n",
    "    'parameters': {\n",
    "        'num_topics': final_nmf.num_topics,\n",
    "        'passes': final_nmf.passes,\n",
    "        'chunksize': final_nmf.chunksize,\n",
    "        'normalize': final_nmf.normalize, # Record if normalization was used\n",
    "        'random_state_seed': used_random_seed\n",
    "    }\n",
    "}\n",
    "\n",
    "eval_path = os.path.join(evaluation_output_dir, 'gensim_nmf_tfidf_evaluation.json') # Indicate TF-IDF\n",
    "try:\n",
    "    with open(eval_path, 'w') as f:\n",
    "        json.dump(evaluation_results, f, indent=4)\n",
    "    print(f\"Evaluation results saved to {eval_path}\")\n",
    "except TypeError as e:\n",
    "    print(f\"Error saving evaluation results to JSON: {e}\")\n",
    "\n",
    "print(\"\\nModel Evaluation Results:\")\n",
    "print(f\"NPMI Coherence Score: {nmf_coherence_npmi:.4f}\") # Updated variable name\n",
    "print(f\"C_V Coherence Score: {nmf_coherence_cv:.4f}\")   # Added C_V output\n",
    "print(f\"Topic Diversity: {topic_diversity:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 9: Function to Display Top Words for each Topic\n",
    "def display_topics(model, n_top_words=15):\n",
    "    \"\"\"Displays topics from a trained Gensim NMF model and returns them as a 0-indexed dictionary.\"\"\"\n",
    "    topics_dict = {}\n",
    "    # Use the helper function defined in Block 5/8\n",
    "    topics_list = get_gensim_nmf_topics(model, n_top_words)\n",
    "    print(\"\\nGensim NMF Topics:\")\n",
    "    for topic_id, topic_words in enumerate(topics_list):\n",
    "        topics_dict[topic_id] = topic_words # 0-based index for internal use\n",
    "        print(f\"Topic {topic_id + 1}: {' '.join(topic_words)}\") # Display as 1-based\n",
    "    return topics_dict\n",
    "\n",
    "# Display NMF topics from the final model\n",
    "nmf_topics_dict = display_topics(final_nmf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 10: Create Word Clouds for Visualization (Combined Plot)\n",
    "def create_combined_wordcloud(topics_dict, model_name):\n",
    "    \"\"\"Creates and saves a single figure with word clouds for each topic.\"\"\"\n",
    "    num_topics = len(topics_dict)\n",
    "    if num_topics == 0:\n",
    "        print(\"No topics to generate word clouds for.\")\n",
    "        return\n",
    "    # Adjust grid layout dynamically (e.g., aim for ~5 columns)\n",
    "    n_cols = min(5, num_topics)\n",
    "    n_rows = (num_topics + n_cols - 1) // n_cols\n",
    "    plt.figure(figsize=(n_cols * 5, n_rows * 3))\n",
    "\n",
    "    print(f\"\\nGenerating combined word cloud for {num_topics} topics...\")\n",
    "    for topic_idx, words in topics_dict.items(): # topic_idx is 0-based\n",
    "        if not words: # Handle empty topics if they occur\n",
    "             print(f\"Skipping word cloud for empty Topic {topic_idx + 1}\")\n",
    "             continue\n",
    "        # Create frequency dict (can adjust if model provides weights)\n",
    "        word_freq = {word: 1 for word in words}\n",
    "        try:\n",
    "            wordcloud = WordCloud(\n",
    "                width=400, height=200, background_color='white', max_words=100\n",
    "            ).generate_from_frequencies(word_freq)\n",
    "\n",
    "            plt.subplot(n_rows, n_cols, topic_idx + 1) # subplot requires 1-based index\n",
    "            plt.imshow(wordcloud, interpolation='bilinear')\n",
    "            plt.axis('off')\n",
    "            plt.title(f'{model_name} Topic {topic_idx + 1}') # Display 1-based index\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating word cloud for Topic {topic_idx + 1}: {e}\")\n",
    "            # Optionally plot a placeholder or skip\n",
    "\n",
    "    plt.tight_layout(pad=2.0)\n",
    "    out_path_all = os.path.join(viz_output_dir, f'{model_name}_all_topics_wordclouds.png')\n",
    "    plt.savefig(out_path_all, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Saved combined word cloud figure to {out_path_all}\")\n",
    "\n",
    "# Use the 0-indexed dictionary from display_topics\n",
    "create_combined_wordcloud(nmf_topics_dict, 'Gensim_NMF')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 11: Assign Topics to Documents\n",
    "\n",
    "print(\"\\nAssigning topics to documents using Gensim NMF model (TF-IDF input)...\")\n",
    "\n",
    "# Apply the trained NMF model to the TF-IDF corpus\n",
    "doc_topic_dist = final_nmf[corpus_tfidf] # *** Use TF-IDF corpus ***\n",
    "\n",
    "# --- Logic for extracting dominant topic and confidence remains the same ---\n",
    "dominant_topics = []\n",
    "topic_confidences = []\n",
    "for doc_vector in doc_topic_dist:\n",
    "    # ... (same extraction logic as before) ...\n",
    "    if not doc_vector:\n",
    "        dominant_topic_idx = -1\n",
    "        max_prob = 0.0\n",
    "    else:\n",
    "        if isinstance(doc_vector, np.ndarray) or isinstance(doc_vector, list) and not isinstance(doc_vector[0], tuple):\n",
    "             if len(doc_vector) > 0:\n",
    "                 dominant_topic_idx = np.argmax(doc_vector)\n",
    "                 max_prob = np.max(doc_vector)\n",
    "             else:\n",
    "                 dominant_topic_idx = -1\n",
    "                 max_prob = 0.0\n",
    "        elif isinstance(doc_vector, list) and isinstance(doc_vector[0], tuple):\n",
    "             if len(doc_vector) > 0:\n",
    "                  dominant_topic_tuple = max(doc_vector, key=lambda item: item[1])\n",
    "                  dominant_topic_idx = dominant_topic_tuple[0]\n",
    "                  max_prob = dominant_topic_tuple[1]\n",
    "             else:\n",
    "                 dominant_topic_idx = -1\n",
    "                 max_prob = 0.0\n",
    "        else:\n",
    "             dominant_topic_idx = -1\n",
    "             max_prob = 0.0\n",
    "    dominant_topics.append(dominant_topic_idx)\n",
    "    topic_confidences.append(max_prob)\n",
    "\n",
    "\n",
    "# --- Add to DataFrame and handle types (remains the same) ---\n",
    "all_posts['nmf_dominant_topic'] = dominant_topics\n",
    "all_posts['nmf_topic_confidence'] = topic_confidences\n",
    "all_posts['nmf_dominant_topic'] = all_posts['nmf_dominant_topic'] + 1\n",
    "all_posts.loc[all_posts['nmf_dominant_topic'] == 0, ['nmf_dominant_topic', 'nmf_topic_confidence']] = [np.nan, np.nan]\n",
    "all_posts['nmf_dominant_topic'] = all_posts['nmf_dominant_topic'].astype('Int64')\n",
    "all_posts['nmf_topic_confidence'] = all_posts['nmf_topic_confidence'].astype('float32')\n",
    "\n",
    "print(\"\\nSample of assigned topics (1-based index):\")\n",
    "print(all_posts[['nmf_dominant_topic', 'nmf_topic_confidence']].head(10))\n",
    "print(\"\\nData types after assignment:\")\n",
    "print(all_posts[['nmf_dominant_topic', 'nmf_topic_confidence']].dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 12: Analyze Topic Distribution by Subreddit\n",
    "print(\"\\nAnalyzing NMF topic distribution by subreddit...\")\n",
    "\n",
    "# Use dropna() in case NaNs were introduced in Block 11\n",
    "# Ensure nmf_dominant_topic is not NaN before crosstab\n",
    "nmf_topic_by_subreddit = pd.crosstab(\n",
    "    all_posts['subreddit'],\n",
    "    all_posts['nmf_dominant_topic'].dropna(), # Drop NaN topics before crosstab\n",
    "    normalize='index'\n",
    ") * 100\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(nmf_topic_by_subreddit, annot=True, cmap='YlGnBu', fmt='.1f')\n",
    "plt.title('Gensim NMF Topic Distribution by Subreddit (%)')\n",
    "plt.xlabel('Topic (1-based)')\n",
    "plt.ylabel('Subreddit')\n",
    "plt.savefig(os.path.join(evaluation_output_dir, 'gensim_nmf_topic_by_subreddit.png'))\n",
    "plt.close()\n",
    "\n",
    "print(\"Topic distribution analysis by subreddit completed and saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 13: Analyze Overall Topic Distribution\n",
    "print(\"\\nAnalyzing overall Gensim NMF topic distribution...\")\n",
    "# Note: Assignment \"confidence\" (weight) values are model-specific.\n",
    "\n",
    "topic_distribution = all_posts['nmf_dominant_topic'].value_counts(normalize=True) * 100\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "topic_distribution.sort_index().plot(kind='bar')\n",
    "plt.title('Gensim NMF: Distribution of Topics Across All Posts')\n",
    "plt.xlabel('Topic Number (1-based)')\n",
    "plt.ylabel('Percentage of Posts (%)')\n",
    "plt.xticks(rotation=0)\n",
    "plt.grid(True, axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(evaluation_output_dir, 'gensim_nmf_topic_distribution.png'))\n",
    "plt.close()\n",
    "\n",
    "print(\"\\nNMF Topic Distribution Statistics (%):\")\n",
    "# Ensure index is sorted for printing\n",
    "print(topic_distribution.sort_index())\n",
    "\n",
    "# Analyze assignment strength (confidence/weight) WITHIN this Gensim NMF model\n",
    "avg_confidence = all_posts['nmf_topic_confidence'].mean()\n",
    "print(f\"\\nAverage Gensim NMF Topic Assignment Strength: {avg_confidence:.3f}\")\n",
    "\n",
    "# Ensure NaNs are handled before grouping\n",
    "topic_confidence = all_posts.dropna(subset=['nmf_dominant_topic', 'nmf_topic_confidence']).groupby('nmf_dominant_topic')['nmf_topic_confidence'].mean()\n",
    "\n",
    "print(\"\\nTop 5 Topics with Highest Average Assignment Strength:\")\n",
    "print(topic_confidence.nlargest(5))\n",
    "print(\"\\nTop 5 Topics with Lowest Average Assignment Strength:\")\n",
    "print(topic_confidence.nsmallest(5))\n",
    "\n",
    "print(\"\\nRemember: Prioritize coherence, diversity, and interpretability over assignment strength for model comparison.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 14: Process Each Subreddit Separately (Commented Out - Adapted for TF-IDF)\n",
    "\n",
    "# def process_subreddit_nmf_tfidf(file_path, dictionary, tfidf_model_g, nmf_model_g):\n",
    "#      with open(file_path, 'r', encoding='utf-8') as f:\n",
    "#          df = pd.DataFrame(json.load(f))\n",
    "#      if 'processed_tokens_lda_nmf' not in df.columns:\n",
    "#          return None\n",
    "#\n",
    "#      texts_subreddit = df['processed_tokens_lda_nmf'].tolist()\n",
    "#      # Create BoW corpus first\n",
    "#      corpus_subreddit_bow = [dictionary.doc2bow(text) for text in texts_subreddit]\n",
    "#      if not corpus_subreddit_bow: return None\n",
    "#\n",
    "#      # Apply TF-IDF transformation\n",
    "#      corpus_subreddit_tfidf = tfidf_model_g[corpus_subreddit_bow]\n",
    "#\n",
    "#      # Get NMF topic distribution using TF-IDF input\n",
    "#      doc_topic_dist_sub = nmf_model_g[corpus_subreddit_tfidf] # *** Use TF-IDF ***\n",
    "#\n",
    "#      # --- Extract dominant topic/confidence (same logic as Block 11) ---\n",
    "#      dominant_topics_sub = []\n",
    "#      topic_confidences_sub = []\n",
    "#      for doc_vector_sub in doc_topic_dist_sub:\n",
    "#          # ... (same extraction logic) ...\n",
    "#          if not doc_vector_sub:\n",
    "#             dominant_topic_idx_sub = -1; max_prob_sub = 0.0\n",
    "#          elif isinstance(doc_vector_sub, list) and len(doc_vector_sub) > 0 and isinstance(doc_vector_sub[0], tuple):\n",
    "#              dominant_topic_tuple_sub = max(doc_vector_sub, key=lambda item: item[1])\n",
    "#              dominant_topic_idx_sub = dominant_topic_tuple_sub[0]; max_prob_sub = dominant_topic_tuple_sub[1]\n",
    "#          else: # Handle other formats or empty lists\n",
    "#              dominant_topic_idx_sub = -1; max_prob_sub = 0.0\n",
    "#          dominant_topics_sub.append(dominant_topic_idx_sub)\n",
    "#          topic_confidences_sub.append(max_prob_sub)\n",
    "#\n",
    "#      df['nmf_dominant_topic'] = dominant_topics_sub\n",
    "#      df['nmf_topic_confidence'] = topic_confidences_sub\n",
    "#      df['nmf_dominant_topic'] = df['nmf_dominant_topic'] + 1\n",
    "#\n",
    "#      df.loc[df['nmf_dominant_topic'] == 0, ['nmf_dominant_topic', 'nmf_topic_confidence']] = [np.nan, np.nan]\n",
    "#      df['nmf_dominant_topic'] = df['nmf_dominant_topic'].astype('Int64')\n",
    "#      df['nmf_topic_confidence'] = df['nmf_topic_confidence'].astype('float32')\n",
    "#\n",
    "#      return df[['id', 'title', 'nmf_dominant_topic', 'nmf_topic_confidence']]\n",
    "#\n",
    "# # --- Loop (requires loading tfidf_model and final_nmf) ---\n",
    "# # print(\"\\nProcessing subreddits individually for NMF (TF-IDF input)...\")\n",
    "# # Load models if running separately:\n",
    "# # dictionary = Dictionary.load(os.path.join(model_output_dir, 'gensim_dictionary.dict'))\n",
    "# # tfidf_model_loaded = TfidfModel.load(os.path.join(model_output_dir, 'gensim_tfidf.model'))\n",
    "# # final_nmf_loaded = Nmf.load(os.path.join(model_output_dir, 'gensim_nmf_tfidf.model'))\n",
    "# #\n",
    "# # for fname in os.listdir(processed_dir):\n",
    "# #      if fname.startswith('lda_nmf_r_') and fname.endswith('.json'):\n",
    "# #          print(f\"Processing {fname} for NMF (TF-IDF)...\")\n",
    "# #          file_path = os.path.join(processed_dir, fname)\n",
    "# #          results_df = process_subreddit_nmf_tfidf(file_path, dictionary, tfidf_model_loaded, final_nmf_loaded)\n",
    "# #          if results_df is not None:\n",
    "# #              out_file = fname.replace('.json', '_gensim_nmf_tfidf_topics.csv')\n",
    "# #              out_path = os.path.join(doc_topics_output_dir, out_file) # Ensure doc_topics_output_dir exists\n",
    "# #              results_df.to_csv(out_path, index=False)\n",
    "# #              print(f\"  Saved NMF results: {out_file}\")\n",
    "# #          else:\n",
    "# #              print(f\"  Skipped NMF processing for {fname} (missing data or empty)\")\n",
    "# # print(\"Finished processing individual subreddits for NMF (TF-IDF).\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
