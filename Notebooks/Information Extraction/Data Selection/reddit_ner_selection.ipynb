{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from tqdm.notebook import tqdm # Use notebook version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_reddit_posts(input_dir, output_file, target_size_per_subreddit=850, min_words=50, max_words=500):\n",
    "    \"\"\"\n",
    "    Load posts from FinBERT processed files for multiple subreddits,\n",
    "    filter by length, and select a balanced subset.\n",
    "    \"\"\"\n",
    "    subreddit_posts = defaultdict(list)\n",
    "    print(f\"Loading posts from directory: {input_dir}\")\n",
    "\n",
    "    try:\n",
    "        # Check if input directory exists\n",
    "        if not os.path.isdir(input_dir):\n",
    "                print(f\"ERROR: Input directory not found: {input_dir}\")\n",
    "                return\n",
    "        filenames = os.listdir(input_dir)\n",
    "    except Exception as e:\n",
    "            print(f\"ERROR: Could not list files in input directory {input_dir}: {e}\")\n",
    "            return\n",
    "\n",
    "    # --- Load and Filter Posts ---\n",
    "    print(\"Processing files...\")\n",
    "    files_processed = 0\n",
    "    for filename in tqdm(filenames):\n",
    "        if filename.startswith('finbert_r_') and filename.endswith('.json'):\n",
    "            files_processed += 1\n",
    "            subreddit = filename[len('finbert_r_'):-len('.json')]\n",
    "            file_path = os.path.join(input_dir, filename)\n",
    "\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    posts = json.load(f)\n",
    "            except FileNotFoundError:\n",
    "                print(f\"\\nWARNING: File not found {file_path} (skipped)\")\n",
    "                continue\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"\\nWARNING: Could not decode JSON from {file_path} (skipped)\")\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                print(f\"\\nERROR: Reading file {file_path}: {e} (skipped)\")\n",
    "                continue\n",
    "\n",
    "            # Filter posts based on length and structure\n",
    "            valid_posts_for_subreddit = []\n",
    "            for post in posts:\n",
    "                if isinstance(post, dict) and 'processed_text_finbert' in post and 'id' in post:\n",
    "                        text = post.get('processed_text_finbert', '')\n",
    "                        word_count = len(text.split())\n",
    "                        if min_words <= word_count <= max_words:\n",
    "                            # Add subreddit info if missing (using filename)\n",
    "                            if 'subreddit' not in post:\n",
    "                                post['subreddit'] = subreddit\n",
    "                            valid_posts_for_subreddit.append(post)\n",
    "\n",
    "            subreddit_posts[subreddit].extend(valid_posts_for_subreddit)\n",
    "            # Optional: Add print inside loop if needed: print(f\"Found {len(valid_posts_for_subreddit)} valid posts in r/{subreddit}\")\n",
    "    print(f\"\\nProcessed {files_processed} potential subreddit files.\")\n",
    "    if not subreddit_posts:\n",
    "        print(\"ERROR: No valid posts found in any subreddit file. Cannot proceed.\")\n",
    "        return\n",
    "\n",
    "    # --- Select Balanced Subset ---\n",
    "    print(f\"\\n--- Selecting Posts (Target per subreddit: {target_size_per_subreddit}) ---\")\n",
    "    selected_posts = []\n",
    "    total_available = 0\n",
    "    for subreddit, posts in subreddit_posts.items():\n",
    "        total_available += len(posts)\n",
    "        print(f\"Subreddit r/{subreddit}: Found {len(posts)} valid posts.\")\n",
    "        count_to_select = min(len(posts), target_size_per_subreddit)\n",
    "\n",
    "        if count_to_select > 0:\n",
    "            selected = random.sample(posts, count_to_select)\n",
    "            print(f\" -> Selecting {len(selected)} posts.\")\n",
    "            selected_posts.extend(selected)\n",
    "        else:\n",
    "                print(f\" -> Not enough valid posts to select.\")\n",
    "\n",
    "    print(f\"\\nTotal valid posts available across subreddits: {total_available}\")\n",
    "\n",
    "    # --- Final Steps ---\n",
    "    print(\"\\n--- Finalizing Selection ---\")\n",
    "    # Shuffle the final selection\n",
    "    random.shuffle(selected_posts)\n",
    "    print(f\"Total posts selected across all subreddits: {len(selected_posts)}\")\n",
    "\n",
    "    # Simplify output format if needed (currently keeps original structure)\n",
    "    # If you only want id and text like the news script:\n",
    "    simplified_posts = [\n",
    "        {\n",
    "            \"id\": post[\"id\"],\n",
    "            \"text\": post[\"processed_text_finbert\"]\n",
    "            # Optionally keep subreddit: \"subreddit\": post.get('subreddit', 'unknown')\n",
    "        } for post in selected_posts\n",
    "    ]\n",
    "    print(f\"Saving {len(simplified_posts)} posts with 'id' and 'text' fields.\")\n",
    "\n",
    "\n",
    "    # Save selected posts\n",
    "    os.makedirs(os.path.dirname(output_file), exist_ok=True) # Ensure output directory exists\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        # json.dump(selected_posts, f, indent=2, ensure_ascii=False) # Use this to save full structure\n",
    "        json.dump(simplified_posts, f, indent=2, ensure_ascii=False) # Use this to save simplified structure\n",
    "    print(f\"\\nSaved selected posts to {output_file}\")\n",
    "\n",
    "    # Print final counts per subreddit\n",
    "    print(\"\\nFinal Posts per subreddit in selection:\")\n",
    "    final_subreddit_counts = defaultdict(int)\n",
    "    # Adjust based on whether you saved simplified_posts or selected_posts\n",
    "    for post in selected_posts: # Iterate original selection to get subreddit info easily\n",
    "            final_subreddit_counts[post.get('subreddit', 'unknown')] += 1\n",
    "\n",
    "    # Check if any simplified posts are missing subreddit info (shouldn't happen with fix)\n",
    "    # for post in simplified_posts:\n",
    "    #    final_subreddit_counts[post.get('subreddit', 'unknown')] += 1\n",
    "\n",
    "    for subreddit, count in final_subreddit_counts.items():\n",
    "            print(f\"- r/{subreddit}: {count} posts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "# !! Adjust this path to point to your FinBERT processed Reddit data directory !!\n",
    "# Assumes it's relative to the IS450 Project/Notebooks/ level\n",
    "input_reddit_dir = \"../../../Data/Historical Reddit/FinBERT_Data/\"\n",
    "\n",
    "# Output file relative to the notebook's location (e.g., inside Information Extraction/Data Selection/)\n",
    "# This will save it in Information Extraction/Unlabeled/\n",
    "output_reddit_file = \"../../../Data/Historical Reddit/NER_Data/Unlabeled/5000_reddit_posts_for_NER.json\"\n",
    "\n",
    "target_posts_per_sub = 850 # Target number of posts per subreddit\n",
    "min_word_count = 50\n",
    "max_word_count = 500\n",
    "\n",
    "# --- Run Selection ---\n",
    "select_reddit_posts(input_reddit_dir, output_reddit_file, target_posts_per_sub, min_word_count, max_word_count)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
