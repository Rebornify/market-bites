{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import os\n",
    "from spacy import displacy # For visualization (optional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths relative to this notebook (located in Model Training/)\n",
    "# --- Paths now point to the 'outputs' folder ---\n",
    "\n",
    "# --- News Models ---\n",
    "news_pretrained_model_path = \"../../outputs/information_extraction/news/grid_search_models_pretrained_sequential/best_model_pretrained_sequential\"\n",
    "news_blank_model_path = \"../../outputs/information_extraction/news/grid_search_models_blank_sequential/best_model_blank_sequential\"\n",
    "\n",
    "# --- Reddit Models ---\n",
    "reddit_pretrained_model_path = \"../../outputs/information_extraction/reddit/grid_search_models_pretrained_sequential/best_model_pretrained_sequential\"\n",
    "reddit_blank_model_path = \"../../outputs/information_extraction/reddit/grid_search_models_blank_sequential/best_model_blank_sequential\"\n",
    "\n",
    "# --- Load Models ---\n",
    "models = {} # Dictionary to store loaded models\n",
    "\n",
    "# Function to safely load a model\n",
    "def load_model(name, path):\n",
    "    print(f\"Attempting to load model '{name}' from: {path}\") # More explicit print\n",
    "    if os.path.exists(path):\n",
    "        try:\n",
    "            models[name] = spacy.load(path)\n",
    "            print(f\" -> Success.\")\n",
    "        except Exception as e:\n",
    "            print(f\" -> ERROR loading model '{name}': {e}\")\n",
    "    else:\n",
    "        print(f\" -> Path not found.\") # Simplified message\n",
    "\n",
    "# Load the models\n",
    "load_model(\"News_Pretrained\", news_pretrained_model_path)\n",
    "load_model(\"News_Blank\", news_blank_model_path)\n",
    "load_model(\"Reddit_Pretrained\", reddit_pretrained_model_path)\n",
    "load_model(\"Reddit_Blank\", reddit_blank_model_path)\n",
    "\n",
    "print(f\"\\nFinished loading attempts. Successfully loaded {len(models)} models.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Sample Texts ---\n",
    "news_sample_text = \"France car registrations down 14.54% in March, Tesla sales fall 36.83%. New car registrations in France fell 14.54% in March from a year earlier to 153,842 vehicles, data from French car body PFA showed on Tuesday.\"\n",
    "\n",
    "reddit_sample_text = \"Thinking of buying more GME and AMC stonks tomorrow. Diamond hands! To the moon! Market might dip 5% but I believe. Fed meeting next week on April 10th might affect things.\"\n",
    "\n",
    "# Choose which text to test\n",
    "text_to_test = news_sample_text\n",
    "# text_to_test = reddit_sample_text\n",
    "\n",
    "print(f\"--- Running Inference on: ---\")\n",
    "print(text_to_test)\n",
    "print(\"-\" * 27)\n",
    "\n",
    "# --- Run Inference with Loaded Models ---\n",
    "for model_name, nlp in models.items():\n",
    "    print(f\"\\nüîç Results from Model: {model_name}\")\n",
    "    if nlp:\n",
    "            try:\n",
    "                doc = nlp(text_to_test)\n",
    "                if doc.ents:\n",
    "                    for ent in doc.ents:\n",
    "                        print(f\"  - {ent.text} ({ent.label_}) [{ent.start_char}:{ent.end_char}]\")\n",
    "                    # Optional: Visualize\n",
    "                    # displacy.render(doc, style=\"ent\", jupyter=True)\n",
    "                else:\n",
    "                    print(\"  (No entities found)\")\n",
    "            except Exception as e:\n",
    "                print(f\"  ERROR during inference with {model_name}: {e}\")\n",
    "    else:\n",
    "            print(f\"  (Model {model_name} not loaded)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install transformers if not already installed\n",
    "# !pip install transformers torch torchvision torchaudio -q\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "import warnings\n",
    "\n",
    "# Suppress specific warnings if desired\n",
    "warnings.filterwarnings(\"ignore\", message=\"Some weights of the model checkpoint.*\")\n",
    "\n",
    "print(\"\\n--- Comparing with Hugging Face (dslim/bert-base-NER) ---\")\n",
    "\n",
    "try:\n",
    "    # Load tokenizer and model\n",
    "    # Using a more robust model like bert-large-NER might give better results\n",
    "    # hf_model_name = \"dslim/bert-large-NER\"\n",
    "    hf_model_name = \"dslim/bert-base-NER\"\n",
    "    print(f\"Loading HF tokenizer and model: {hf_model_name}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(hf_model_name)\n",
    "    model = AutoModelForTokenClassification.from_pretrained(hf_model_name)\n",
    "\n",
    "    # Create NER pipeline\n",
    "    # Set aggregation_strategy for cleaner output (e.g., 'simple', 'first', 'average', 'max')\n",
    "    nlp_hf = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")\n",
    "\n",
    "    # Run NER on the chosen sample text\n",
    "    print(f\"\\nRunning HF NER on:\")\n",
    "    print(text_to_test)\n",
    "    ner_results_hf = nlp_hf(text_to_test)\n",
    "\n",
    "    print(\"\\nüîç Results from Hugging Face Model:\")\n",
    "    if ner_results_hf:\n",
    "        for entity in ner_results_hf:\n",
    "                print(f\"  - {entity['word']} ({entity['entity_group']}) (Score: {entity['score']:.4f}) [{entity['start']}:{entity['end']}]\")\n",
    "    else:\n",
    "        print(\"  (No entities found)\")\n",
    "\n",
    "except ImportError:\n",
    "    print(\"\\nNOTE: 'transformers' library not installed. Skipping Hugging Face comparison.\")\n",
    "    print(\"Install using: pip install transformers torch\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nERROR during Hugging Face comparison: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
