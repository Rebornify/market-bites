{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base Imports\n",
    "import spacy\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import itertools\n",
    "import shutil\n",
    "import traceback\n",
    "import time # Added for timing\n",
    "\n",
    "# spaCy Specific Imports\n",
    "from spacy.util import compounding, minibatch\n",
    "from tqdm.notebook import tqdm # Use notebook version\n",
    "from spacy.tokens import DocBin\n",
    "from spacy.training.example import Example\n",
    "from spacy.scorer import Scorer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_json_to_spacy_blank(json_file, output_file):\n",
    "    \"\"\"\n",
    "    Convert JSON labeled data to SpaCy's binary format — no label remapping (for blank model training).\n",
    "    Uses strict alignment, checks for overlaps, and reports stats.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(json_file):\n",
    "            print(f\"ERROR: Input JSON file not found: {json_file}\")\n",
    "            raise FileNotFoundError(f\"Input JSON file not found: {json_file}\")\n",
    "\n",
    "    with open(json_file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    nlp = spacy.blank(\"en\")  # Blank model for tokenization\n",
    "    db = DocBin()\n",
    "    labels_used = set()\n",
    "    skipped_docs = 0\n",
    "    total_entities_found = 0\n",
    "    skipped_entities = 0\n",
    "\n",
    "    print(f\"Converting data from {json_file} to SpaCy format for blank model...\")\n",
    "    for item in tqdm(data, desc=\"Converting JSON (Blank)\"):\n",
    "        text = item.get(\"text\", \"\")\n",
    "        entities = item.get(\"entities\", [])\n",
    "        if not text:\n",
    "            # print(\"\\nWarning: Skipping item with empty text.\") # Reduce verbosity\n",
    "            skipped_docs += 1\n",
    "            continue\n",
    "        total_entities_found += len(entities)\n",
    "\n",
    "        try:\n",
    "            doc = nlp.make_doc(text)\n",
    "        except Exception as e:\n",
    "            print(f\"\\nWarning: Skipping doc due to nlp.make_doc error: {e}. Text: '{text[:50]}...'\")\n",
    "            skipped_docs += 1\n",
    "            skipped_entities += len(entities)\n",
    "            continue\n",
    "\n",
    "        ents = []\n",
    "        seen_tokens = set()\n",
    "        for ent in entities:\n",
    "            label = ent.get(\"label\")\n",
    "            start = ent.get(\"start\")\n",
    "            end = ent.get(\"end\")\n",
    "\n",
    "            if label is None or start is None or end is None:\n",
    "                # print(f\"\\nWarning: Skipping entity with missing fields {ent} in text: '{text[:50]}...'\")\n",
    "                skipped_entities += 1\n",
    "                continue\n",
    "\n",
    "            labels_used.add(label) # No mapping needed for blank model\n",
    "\n",
    "            try:\n",
    "                # Use strict alignment first\n",
    "                span = doc.char_span(start, end, label=label, alignment_mode=\"strict\")\n",
    "                if span is None:\n",
    "                    # print(f\"\\nWarning: Skipping entity {ent} (label: {label}) due to invalid span (alignment failed) in text: '{text[max(0, start-10):min(len(text), end+10)]}...' text-len: {len(text)}\")\n",
    "                    skipped_entities += 1\n",
    "                    continue # Skip this entity\n",
    "            except Exception as e:\n",
    "                    print(f\"\\nWarning: Error creating span for entity {ent} (label: {label}) in text: '{text[:50]}...'. Error: {e}\")\n",
    "                    skipped_entities += 1\n",
    "                    span = None\n",
    "\n",
    "            if span is None:\n",
    "                continue # Skip if span creation failed\n",
    "\n",
    "            # Check for overlapping tokens\n",
    "            token_indices = {tok.i for tok in span}\n",
    "            if token_indices.intersection(seen_tokens):\n",
    "                # print(f\"\\nWarning: Skipping entity {ent} label='{span.label_}' text='{span.text}' due to overlapping tokens in text: '{text[max(0, span.start_char-10):min(len(text), span.end_char+10)]}...'\" )\n",
    "                skipped_entities += 1\n",
    "                continue # Skip entity\n",
    "\n",
    "            ents.append(span)\n",
    "            seen_tokens.update(token_indices)\n",
    "\n",
    "        # Only add the doc if it's considered valid\n",
    "        try:\n",
    "            ents.sort(key=lambda s: s.start_char) # Sort ents\n",
    "            doc.ents = ents\n",
    "            db.add(doc)\n",
    "        except ValueError as e: # Catches potential overlap errors spaCy might find\n",
    "            print(f\"\\nSkipping doc due to ValueError during final assignment (likely overlap): {e}. Text: '{text[:50]}...'\")\n",
    "            print(f\"Entities attempted: {[(e.start_char, e.end_char, e.label_) for e in ents] if ents else '[]'}\")\n",
    "            skipped_docs += 1\n",
    "            skipped_entities += len(ents) # Count entities in the skipped doc as skipped\n",
    "        except Exception as e:\n",
    "            print(f\"\\nSkipping doc due to unexpected error during final assignment: {e}. Text: '{text[:50]}...'\")\n",
    "            skipped_docs += 1\n",
    "            skipped_entities += len(ents)\n",
    "\n",
    "    db.to_disk(output_file)\n",
    "    total_docs = len(data)\n",
    "    saved_docs = len(db)\n",
    "    print(f\"\\n✅ Saved {saved_docs}/{total_docs} documents to {output_file}\")\n",
    "    if skipped_docs > 0:\n",
    "        print(f\"  Skipped {skipped_docs} documents due to errors during conversion.\")\n",
    "    print(f\"🧾 Labels used in conversion: {sorted(labels_used)}\")\n",
    "\n",
    "    # Print entity statistics\n",
    "    processed_entities = total_entities_found - skipped_entities\n",
    "    print(f\"\\n📊 Entity Conversion Stats:\")\n",
    "    print(f\"  Total entities in JSON: {total_entities_found}\")\n",
    "    print(f\"  Skipped entities (invalid/overlap/error): {skipped_entities}\")\n",
    "    print(f\"  Entities successfully converted: {processed_entities}\")\n",
    "    if total_entities_found > 0:\n",
    "        success_rate = (processed_entities / total_entities_found) * 100\n",
    "        print(f\"  Entity success rate: {success_rate:.2f}%\")\n",
    "    else:\n",
    "        print(\"  No entities found to calculate success rate.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is identical to the one in 01a_spacy_pretrained_training.ipynb\n",
    "# It's included here for completeness of the notebook.\n",
    "def split_data(full_data_path, train_ratio=0.9, prefix=\"blank\"): # Default prefix changed\n",
    "    \"\"\"\n",
    "    Split data from a full .spacy file into training and test sets based on train_ratio.\n",
    "    Returns paths to temporary train and test files, using a prefix for uniqueness.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(full_data_path):\n",
    "            raise FileNotFoundError(f\"Full data file not found: {full_data_path}\")\n",
    "\n",
    "    try:\n",
    "        doc_bin = DocBin().from_disk(full_data_path)\n",
    "        # Use blank model vocab for splitting\n",
    "        data = list(doc_bin.get_docs(spacy.blank(\"en\").vocab))\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Failed to load documents from {full_data_path}. Check file integrity. Error: {e}\")\n",
    "\n",
    "    if not data:\n",
    "        raise ValueError(f\"No documents loaded from {full_data_path}.\")\n",
    "\n",
    "    random.shuffle(data)\n",
    "    split_point = int(len(data) * train_ratio)\n",
    "    train_data = data[:split_point]\n",
    "    test_data = data[split_point:]\n",
    "\n",
    "    if not train_data or not test_data:\n",
    "            raise ValueError(f\"Could not split data. Train size: {len(train_data)}, Test size: {len(test_data)}. Check train_ratio ({train_ratio}).\")\n",
    "\n",
    "    # Use temporary file names with prefix in the current working directory\n",
    "    train_path = f\"temp_train_data_{prefix}.spacy\"\n",
    "    test_path = f\"temp_test_data_{prefix}.spacy\"\n",
    "\n",
    "    try:\n",
    "        DocBin(docs=train_data).to_disk(train_path)\n",
    "        DocBin(docs=test_data).to_disk(test_path)\n",
    "    except Exception as e:\n",
    "        # Clean up files if writing fails\n",
    "        if os.path.exists(train_path): os.remove(train_path)\n",
    "        if os.path.exists(test_path): os.remove(test_path)\n",
    "        raise IOError(f\"Failed to write temporary split data files ('{train_path}', '{test_path}'). Error: {e}\")\n",
    "\n",
    "    print(f\"Split data with ratio {train_ratio}: {len(train_data)} train, {len(test_data)} test (prefix: {prefix})\")\n",
    "    return train_path, test_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_blank_spacy_model(train_data_path, output_model_path=None, n_iter=10,\n",
    "                            dropout=0.1, batch_start=4.0, batch_end=32.0, batch_compound=1.5):\n",
    "    \"\"\"\n",
    "    Train a SpaCy NER model from a blank English model.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Training Model (Base: blank 'en') ---\")\n",
    "    print(f\"Params: n_iter={n_iter}, dropout={dropout}, batch=({batch_start}, {batch_end}, {batch_compound})\")\n",
    "    print(\"🧼 Loading blank English model...\")\n",
    "    nlp = spacy.blank(\"en\")\n",
    "\n",
    "    # Add NER pipeline\n",
    "    if \"ner\" not in nlp.pipe_names:\n",
    "        ner = nlp.add_pipe(\"ner\")\n",
    "        # print(\"Added 'ner' pipeline to blank model.\") # Less verbose\n",
    "    else:\n",
    "        ner = nlp.get_pipe(\"ner\")\n",
    "\n",
    "    # Load training data\n",
    "    if not os.path.exists(train_data_path):\n",
    "            raise FileNotFoundError(f\"Training data file not found: {train_data_path}\")\n",
    "    try:\n",
    "        doc_bin = DocBin().from_disk(train_data_path)\n",
    "        train_data = list(doc_bin.get_docs(nlp.vocab))\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Failed to load training documents from {train_data_path}. Error: {e}\")\n",
    "\n",
    "    if not train_data:\n",
    "        raise ValueError(f\"No training data loaded from {train_data_path}.\")\n",
    "\n",
    "    # Add entity labels dynamically from the training data\n",
    "    labels = {ent.label_ for doc in train_data if doc.ents for ent in doc.ents}\n",
    "    if not labels:\n",
    "        print(\"Warning: No entity labels found in the training data!\")\n",
    "    else:\n",
    "        # print(f\"Found labels in training data: {sorted(list(labels))}\") # Less verbose\n",
    "        for label in sorted(list(labels)):\n",
    "            ner.add_label(label)\n",
    "        print(f\"🏷️ Labels added to NER component: {sorted(ner.labels)}\")\n",
    "\n",
    "    print(f\"\\n🚀 Starting training from scratch for {n_iter} iterations...\")\n",
    "    # Disable other pipes (though likely none in a blank model)\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n",
    "    with nlp.disable_pipes(*other_pipes):\n",
    "        # Use begin_training for blank models\n",
    "        try:\n",
    "            optimizer = nlp.begin_training()\n",
    "        except Exception as e:\n",
    "            print(f\"Error during nlp.begin_training: {e}\")\n",
    "            raise e\n",
    "\n",
    "        all_losses = []\n",
    "        for itn in range(n_iter):\n",
    "            random.shuffle(train_data)\n",
    "            losses = {}\n",
    "\n",
    "            # Set up batching with hyperparameters\n",
    "            try:\n",
    "                batch_config = compounding(batch_start, batch_end, batch_compound)\n",
    "            except ValueError as e:\n",
    "                print(f\"Error creating compounding batch size: {e}. Using fixed size 8.\")\n",
    "                batch_config = 8 # Fallback\n",
    "\n",
    "            batches = minibatch(train_data, size=batch_config)\n",
    "            processed_batches = 0\n",
    "            progress = tqdm(batches, desc=f\"Iter {itn+1}\", unit=\"batch\", leave=False)\n",
    "\n",
    "            for batch in progress:\n",
    "                if not batch: continue\n",
    "                examples = []\n",
    "                for doc in batch:\n",
    "                    try:\n",
    "                        # We assume train_data has Doc objects with gold standard annotations\n",
    "                        ex = Example(nlp.make_doc(doc.text), doc)\n",
    "                        examples.append(ex)\n",
    "                    except Exception as e:\n",
    "                        print(f\"\\nWarning: Unexpected error creating Example: '{doc.text[:30]}...'. Error: {e}\")\n",
    "                        continue\n",
    "\n",
    "                if not examples: continue\n",
    "\n",
    "                try:\n",
    "                    nlp.update(examples, sgd=optimizer, losses=losses, drop=dropout)\n",
    "                except Exception as e:\n",
    "                    print(f\"\\n❌ Error during nlp.update: {e}\")\n",
    "                    print(f\"Failed on batch with {len(examples)} examples. First text: '{examples[0].text[:50]}...'\")\n",
    "                    break # Stop this iteration\n",
    "\n",
    "                processed_batches += 1\n",
    "                progress.set_postfix(loss=f\"{losses.get('ner', 0.0):.3f}\", batches=processed_batches)\n",
    "\n",
    "            ner_loss = losses.get('ner', None)\n",
    "            all_losses.append(ner_loss)\n",
    "            loss_str = f\"{ner_loss:.3f}\" if ner_loss is not None else \"N/A\"\n",
    "            print(f\"  ✅ Iteration {itn + 1}/{n_iter} - NER Loss: {loss_str}\")\n",
    "\n",
    "    # Save the trained model if path provided\n",
    "    if output_model_path:\n",
    "        try:\n",
    "            nlp.to_disk(output_model_path)\n",
    "            print(f\"\\n📦 Blank model saved to {output_model_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\n❌ Error saving blank model to {output_model_path}: {e}\")\n",
    "\n",
    "    return nlp, all_losses # Return trained nlp object and losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is identical to the one in 01a_spacy_pretrained_training.ipynb\n",
    "# It's included here for completeness.\n",
    "def evaluate_model(model_path, test_data_path):\n",
    "    \"\"\"\n",
    "    Evaluate the trained model and return scores.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Evaluating Model: {model_path} ---\")\n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"❌ Model path does not exist: {model_path}\")\n",
    "        return None\n",
    "    if not os.path.exists(test_data_path):\n",
    "            print(f\"❌ Test data path does not exist: {test_data_path}\")\n",
    "            return None\n",
    "\n",
    "    try:\n",
    "        nlp_eval = spacy.load(model_path)\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading model {model_path} for evaluation: {e}\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        doc_bin = DocBin().from_disk(test_data_path)\n",
    "        test_data = list(doc_bin.get_docs(nlp_eval.vocab))\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading test documents from {test_data_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "    if not test_data:\n",
    "        print(f\"❌ No test data loaded from {test_data_path}.\")\n",
    "        return None\n",
    "\n",
    "    print(f\"Evaluating on {len(test_data)} test documents...\")\n",
    "    examples = []\n",
    "    skipped_examples = 0\n",
    "    for doc in tqdm(test_data, desc=\"Creating evaluation examples\"):\n",
    "            try:\n",
    "                # Create Example objects for evaluation\n",
    "                ex = Example(nlp_eval(doc.text), doc) # Use nlp_eval to predict\n",
    "                examples.append(ex)\n",
    "            except Exception as e:\n",
    "                print(f\"\\nWarning: Unexpected error creating Example for evaluation: '{doc.text[:30]}...'. Error: {e}\")\n",
    "                skipped_examples += 1\n",
    "                continue\n",
    "\n",
    "    if skipped_examples > 0:\n",
    "        print(f\"Skipped {skipped_examples} examples during evaluation preparation.\")\n",
    "\n",
    "    if not examples:\n",
    "        print(\"❌ No valid examples created for evaluation.\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        scorer = Scorer()\n",
    "        scores = scorer.score_examples(examples, \"ents\") # Evaluate entities only\n",
    "\n",
    "        # More detailed scores (optional but useful)\n",
    "        per_entity_scores = scores.get(\"ents_per_type\", {})\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error during scoring: {e}\")\n",
    "        return None\n",
    "\n",
    "    print(\"\\n--- Evaluation Results ---\")\n",
    "    print(f\"Overall NER Precision: {scores.get('ents_p', 0.0):.4f}\")\n",
    "    print(f\"Overall NER Recall:    {scores.get('ents_r', 0.0):.4f}\")\n",
    "    print(f\"Overall NER F1-Score:  {scores.get('ents_f', 0.0):.4f}\")\n",
    "\n",
    "    if per_entity_scores:\n",
    "        print(\"\\nScores per Entity Type:\")\n",
    "        # Header\n",
    "        print(f\"{'Label':<15} {'P':<10} {'R':<10} {'F':<10}\")\n",
    "        print(\"-\" * 45)\n",
    "        # Sort by label for consistent output\n",
    "        for label, metrics in sorted(per_entity_scores.items()):\n",
    "            p = metrics.get('p', 0.0)\n",
    "            r = metrics.get('r', 0.0)\n",
    "            f = metrics.get('f', 0.0)\n",
    "            print(f\"{label:<15} {p:<10.4f} {r:<10.4f} {f:<10.4f}\")\n",
    "    else:\n",
    "        print(\"\\nNo per-entity scores available.\")\n",
    "\n",
    "    return scores # Return the full scores dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_blank(train_data_path, test_data_path, grid_search_output_dir, param_grid):\n",
    "    \"\"\"\n",
    "    Perform grid search over specified hyperparameters for blank model training.\n",
    "    Saves the best model based on F1-score.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Starting Grid Search (Base: blank 'en') ---\")\n",
    "    # Ensure grid search directory exists and is clean\n",
    "    if os.path.exists(grid_search_output_dir):\n",
    "        print(f\"Clearing existing grid search directory: {grid_search_output_dir}\")\n",
    "        shutil.rmtree(grid_search_output_dir)\n",
    "    os.makedirs(grid_search_output_dir, exist_ok=True)\n",
    "\n",
    "    # Prepare parameter combinations\n",
    "    keys, values = zip(*param_grid.items())\n",
    "    combinations = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "    print(f\"Total parameter combinations to test: {len(combinations)}\")\n",
    "\n",
    "    best_f1 = -1.0\n",
    "    best_params = None\n",
    "    best_model_path_overall = None\n",
    "    results = [] # Store results for each combination\n",
    "\n",
    "    for i, params in enumerate(combinations):\n",
    "        print(f\"\\n--- Combination {i+1}/{len(combinations)} ---\")\n",
    "        print(f\"Parameters: {params}\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Construct a unique model path for this combination\n",
    "        param_str = \"_\".join(f\"{k}{v}\" for k, v in sorted(params.items()))\n",
    "        current_model_output_path = os.path.join(grid_search_output_dir, f\"model_{param_str}\")\n",
    "\n",
    "        try:\n",
    "            # Train the blank model with current parameters\n",
    "            nlp_trained, losses = train_blank_spacy_model(\n",
    "                train_data_path=train_data_path,\n",
    "                output_model_path=current_model_output_path,\n",
    "                n_iter=params.get('n_iter', 10), # Use get with default\n",
    "                dropout=params.get('dropout', 0.1),\n",
    "                batch_start=params.get('batch_start', 4.0),\n",
    "                batch_end=params.get('batch_end', 32.0),\n",
    "                batch_compound=params.get('batch_compound', 1.5)\n",
    "            )\n",
    "\n",
    "            # Evaluate the trained model\n",
    "            scores = evaluate_model(current_model_output_path, test_data_path)\n",
    "\n",
    "            end_time = time.time()\n",
    "            duration = end_time - start_time\n",
    "\n",
    "            if scores:\n",
    "                current_f1 = scores.get('ents_f', 0.0)\n",
    "                results.append({\n",
    "                    \"params\": params,\n",
    "                    \"f1\": current_f1,\n",
    "                    \"precision\": scores.get('ents_p', 0.0),\n",
    "                    \"recall\": scores.get('ents_r', 0.0),\n",
    "                    \"duration_seconds\": duration,\n",
    "                    \"model_path\": current_model_output_path,\n",
    "                    \"final_loss\": losses[-1] if losses else None\n",
    "                })\n",
    "\n",
    "                print(f\"Combination {i+1} F1-Score: {current_f1:.4f} (Duration: {duration:.2f}s)\")\n",
    "\n",
    "                # Check if this is the best model so far\n",
    "                if current_f1 > best_f1:\n",
    "                    best_f1 = current_f1\n",
    "                    best_params = params\n",
    "                    best_model_path_overall = current_model_output_path\n",
    "                    print(f\"🏆 New Best F1 Score Found!\")\n",
    "            else:\n",
    "                print(f\"Evaluation failed for combination {i+1}. Skipping.\")\n",
    "                results.append({\n",
    "                    \"params\": params, \"f1\": None, \"precision\": None, \"recall\": None,\n",
    "                    \"duration_seconds\": duration, \"model_path\": None, \"final_loss\": None\n",
    "                })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\\n❌❌ Error during grid search combination {i+1} ({params}):\")\n",
    "            traceback.print_exc() # Print detailed traceback\n",
    "            end_time = time.time()\n",
    "            duration = end_time - start_time\n",
    "            results.append({\n",
    "                \"params\": params, \"f1\": None, \"precision\": None, \"recall\": None,\n",
    "                \"duration_seconds\": duration, \"model_path\": None, \"error\": str(e)\n",
    "            })\n",
    "            # Continue to the next combination\n",
    "\n",
    "    print(\"\\n--- Grid Search Complete ---\")\n",
    "    if best_params:\n",
    "        print(f\"Best F1-Score: {best_f1:.4f}\")\n",
    "        print(f\"Best Parameters: {best_params}\")\n",
    "        print(f\"Best Model Path: {best_model_path_overall}\")\n",
    "\n",
    "        # Optionally, copy the best model to a fixed \"best_model\" directory\n",
    "        best_model_final_dir = os.path.join(grid_search_output_dir, \"best_model_blank_sequential\") # Match original script name\n",
    "        try:\n",
    "            if os.path.exists(best_model_final_dir):\n",
    "                shutil.rmtree(best_model_final_dir)\n",
    "            shutil.copytree(best_model_path_overall, best_model_final_dir)\n",
    "            print(f\"Copied best model to: {best_model_final_dir}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error copying best model: {e}\")\n",
    "    else:\n",
    "        print(\"Grid search did not find a successful model.\")\n",
    "\n",
    "    # Save results summary\n",
    "    results_summary_path = os.path.join(grid_search_output_dir, \"grid_search_summary.json\")\n",
    "    try:\n",
    "        with open(results_summary_path, 'w', encoding='utf-8') as f:\n",
    "                # Sort results by F1 score descending for readability\n",
    "            sorted_results = sorted([r for r in results if r['f1'] is not None], key=lambda x: x['f1'], reverse=True)\n",
    "            json.dump(sorted_results, f, indent=2)\n",
    "        print(f\"Grid search summary saved to {results_summary_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving grid search summary: {e}\")\n",
    "\n",
    "    return best_model_path_overall # Return path to best model dir inside grid search output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "# Source: News Data\n",
    "SOURCE_NAME = \"News\"\n",
    "PREFIX = \"blank_news\" # Unique prefix for temp files\n",
    "\n",
    "# Paths relative to this notebook (located in Model Training/News/)\n",
    "INPUT_JSON_PATH = \"../../../Data/Historical News/NER_Data/Labeled/ner_labeled_news_dataset_spacy.json\"\n",
    "# Output of conversion / Input to split - NOW IN DATA FOLDER\n",
    "FULL_SPACY_DATA_PATH = \"../../../Data/Historical News/NER_Data/spaCy_Format/full_data_blank.spacy\"\n",
    "# Output of grid search - NOW IN OUTPUTS FOLDER\n",
    "GRID_SEARCH_DIR = \"../../../outputs/information_extraction/news/grid_search_models_blank_sequential\"\n",
    "BEST_MODEL_DIR = os.path.join(GRID_SEARCH_DIR, \"best_model_blank_sequential\") # Final best model location\n",
    "\n",
    "# Grid Search Parameters (Example - Adjust as needed)\n",
    "param_grid = {\n",
    "    'n_iter': [15, 25], # Might need more iterations for blank models\n",
    "    'dropout': [0.1, 0.25, 0.5],\n",
    "    'batch_start': [4],\n",
    "    'batch_end': [32, 64],\n",
    "    'batch_compound': [1.001]\n",
    "}\n",
    "\n",
    "TRAIN_RATIO = 0.9\n",
    "\n",
    "# --- Execution ---\n",
    "temp_train_path = None\n",
    "temp_test_path = None\n",
    "best_model_path = None\n",
    "\n",
    "# Ensure output directories exist before starting\n",
    "os.makedirs(os.path.dirname(FULL_SPACY_DATA_PATH), exist_ok=True)\n",
    "# Grid search function will handle creation of GRID_SEARCH_DIR\n",
    "\n",
    "try:\n",
    "    # 1. Convert JSON data to .spacy format\n",
    "    print(\"--- Step 1: Converting JSON to spaCy format (Blank) ---\")\n",
    "    convert_json_to_spacy_blank(INPUT_JSON_PATH, FULL_SPACY_DATA_PATH)\n",
    "\n",
    "    # 2. Split data into temporary train/test files (will be created in current dir)\n",
    "    print(\"\\n--- Step 2: Splitting data ---\")\n",
    "    temp_train_path, temp_test_path = split_data(FULL_SPACY_DATA_PATH, TRAIN_RATIO, prefix=PREFIX)\n",
    "\n",
    "    # 3. Run Grid Search\n",
    "    print(\"\\n--- Step 3: Running Grid Search (Blank) ---\")\n",
    "    best_model_path = grid_search_blank( # Call blank version of grid search\n",
    "        train_data_path=temp_train_path,\n",
    "        test_data_path=temp_test_path,\n",
    "        grid_search_output_dir=GRID_SEARCH_DIR,\n",
    "        param_grid=param_grid\n",
    "    )\n",
    "\n",
    "    # 4. Final Evaluation of the Best Model (optional)\n",
    "    if best_model_path and os.path.exists(BEST_MODEL_DIR): # Check if best model was found and copied\n",
    "         print(\"\\n--- Step 4: Final Evaluation of Best Model (Blank) ---\")\n",
    "         evaluate_model(BEST_MODEL_DIR, temp_test_path)\n",
    "    elif best_model_path:\n",
    "         print(\"\\n--- Step 4: Final Evaluation Skipped (Best model not copied?) ---\")\n",
    "    else:\n",
    "         print(\"\\n--- Step 4: Final Evaluation Skipped (No best model found) ---\")\n",
    "\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"\\n❌ ERROR: Required file not found. {e}\")\n",
    "except ValueError as e:\n",
    "    print(f\"\\n❌ ERROR: Data loading or processing issue. {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ An unexpected error occurred during the main execution:\")\n",
    "    traceback.print_exc()\n",
    "finally:\n",
    "    # Clean up temporary files\n",
    "    print(\"\\n--- Cleaning up temporary files ---\")\n",
    "    if temp_train_path and os.path.exists(temp_train_path):\n",
    "        os.remove(temp_train_path)\n",
    "        print(f\"Removed {temp_train_path}\")\n",
    "    if temp_test_path and os.path.exists(temp_test_path):\n",
    "        os.remove(temp_test_path)\n",
    "        print(f\"Removed {temp_test_path}\")\n",
    "    print(\"Cleanup complete.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
