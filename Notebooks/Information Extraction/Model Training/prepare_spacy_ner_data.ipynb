{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import os\n",
    "from tqdm.notebook import tqdm # Use notebook version for progress bar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "\n",
    "# Choose 'news' or 'reddit' to process\n",
    "data_source = 'reddit' # Or 'news'\n",
    "\n",
    "if data_source == 'reddit':\n",
    "    # Input: Labeled data from OpenAI (without spans)\n",
    "    input_json_path = \"../../../Data/Historical Reddit/NER_Data/Labeled/ner_labeled_reddit_dataset.json\"\n",
    "    # Output: Labeled data ready for spaCy training (with spans)\n",
    "    output_json_path = \"../../../Data/Historical Reddit/NER_Data/Labeled/ner_labeled_reddit_dataset_spacy.json\"\n",
    "elif data_source == 'news':\n",
    "    # Input: Labeled data from OpenAI (without spans)\n",
    "    input_json_path = \"../../../Data/Historical News/NER_Data/Labeled/ner_labeled_news_dataset.json\"\n",
    "    # Output: Labeled data ready for spaCy training (with spans)\n",
    "    output_json_path = \"../../../Data/Historical News/NER_Data/Labeled/ner_labeled_news_dataset_spacy.json\"\n",
    "else:\n",
    "    raise ValueError(\"data_source must be 'reddit' or 'news'\")\n",
    "\n",
    "print(f\"Processing source: {data_source}\")\n",
    "print(f\"Input path: {input_json_path}\")\n",
    "print(f\"Output path: {output_json_path}\")\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(os.path.dirname(output_json_path), exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the labeled data\n",
    "if not os.path.exists(input_json_path):\n",
    "    print(f\"Error: Input file not found at {input_json_path}\")\n",
    "    # Or raise FileNotFoundError(\"Input file not found...\")\n",
    "    data = None # Indicate data wasn't loaded\n",
    "else:\n",
    "    print(f\"Loading data from {input_json_path}...\")\n",
    "    try:\n",
    "        with open(input_json_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        print(f\"Loaded {len(data)} articles.\")\n",
    "    except json.JSONDecodeError as e:\n",
    "            print(f\"Error decoding JSON from {input_json_path}: {e}\")\n",
    "            data = None\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred loading {input_json_path}: {e}\")\n",
    "        data = None\n",
    "\n",
    "# Proceed only if data was loaded successfully\n",
    "if data is None:\n",
    "    print(\"Cannot proceed without loaded data.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data is not None: # Check if data loaded successfully\n",
    "    processed_data_with_spans = []\n",
    "    total_entities_processed = 0\n",
    "    total_find_errors = 0 # Errors finding *any* match\n",
    "    total_assignment_errors = 0 # Errors finding an *unassigned* match during fallback\n",
    "    total_validation_errors = 0 # Errors where found span text doesn't match entity text exactly (after sequential check failed)\n",
    "\n",
    "\n",
    "    print(\"\\nProcessing articles to find unique entity spans...\")\n",
    "    # Use tqdm for progress bar over articles\n",
    "    for article_index, article in enumerate(tqdm(data, desc=\"Processing Articles\")):\n",
    "        article_id = article.get(\"id\")\n",
    "        article_text = article.get(\"text\", \"\")\n",
    "        # Ensure entities is a list, default to empty list if missing or null\n",
    "        entities = article.get(\"entities\") if isinstance(article.get(\"entities\"), list) else []\n",
    "\n",
    "        if not article_text:\n",
    "                print(f\"Warning [{article_id}]: Skipping article with empty text.\")\n",
    "                processed_data_with_spans.append({\n",
    "                    \"id\": article_id,\n",
    "                    \"text\": article_text,\n",
    "                    \"entities\": [] # Ensure entities key exists even if skipped\n",
    "                })\n",
    "                continue\n",
    "\n",
    "        processed_entities = []\n",
    "        assigned_spans = set() # Keep track of (start, end) tuples already assigned\n",
    "        search_start_pos = 0 # Position to start the next sequential search\n",
    "\n",
    "        for entity_index, entity in enumerate(entities):\n",
    "            # Ensure entity is a dictionary\n",
    "            if not isinstance(entity, dict):\n",
    "                    print(f\"Warning [{article_id}/{entity_index}]: Skipping invalid entity format (not a dictionary): {entity}\")\n",
    "                    continue\n",
    "\n",
    "            entity_text = entity.get(\"text\")\n",
    "            entity_label = entity.get(\"label\")\n",
    "\n",
    "            # Validate entity text and label\n",
    "            if not isinstance(entity_text, str) or not entity_text or not isinstance(entity_label, str) or not entity_label:\n",
    "                print(f\"Warning [{article_id}/{entity_index}]: Skipping entity with missing/invalid text ('{entity_text}') or label ('{entity_label}').\")\n",
    "                continue\n",
    "\n",
    "            total_entities_processed += 1\n",
    "            assigned_this_entity = False # Track if the current entity was successfully assigned\n",
    "\n",
    "            try:\n",
    "                # Escape potential regex special characters\n",
    "                escaped_entity_text = re.escape(entity_text)\n",
    "                match = None\n",
    "                potential_sequential_span = None\n",
    "\n",
    "                # --- Pre-check for exact match existence ---\n",
    "                # This helps decide later if a case-insensitive match is acceptable\n",
    "                # Use word boundaries (\\b) for potentially more robust matching, esp. for shorter entities\n",
    "                # However, \\b might fail if entity starts/ends with non-alphanumeric, so use carefully\n",
    "                # Let's stick to simple search for now unless problems arise.\n",
    "                # has_exact_match_anywhere = re.search(r'\\b' + escaped_entity_text + r'\\b', article_text) is not None\n",
    "                has_exact_match_anywhere = re.search(escaped_entity_text, article_text) is not None\n",
    "\n",
    "\n",
    "                # 1. Try sequential search (case-sensitive, exact match)\n",
    "                # Search from the last known end position to prioritize sequential flow\n",
    "                if search_start_pos < len(article_text):\n",
    "                    match_cs = re.search(escaped_entity_text, article_text[search_start_pos:])\n",
    "                    if match_cs:\n",
    "                        start_char = match_cs.start() + search_start_pos\n",
    "                        end_char = match_cs.end() + search_start_pos\n",
    "                        span = (start_char, end_char)\n",
    "                        # Crucially check if the found text is EXACTLY the entity text\n",
    "                        if article_text[start_char:end_char] == entity_text and span not in assigned_spans:\n",
    "                            processed_entities.append({\n",
    "                                \"text\": entity_text, # Use original entity text\n",
    "                                \"label\": entity_label,\n",
    "                                \"start\": start_char,\n",
    "                                \"end\": end_char\n",
    "                            })\n",
    "                            assigned_spans.add(span)\n",
    "                            search_start_pos = end_char # Update sequential search position\n",
    "                            assigned_this_entity = True\n",
    "\n",
    "                # 2. Fallback: If sequential exact match failed, search entire text for first available EXACT match\n",
    "                if not assigned_this_entity:\n",
    "                    matches_cs_all = list(re.finditer(escaped_entity_text, article_text))\n",
    "                    found_fallback_exact = False\n",
    "                    for m in matches_cs_all:\n",
    "                        start_char = m.start()\n",
    "                        end_char = m.end()\n",
    "                        span = (start_char, end_char)\n",
    "                        # Check for exact text match and availability\n",
    "                        if article_text[start_char:end_char] == entity_text and span not in assigned_spans:\n",
    "                            processed_entities.append({\n",
    "                                \"text\": entity_text, # Use original entity text\n",
    "                                \"label\": entity_label,\n",
    "                                \"start\": start_char,\n",
    "                                \"end\": end_char\n",
    "                            })\n",
    "                            assigned_spans.add(span)\n",
    "                            # DO NOT update search_start_pos here, as we broke sequential flow\n",
    "                            assigned_this_entity = True\n",
    "                            found_fallback_exact = True\n",
    "                            break # Found first available exact match\n",
    "\n",
    "                # 3. Fallback 2: If NO exact match was assignable, try case-insensitive IF no exact match exists ANYWHERE\n",
    "                if not assigned_this_entity and not has_exact_match_anywhere:\n",
    "                    matches_ci_all = list(re.finditer(escaped_entity_text, article_text, re.IGNORECASE))\n",
    "                    found_fallback_ci = False\n",
    "                    for m in matches_ci_all:\n",
    "                            start_char = m.start()\n",
    "                            end_char = m.end()\n",
    "                            span = (start_char, end_char)\n",
    "                            matched_text_in_article = article_text[start_char:end_char]\n",
    "                            # Check length and availability, accept case difference ONLY if no exact match existed\n",
    "                            if len(matched_text_in_article) == len(entity_text) and span not in assigned_spans:\n",
    "                                processed_entities.append({\n",
    "                                    \"text\": matched_text_in_article, # Use the text found in the article\n",
    "                                    \"label\": entity_label,\n",
    "                                    \"start\": start_char,\n",
    "                                    \"end\": end_char\n",
    "                                })\n",
    "                                assigned_spans.add(span)\n",
    "                                assigned_this_entity = True\n",
    "                                found_fallback_ci = True\n",
    "                                print(f\"Info [{article_id}/{entity_index}]: Used case-insensitive match '{matched_text_in_article}' for entity '{entity_text}' at {span} (no exact match found anywhere).\")\n",
    "                                break # Found first available case-insensitive match\n",
    "\n",
    "                # If still not assigned after all attempts\n",
    "                if not assigned_this_entity:\n",
    "                    # Determine reason for logging\n",
    "                    if not list(re.finditer(escaped_entity_text, article_text, re.IGNORECASE)):\n",
    "                        print(f\"Error [{article_id}/{entity_index}]: Could not find any occurrence (CS/CI) of entity text '{entity_text}'. Skipping.\")\n",
    "                        total_find_errors += 1\n",
    "                    else:\n",
    "                        # It exists, but all occurrences were already assigned or failed validation (e.g., case mismatch when exact existed)\n",
    "                        print(f\"Warning [{article_id}/{entity_index}]: Could not assign a span for entity text '{entity_text}'. All occurrences might be assigned or invalid (e.g. case mismatch). Skipping.\")\n",
    "                        total_assignment_errors += 1\n",
    "\n",
    "\n",
    "            except re.error as e:\n",
    "                print(f\"Regex Error [{article_id}/{entity_index}] for entity '{entity_text}': {e}\")\n",
    "                total_find_errors += 1 # Count regex errors as find errors\n",
    "            except Exception as e:\n",
    "                print(f\"Unexpected Error processing entity [{article_id}/{entity_index}] '{entity_text}': {e}\")\n",
    "                # Depending on severity, you might count this differently or re-raise\n",
    "\n",
    "\n",
    "        # Add the article with processed entities (including spans) to the output list\n",
    "        processed_data_with_spans.append({\n",
    "            \"id\": article_id,\n",
    "            \"text\": article_text,\n",
    "            # Sort processed entities by start index for consistency before saving\n",
    "            \"entities\": sorted(processed_entities, key=lambda x: x.get('start', -1))\n",
    "        })\n",
    "    print(\"\\nFinished processing articles.\") # Add completion message for the loop\n",
    "# End of `if data is not None:` block\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data is not None: # Only save and print summary if processing happened\n",
    "    # Save the processed data with spans to the output file\n",
    "    print(f\"\\nSaving data with spans to {output_json_path}...\")\n",
    "    try:\n",
    "        with open(output_json_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(processed_data_with_spans, f, indent=2, ensure_ascii=False)\n",
    "        print(\"Save successful.\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Failed to save output file {output_json_path}: {e}\")\n",
    "\n",
    "\n",
    "    print(\"\\n--- Processing Summary ---\")\n",
    "    print(f\"Total articles loaded: {len(data)}\")\n",
    "    print(f\"Total entities processed (attempted): {total_entities_processed}\")\n",
    "    total_assigned = sum(len(art['entities']) for art in processed_data_with_spans)\n",
    "    print(f\"Total entities successfully assigned spans: {total_assigned}\")\n",
    "    print(f\"  - Errors finding any occurrence: {total_find_errors}\")\n",
    "    print(f\"  - Warnings (occurrence found but could not be assigned): {total_assignment_errors}\")\n",
    "    # print(f\"Total validation errors (found span text mismatch - sequential only): {total_validation_errors}\") # Removed this metric as logic changed\n",
    "\n",
    "    if total_entities_processed > 0:\n",
    "        # Consider find errors and assignment warnings as issues\n",
    "        total_issues = total_find_errors + total_assignment_errors\n",
    "        issue_rate = (total_issues / total_entities_processed) * 100\n",
    "        success_rate = (total_assigned / total_entities_processed) * 100\n",
    "        print(f\"Overall entity assignment success rate: {success_rate:.2f}%\")\n",
    "        print(f\"Overall entity assignment issue rate: {issue_rate:.2f}%\")\n",
    "    print(f\"Output saved to {output_json_path}\")\n",
    "else:\n",
    "    print(\"\\nNo data was processed or saved due to loading errors.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
