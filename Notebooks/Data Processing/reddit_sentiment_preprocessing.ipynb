{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBU3jmc5xgxl",
        "outputId": "b3a16569-9460-4989-bea2-5d3f241a113d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading NLTK resource: wordnet\n",
            "Filtered posts directory: ../../Data\\Historical Reddit\\Filtered Posts\n",
            "ML Data directory: ../../Data\\Historical Reddit\\ML_Data\n",
            "FinBERT Data directory: ../../Data\\Historical Reddit\\FinBERT_Data\n",
            "Lexicon Data directory: ../../Data\\Historical Reddit\\Lexicon_Data\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to C:\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import re\n",
        "import html\n",
        "import os\n",
        "import contractions\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import pos_tag\n",
        "import nltk\n",
        "\n",
        "# Ensure NLTK data is available (run this once if needed)\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "# --- Change this line ---\n",
        "except LookupError: # Changed from nltk.downloader.DownloadError\n",
        "    print(\"Downloading NLTK resource: punkt\")\n",
        "    nltk.download('punkt')\n",
        "try:\n",
        "    nltk.data.find('corpora/stopwords')\n",
        "# --- Change this line ---\n",
        "except LookupError: # Changed from nltk.downloader.DownloadError\n",
        "    print(\"Downloading NLTK resource: stopwords\")\n",
        "    nltk.download('stopwords')\n",
        "try:\n",
        "    nltk.data.find('corpora/wordnet')\n",
        "# --- Change this line ---\n",
        "except LookupError: # Changed from nltk.downloader.DownloadError\n",
        "    print(\"Downloading NLTK resource: wordnet\")\n",
        "    nltk.download('wordnet')\n",
        "try:\n",
        "    nltk.data.find('taggers/averaged_perceptron_tagger')\n",
        "# --- Change this line ---\n",
        "except LookupError: # Changed from nltk.downloader.DownloadError\n",
        "    print(\"Downloading NLTK resource: averaged_perceptron_tagger\")\n",
        "    nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Define the project root relative to the current notebook's directory\n",
        "project_root = '../../'\n",
        "\n",
        "# Define paths relative to the project root\n",
        "filtered_posts_dir = os.path.join(project_root, 'Data', 'Historical Reddit', 'Filtered Posts')\n",
        "ml_data_dir = os.path.join(project_root, 'Data', 'Historical Reddit', 'ML_Data')\n",
        "finbert_data_dir = os.path.join(project_root, 'Data', 'Historical Reddit', 'FinBERT_Data')\n",
        "lexicon_data_dir = os.path.join(project_root, 'Data', 'Historical Reddit', 'Lexicon_Data')\n",
        "\n",
        "# Ensure directories exist\n",
        "if not os.path.exists(ml_data_dir):\n",
        "    os.makedirs(ml_data_dir)\n",
        "if not os.path.exists(finbert_data_dir):\n",
        "    os.makedirs(finbert_data_dir)\n",
        "if not os.path.exists(lexicon_data_dir):\n",
        "    os.makedirs(lexicon_data_dir)\n",
        "\n",
        "print(f\"Filtered posts directory: {filtered_posts_dir}\")\n",
        "print(f\"ML Data directory: {ml_data_dir}\")\n",
        "print(f\"FinBERT Data directory: {finbert_data_dir}\")\n",
        "print(f\"Lexicon Data directory: {lexicon_data_dir}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQsLDp5ey16r",
        "outputId": "959d0776-75df-4fb4-d499-681593c496ca"
      },
      "outputs": [],
      "source": [
        "# Dictionary for lexicon-based sentiment\n",
        "sentiment_jargon_dict = {\n",
        "    # ðŸš€ Bullish Sentiment / Positive\n",
        "    \"diamond hands\": \"diamond_hands\", \"hodl\": \"hodl\", \"hodler\": \"hodler\",\n",
        "    \"wagmi\": \"wagmi\", \"ape in\": \"ape_in\", \"btfd\": \"btfd\", \"buy the dip\": \"buy_the_dip\",\n",
        "    \"go long\": \"go_long\", \"long it\": \"long_it\", \"going long\": \"going_long\",\n",
        "    \"bull run\": \"bull_run\", \"bullish\": \"bullish\", \"long call\": \"long_call\",\n",
        "    \"long position\": \"long_position\", \"long stock\": \"long_stock\",\n",
        "    \"to the moon\": \"to_the_moon\", \"moonshot\": \"moonshot\", \"ath\": \"ath\",\n",
        "    \"stonks\": \"stonks\", \"dca\": \"dca\", \"flippening\": \"flippening\",\n",
        "    \"wen lambo\": \"wen_lambo\", \"bear trap\": \"bear_trap\",\n",
        "\n",
        "    # ðŸ˜¬ Bearish Sentiment / Negative\n",
        "    \"paper hands\": \"paper_hands\", \"bagholder\": \"bagholder\", \"ngmi\": \"ngmi\",\n",
        "    \"rekt\": \"rekt\", \"gn\": \"gn\", \"bearish\": \"bearish\", \"long put\": \"long_put\",\n",
        "    \"exit scam\": \"exit_scam\", \"shill\": \"shill\", \"rug pull\": \"rug_pull\",\n",
        "    \"pump and dump\": \"pump_and_dump\", \"margin call\": \"margin_call\",\n",
        "    \"liquidated\": \"liquidated\", \"overleveraged\": \"overleveraged\",\n",
        "    \"capitulate\": \"capitulate\", \"bull trap\": \"bull_trap\",\n",
        "    \"short position\": \"short_position\", \"short sell\": \"short_sell\",\n",
        "    \"shorting\": \"shorting\", \"shorts\": \"shorts\", \"shorted\": \"shorted\",\n",
        "    \"short interest\": \"short_interest\", \"short call\": \"short_call\",\n",
        "    \"short it\": \"short_it\", \"short-selling\": \"short_selling\",\n",
        "    \"short selling\": \"short_selling\"\n",
        "}\n",
        "\n",
        "# Comprehensive dictionary for ML preprocessing\n",
        "ml_jargon_dict = {\n",
        "    **sentiment_jargon_dict,\n",
        "    # Neutral financial terms\n",
        "    \"market cap\": \"market_cap\", \"price target\": \"price_target\",\n",
        "    \"earnings per share\": \"eps\", \"price to earnings\": \"pe_ratio\",\n",
        "    \"return on equity\": \"roe\", \"return on investment\": \"roi\",\n",
        "    \"dollar cost averaging\": \"dollar_cost_averaging\", \"day trading\": \"day_trading\",\n",
        "    \"swing trading\": \"swing_trading\", \"technical analysis\": \"technical_analysis\",\n",
        "    \"fundamental analysis\": \"fundamental_analysis\", \"market sentiment\": \"market_sentiment\",\n",
        "    \"trading volume\": \"trading_volume\", \"price action\": \"price_action\",\n",
        "    \"support level\": \"support_level\", \"resistance level\": \"resistance_level\",\n",
        "    \"moving average\": \"moving_average\", \"relative strength\": \"relative_strength\",\n",
        "    \"market trend\": \"market_trend\", \"trading strategy\": \"trading_strategy\",\n",
        "    \"investment portfolio\": \"investment_portfolio\", \"risk management\": \"risk_management\",\n",
        "    \"position sizing\": \"position_sizing\", \"stop loss\": \"stop_loss\",\n",
        "    \"take profit\": \"take_profit\", \"market order\": \"market_order\",\n",
        "    \"limit order\": \"limit_order\", \"options chain\": \"options_chain\",\n",
        "    \"implied volatility\": \"implied_volatility\", \"time value\": \"time_value\",\n",
        "    \"intrinsic value\": \"intrinsic_value\", \"dividend yield\": \"dividend_yield\",\n",
        "    \"market maker\": \"market_maker\", \"bid ask spread\": \"bid_ask_spread\",\n",
        "    \"liquidity\": \"liquidity\", \"market depth\": \"market_depth\",\n",
        "    \"order book\": \"order_book\", \"market hours\": \"market_hours\",\n",
        "    \"pre market\": \"pre_market\", \"after hours\": \"after_hours\",\n",
        "    \"market open\": \"market_open\", \"market close\": \"market_close\"\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "htEXk9Aey4YD"
      },
      "outputs": [],
      "source": [
        "# Combined and conservatively reviewed additional stopwords for sentiment analysis\n",
        "\n",
        "additional_stopwords = {\n",
        "    # --- Original list from sentiment notebook ---\n",
        "    'say', 'like', 'make', 'think', 'know', 'people', 'use', 'want', # Generic conversational noise\n",
        "    'post', 'link', 'click', 'reddit', 'user', 'thread', 'discussion', 'karma', # Reddit meta noise\n",
        "    'rule', 'comment', 'mod', 'topic', 'share', 'list',\n",
        "    'day', 'week', 'month', 'year', 'today', 'january', 'april', 'friday', 'thursday', # Generic time words (original)\n",
        "\n",
        "    # Generic conversational noise (neutral additions)\n",
        "    'also', 'back', 'come', 'even', 'every', 'find', 'get', 'give', 'go',\n",
        "    'keep', 'let', 'look', 'lot', 'many', 'may', 'must', 'new', 'one',\n",
        "    'part', 'really', 'right', 'see', 'seem', 'something', 'still', 'thing',\n",
        "    'try', 'way', 'would', # 'well', 'feel', 'much' excluded\n",
        "\n",
        "    # Reddit meta noise (neutral additions)\n",
        "    'edit', 'general', 'information', 'meta', 'op', 'poll', 'prior', 'read',\n",
        "    'sub', 'subreddit', 'welcome', 'please',\n",
        "\n",
        "    # Generic time words (neutral additions)\n",
        "    'currently', 'daily', 'et', 'last', 'monthly', 'pm', 'recent', 'since',\n",
        "    'time', 'tomorrow', 'yesterday', # Added more specific time refs\n",
        "\n",
        "    # Common actions/verbs/nouns (mostly neutral additions)\n",
        "    'account', 'around', 'article', 'data', 'different', 'example', 'move',\n",
        "    'need', 'pay', 'project', 'start', 'take', 'work',\n",
        "\n",
        "    # Conversational noise (neutral additions)\n",
        "    'answer', 'ask', 'help', 'hi', 'live', 'open', 'question', 'talk', 'tell', 'you',\n",
        "\n",
        "    # Artifacts (neutral additions)\n",
        "    'com', 'dot', 'inc'\n",
        "}\n",
        "\n",
        "# Ensure negations are kept for sentiment\n",
        "stopwords_to_keep = {\n",
        "    \"not\", \"no\", \"but\", \"without\", \"against\", \"never\", \"neither\", \"nor\", \"cannot\"\n",
        "}\n",
        "\n",
        "# Create final custom stopwords set\n",
        "default_stopwords = set(stopwords.words('english'))\n",
        "custom_stopwords = default_stopwords.copy()\n",
        "custom_stopwords.update(additional_stopwords)\n",
        "custom_stopwords = custom_stopwords - stopwords_to_keep\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MytBEAaR510f",
        "outputId": "89a32136-89d4-4cd3-b88b-4a0a28f6aedc"
      },
      "outputs": [],
      "source": [
        "def remove_markdown(text):\n",
        "    \"\"\"Removes common markdown formatting (but preserves newlines).\"\"\"\n",
        "    # NOTE: DO NOT remove newlines here anymore! It's done in initial_clean.\n",
        "    # text = re.sub(r'\\\\n+|\\n+', ' ', text)                   # <<-- REMOVED FROM HERE\n",
        "    text = re.sub(r'\\*\\*(.*?)\\*\\*', r'\\1', text)            # Remove bold markdown (**text**)\n",
        "    text = re.sub(r'__(.*?)__', r'\\1', text)                # Remove bold markdown (__text__)\n",
        "    text = re.sub(r'\\*(.*?)\\*', r'\\1', text)                # Remove italic markdown (*text*)\n",
        "    text = re.sub(r'_(.*?)_', r'\\1', text)                  # Remove italic markdown (_text_) - may affect underscores in words\n",
        "    text = re.sub(r'^\\s*#+\\s*(.*?)\\s*#*\\s*$', r'\\1', text, flags=re.MULTILINE) # Remove markdown headers (# Header)\n",
        "    text = re.sub(r'^\\s*>\\s?(.*)', r'\\1', text, flags=re.MULTILINE) # Remove markdown blockquotes (> quote)\n",
        "    text = re.sub(r'\\[(.*?)\\]\\(.*?\\)', r'\\1', text)         # Remove markdown links, keeping the link text ([text](url) -> text)\n",
        "    text = re.sub(r'`{1,3}(.*?)`{1,3}', r'\\1', text, flags=re.DOTALL) # Remove markdown code ticks/fences (`code` or ```code```)\n",
        "    text = re.sub(r'~~(.*?)~~', r'\\1', text)                # Remove markdown strikethrough (~~text~~)\n",
        "    text = re.sub(r'^\\s*[\\*\\-\\+]\\s+', '', text, flags=re.MULTILINE) # Remove unordered list markers (*, -, +) at line start\n",
        "    text = re.sub(r'^\\s*\\d+\\.\\s+', '', text, flags=re.MULTILINE) # Remove ordered list markers (1., 2.) at line start\n",
        "    text = re.sub(r'^\\s*[-*_]{3,}\\s*$', '', text, flags=re.MULTILINE) # Remove horizontal rules (---, ***, ___ )\n",
        "\n",
        "    # Do not strip here, final stripping happens in initial_clean\n",
        "    # return text.strip()\n",
        "    return text\n",
        "\n",
        "def replace_jargon(text, jargon_dict):\n",
        "    \"\"\"Replaces phrases from the jargon dict with their standardized form.\"\"\"\n",
        "    for phrase, replacement in jargon_dict.items():\n",
        "        # Use word boundaries to avoid partial matches within words\n",
        "        # Make the pattern case-insensitive\n",
        "        pattern = re.compile(r'\\b' + re.escape(phrase) + r'\\b', re.IGNORECASE)\n",
        "        text = pattern.sub(replacement, text)\n",
        "    return text\n",
        "\n",
        "def initial_clean(text):\n",
        "    \"\"\"Performs initial text cleaning common to most pipelines.\"\"\"\n",
        "    text = re.sub(r'<.*?>', '', text)                       # Remove any remaining HTML tags (e.g., <b>, <i>) BEFORE unescaping\n",
        "\n",
        "    text = html.unescape(text)                              # Unescape HTML entities like &amp; -> &\n",
        "    text = html.unescape(text)                              # Double unescape just in case some entities were doubly encoded\n",
        "\n",
        "    # Remove subreddit (r/) and user (u/) references\n",
        "    # Use word boundaries (\\b) to avoid matching mid-word\n",
        "    # Handles r/subreddit, /r/subreddit, u/user, /u/user formats\n",
        "    text = re.sub(r'\\b[/\\\\]?[ur]/\\w+\\b', '', text)\n",
        "\n",
        "    text = remove_markdown(text)                            # Apply general markdown formatting removal (links, bold, lists etc.) -- *NOTE: Newlines still present at this point*\n",
        "\n",
        "    # --- Table Removal (Requires newlines to be present for ^ with MULTILINE) ---\n",
        "    # This single regex handles both |:---| and :--| formats:\n",
        "    text = re.sub(r'^\\s*[|: -]+\\|?\\s*$\\n?', '', text, flags=re.MULTILINE) # Remove separator lines  <- CHECK THIS LINE\n",
        "    text = re.sub(r'^.*\\|(?:.*\\|)+.*$\\n?', '', text, flags=re.MULTILINE)\n",
        "    # --- End Table Removal ---\n",
        "\n",
        "    text = re.sub(r'http\\S+|www\\.\\S+', '', text)             # Remove URLs (http/https or www.)\n",
        "\n",
        "    try:\n",
        "        text = contractions.fix(text)                       # Expand contractions (e.g., \"don't\" -> \"do not\")\n",
        "    except Exception as e:\n",
        "        # print(f\"Contraction fixing failed for text: {text[:100]}... Error: {e}\") # Optional: uncomment to log errors\n",
        "        pass                                                # Continue if contraction fixing fails for an edge case\n",
        "\n",
        "    # --- Final Cleanup ---\n",
        "    text = re.sub(r'\\\\n+|\\n+', ' ', text)                   # Replace literal '\\n' strings and actual newlines with a space *NOW*\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()                # Normalize whitespace (multiple spaces to one space, trim ends)\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Map Treebank POS tags to WordNet POS tags\n",
        "def get_wordnet_pos(treebank_tag):\n",
        "    \"\"\"Maps NLTK POS tags to WordNet POS tags.\"\"\"\n",
        "    if treebank_tag.startswith('J'): return wordnet.ADJ\n",
        "    elif treebank_tag.startswith('V'): return wordnet.VERB\n",
        "    elif treebank_tag.startswith('N'): return wordnet.NOUN\n",
        "    elif treebank_tag.startswith('R'): return wordnet.ADV\n",
        "    else: return wordnet.NOUN # Default to noun\n",
        "\n",
        "def resolve_us_token(token, pos_tag):\n",
        "    \"\"\"Disambiguates 'us' (pronoun) vs 'US' (United States).\"\"\"\n",
        "    # Check the original token's case if needed, but POS tag is usually sufficient\n",
        "    if token.lower() == \"us\" and pos_tag == \"NNP\": # NNP = Proper noun, singular\n",
        "        return \"united_states\"\n",
        "    return token\n",
        "\n",
        "def tokenize_lemmatize_lowercase(text):\n",
        "    \"\"\"Tokenizes, POS tags, lemmatizes, and lowercases text.\"\"\"\n",
        "    tokens = word_tokenize(text)\n",
        "    tagged = pos_tag(tokens)\n",
        "    lemmatized = [\n",
        "        resolve_us_token(\n",
        "            lemmatizer.lemmatize(token, get_wordnet_pos(pos)),\n",
        "            pos\n",
        "        )\n",
        "        for token, pos in tagged\n",
        "    ]\n",
        "    # Lowercase AFTER lemmatization and US resolution\n",
        "    lemmatized_lower = [token.lower() for token in lemmatized]\n",
        "    return lemmatized_lower\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def merge_financial_tokens(tokens):\n",
        "    \"\"\"Merges adjacent financial tokens ($, numbers, %, tickers) with corrected logic.\"\"\"\n",
        "    merged = []\n",
        "    i = 0\n",
        "    # print(f\"  DEBUG MERGE Input: {tokens}\") # Optional debug\n",
        "    while i < len(tokens):\n",
        "        current_token = tokens[i]\n",
        "        next_token = tokens[i + 1] if i + 1 < len(tokens) else None\n",
        "        merged_flag = False # Flag to track if a merge happened in this iteration\n",
        "\n",
        "        if next_token:\n",
        "            # Try merging conditions IN ORDER OF PRECEDENCE/SPECIFICITY\n",
        "\n",
        "            # Merge $ + comma-formatted number (e.g., \"$\" + \"100,000\" â†’ \"$100000\")\n",
        "            if current_token == \"$\" and re.match(r'^\\d{1,3}(,\\d{3})+(\\.\\d+)?$', next_token):\n",
        "                cleaned_num = next_token.replace(',', '')\n",
        "                merged.append(f\"${cleaned_num}\")\n",
        "                i += 2; merged_flag = True # Advance past both tokens\n",
        "\n",
        "            # Merge $ + number (e.g., \"$\" + \"100\" -> \"$100\")\n",
        "            elif current_token == \"$\" and re.match(r'^\\d+(\\.\\d+)?$', next_token):\n",
        "                 merged.append(f\"${next_token}\")\n",
        "                 i += 2; merged_flag = True # Advance past both tokens\n",
        "\n",
        "            # Merge $ + shorthand amount (e.g., \"$\" + \"30k\" â†’ \"$30k\")\n",
        "            # Check this BEFORE $ + ticker to avoid capturing '$k' etc.\n",
        "            elif current_token == \"$\" and re.match(r'^\\d+(\\.\\d+)?[kKmMbB]$', next_token, re.IGNORECASE): # Added re.IGNORECASE for k/m/b\n",
        "                merged.append(f\"${next_token.lower()}\") # Standardize shorthand to lowercase\n",
        "                i += 2; merged_flag = True # Advance past both tokens\n",
        "\n",
        "            # Merge number + % (e.g., \"5\" + \"%\" â†’ \"5%\")\n",
        "            # Ensure the first token is purely numeric before checking for %\n",
        "            elif re.match(r'^\\d+(\\.\\d+)?$', current_token) and next_token == \"%\":\n",
        "                merged.append(f\"{current_token}%\")\n",
        "                i += 2; merged_flag = True # Advance past both tokens\n",
        "\n",
        "            # Merge $ + ticker (e.g., \"$\" + \"aapl\" â†’ \"$aapl\") - handles only lowercase tickers now\n",
        "            # Ensure it's not just '$' followed by a regular word or stopword\n",
        "            elif current_token == \"$\" and re.match(r'^[a-z]{1,7}$', next_token) and next_token not in custom_stopwords and len(next_token)>1:\n",
        "                 merged.append(f\"${next_token}\")\n",
        "                 i += 2; merged_flag = True # Advance past both tokens\n",
        "\n",
        "        # If no merge occurred involving the current token\n",
        "        if not merged_flag:\n",
        "            # Clean stand-alone comma-formatted numbers if they weren't part of a merge\n",
        "            if re.match(r'^\\d{1,3}(,\\d{3})+(\\.\\d+)?$', current_token):\n",
        "                 cleaned_num = current_token.replace(',', '')\n",
        "                 merged.append(cleaned_num)\n",
        "            # Otherwise, just append the current token as is\n",
        "            else:\n",
        "                 merged.append(current_token)\n",
        "            # Advance by only one token since no merge happened (or only cleaning occurred)\n",
        "            i += 1\n",
        "\n",
        "    # print(f\"  DEBUG MERGE Output: {merged}\") # Optional debug\n",
        "    return merged\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Stopword removal for ML (keeps numbers, $, %, tickers, etc.)\n",
        "def remove_stopwords_ml(tokens, stopwords_set):\n",
        "    \"\"\"Removes stopwords while keeping relevant financial/numeric tokens for ML.\"\"\"\n",
        "    cleaned_tokens = []\n",
        "    for token in tokens:\n",
        "        # Basic stopword check (case-insensitive, as tokens are lowercased)\n",
        "        if token in stopwords_set:\n",
        "            continue\n",
        "\n",
        "        # Keep merged dollar amounts (e.g., \"$100000\", \"$30k\")\n",
        "        elif re.match(r'^\\\\$\\\\d+(\\\\.\\\\d+)?[kKmMbB]?$', token):\n",
        "             cleaned_tokens.append(token)\n",
        "\n",
        "        # Keep percentages like \"5%\", \"2.5%\"\n",
        "        elif re.match(r'^\\\\d+(\\\\.\\\\d+)?%$', token):\n",
        "            cleaned_tokens.append(token)\n",
        "\n",
        "        # Keep merged stock tickers like \"$aapl\" (lowercase only)\n",
        "        elif re.match(r'^\\\\$[a-z]{1,7}$', token): # Adjusted regex\n",
        "            cleaned_tokens.append(token)\n",
        "\n",
        "        # Keep numbers (already cleaned of commas by merge function)\n",
        "        elif re.match(r'^[+-]?\\d+(\\.\\d+)?$', token):\n",
        "            cleaned_tokens.append(token)\n",
        "\n",
        "        # Keep alphanumeric words, unders_cored jargon, potentially tickers without '$'\n",
        "        # Ensure it contains at least one letter to filter out pure punctuation/symbols missed earlier\n",
        "        elif re.match(r'^[a-zA-Z0-9_-]*[a-zA-Z]+[a-zA-Z0-9_-]*$', token):\n",
        "            cleaned_tokens.append(token)\n",
        "\n",
        "        # else: skip remaining punctuation, symbols, or malformed tokens\n",
        "    return cleaned_tokens\n",
        "\n",
        "# Stopword removal for Lexicon (keeps only lower alpha/underscore words)\n",
        "def remove_stopwords_lexicon(tokens, stopwords_set):\n",
        "    \"\"\"Removes stopwords, keeping only alphabetic/underscore tokens AND key financial symbols for Lexicon.\"\"\"\n",
        "    cleaned_tokens = []\n",
        "    for token in tokens:\n",
        "        # Basic stopword check\n",
        "        if token in stopwords_set:\n",
        "            continue\n",
        "\n",
        "        # Keep specific merged dollar amounts (e.g., \"$100000\", \"$30k\")\n",
        "        elif re.match(r'^\\$\\d+(\\.\\d+)?[kKmMbB]?$', token):\n",
        "             cleaned_tokens.append(token)\n",
        "\n",
        "        # Keep specific percentages like \"5%\", \"2.5%\"\n",
        "        elif re.match(r'^\\d+(\\.\\d+)?%$', token):\n",
        "            cleaned_tokens.append(token)\n",
        "\n",
        "        # Keep specific merged stock tickers like \"$aapl\" (lowercase only)\n",
        "        elif re.match(r'^\\$[a-z]{1,7}$', token):\n",
        "            cleaned_tokens.append(token)\n",
        "\n",
        "        # Keep original condition: lower alpha/underscore words (includes underscore_jargon)\n",
        "        elif re.match(r'^[a-z_]+$', token):\n",
        "             cleaned_tokens.append(token)\n",
        "\n",
        "        # else: skip everything else (standalone numbers, other symbols, punctuation)\n",
        "\n",
        "    return cleaned_tokens\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess_for_finbert_pipeline(text):\n",
        "    \"\"\"Pipeline for generating clean text suitable for FinBERT.\"\"\"\n",
        "    # FinBERT generally prefers closer-to-original text,\n",
        "    # so we only apply initial cleaning. No jargon replacement needed.\n",
        "    cleaned_text = initial_clean(text)\n",
        "    return cleaned_text\n",
        "\n",
        "def preprocess_for_lexicon_pipeline(text, jargon_dict, stopwords_set):\n",
        "    \"\"\"Pipeline for generating tokens for lexicon-based analysis.\"\"\"\n",
        "    # 1. Initial clean (HTML, markdown, URLs, contractions)\n",
        "    cleaned_text = initial_clean(text)\n",
        "    # 2. Replace jargon using the sentiment-specific dictionary\n",
        "    jargon_replaced_text = replace_jargon(cleaned_text, jargon_dict)\n",
        "    # 3. Further cleanups specific to tokenization (e.g., slashes)\n",
        "    jargon_replaced_text = re.sub(r'[/]', ' ', jargon_replaced_text)\n",
        "    jargon_replaced_text = re.sub(r'\\\\s+', ' ', jargon_replaced_text).strip()\n",
        "    # 4. Tokenize, lemmatize, lowercase\n",
        "    lemmatized_tokens = tokenize_lemmatize_lowercase(jargon_replaced_text)\n",
        "    # 5. Merge financial tokens\n",
        "    merged_tokens = merge_financial_tokens(lemmatized_tokens)\n",
        "    # 6. Remove stopwords using the lexicon-specific filter\n",
        "    final_tokens = remove_stopwords_lexicon(merged_tokens, stopwords_set)\n",
        "    return final_tokens\n",
        "\n",
        "def preprocess_for_ml_pipeline(text, jargon_dict, stopwords_set):\n",
        "    \"\"\"Pipeline for generating tokens for traditional ML models.\"\"\"\n",
        "    # 1. Initial clean (HTML, markdown, URLs, contractions)\n",
        "    cleaned_text = initial_clean(text)\n",
        "    # 2. Replace jargon using the comprehensive ML dictionary\n",
        "    jargon_replaced_text = replace_jargon(cleaned_text, jargon_dict)\n",
        "    # 3. Further cleanups specific to tokenization (e.g., slashes)\n",
        "    jargon_replaced_text = re.sub(r'[/]', ' ', jargon_replaced_text)\n",
        "    jargon_replaced_text = re.sub(r'\\\\s+', ' ', jargon_replaced_text).strip()\n",
        "    # 4. Tokenize, lemmatize, lowercase\n",
        "    lemmatized_tokens = tokenize_lemmatize_lowercase(jargon_replaced_text)\n",
        "    # 5. Merge financial tokens\n",
        "    merged_tokens = merge_financial_tokens(lemmatized_tokens)\n",
        "    # 6. Remove stopwords using the ML-specific filter\n",
        "    final_tokens = remove_stopwords_ml(merged_tokens, stopwords_set)\n",
        "    return final_tokens\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_all_files():\n",
        "    if not os.path.exists(filtered_posts_dir):\n",
        "        print(f\"Error: Filtered posts directory does not exist: {filtered_posts_dir}\")\n",
        "        return\n",
        "\n",
        "    # Ensure output directories exist (using updated variables)\n",
        "    os.makedirs(ml_data_dir, exist_ok=True)\n",
        "    os.makedirs(finbert_data_dir, exist_ok=True)\n",
        "    os.makedirs(lexicon_data_dir, exist_ok=True)\n",
        "\n",
        "    files = [f for f in os.listdir(filtered_posts_dir) if f.endswith('.json') and f.startswith('filtered_r_')]\n",
        "    if not files:\n",
        "        print(f\"No 'filtered_r_*.json' files found in {filtered_posts_dir}\")\n",
        "        return\n",
        "\n",
        "    print(f\"Found {len(files)} files in {filtered_posts_dir}. Processing...\")\n",
        "\n",
        "    for file in files:\n",
        "        input_filepath = os.path.join(filtered_posts_dir, file)\n",
        "        # Define output paths (using updated dirs and new file prefixes)\n",
        "        ml_output_filepath = os.path.join(ml_data_dir, file.replace(\"filtered_\", \"ml_\"))\n",
        "        finbert_output_filepath = os.path.join(finbert_data_dir, file.replace(\"filtered_\", \"finbert_\"))\n",
        "        lexicon_output_filepath = os.path.join(lexicon_data_dir, file.replace(\"filtered_\", \"lexicon_\"))\n",
        "\n",
        "        try:\n",
        "            with open(input_filepath, 'r', encoding='utf-8') as infile:\n",
        "                data = json.load(infile)\n",
        "\n",
        "            # Use clearer list names\n",
        "            ml_output_data = []   # For ML results + original data\n",
        "            finbert_output_data = [] # For FinBERT results\n",
        "            lexicon_output_data = [] # For Lexicon results\n",
        "\n",
        "            for post in data:\n",
        "                post_id = post.get(\"id\")\n",
        "                if not post_id:\n",
        "                    print(f\"Warning: Post in {file} missing 'id'. Skipping this post.\")\n",
        "                    continue # Skip posts without an ID\n",
        "\n",
        "                # Combine title and selftext safely\n",
        "                title = post.get(\"title\", \"\") or \"\"\n",
        "                selftext = post.get(\"selftext\", \"\") or \"\"\n",
        "                combined_text = f\"{title} {selftext}\".strip()\n",
        "\n",
        "                # --- Run the Pipelines ---\n",
        "                # FinBERT Pipeline\n",
        "                text_for_finbert = preprocess_for_finbert_pipeline(combined_text)\n",
        "                finbert_output_data.append({ # Append to renamed list\n",
        "                    \"id\": post_id,\n",
        "                    \"processed_text_finbert\": text_for_finbert\n",
        "                })\n",
        "\n",
        "                # Lexicon Pipeline\n",
        "                lexicon_tokens = preprocess_for_lexicon_pipeline(\n",
        "                    combined_text, sentiment_jargon_dict, custom_stopwords\n",
        "                )\n",
        "                lexicon_output_data.append({ # Append to renamed list\n",
        "                    \"id\": post_id,\n",
        "                    \"processed_tokens_lexicon\": lexicon_tokens\n",
        "                })\n",
        "\n",
        "                # ML Pipeline\n",
        "                ml_tokens = preprocess_for_ml_pipeline(\n",
        "                    combined_text, ml_jargon_dict, custom_stopwords\n",
        "                )\n",
        "\n",
        "                # Store results for main ML data file (ID + ML Tokens ONLY)\n",
        "                ml_entry = {\n",
        "                    \"id\": post_id,\n",
        "                    \"processed_tokens_ml\": ml_tokens\n",
        "                }\n",
        "                ml_output_data.append(ml_entry) # Append the specific entry\n",
        "\n",
        "            # --- Write Outputs ---\n",
        "            # Write main ML data (Original + ML)\n",
        "            if ml_output_data: # Use renamed list\n",
        "                with open(ml_output_filepath, 'w', encoding='utf-8') as outfile: # Use renamed path var\n",
        "                    json.dump(ml_output_data, outfile, ensure_ascii=False, indent=2) # Use renamed list\n",
        "                # Update print statement with new path var and description\n",
        "                print(f\"Saved {len(ml_output_data)} ML token sets from {file} -> {ml_output_filepath}\")\n",
        "            else:\n",
        "                 print(f\"No processable posts found in {file} (all might have been missing IDs).\")\n",
        "\n",
        "            # Write FinBERT processed data separately\n",
        "            if finbert_output_data: # Use renamed list\n",
        "                with open(finbert_output_filepath, 'w', encoding='utf-8') as finbert_outfile: # Use correct path var\n",
        "                   json.dump(finbert_output_data, finbert_outfile, ensure_ascii=False, indent=2) # Use renamed list\n",
        "                # Update print statement with correct path var and description\n",
        "                print(f\"Saved {len(finbert_output_data)} FinBERT texts -> {finbert_output_filepath}\")\n",
        "\n",
        "            # Write Lexicon processed data separately\n",
        "            if lexicon_output_data: # Use renamed list\n",
        "                with open(lexicon_output_filepath, 'w', encoding='utf-8') as lexicon_outfile: # Use correct path var\n",
        "                   json.dump(lexicon_output_data, lexicon_outfile, ensure_ascii=False, indent=2) # Use renamed list\n",
        "                # Update print statement with correct path var and description\n",
        "                print(f\"Saved {len(lexicon_output_data)} Lexicon tokens -> {lexicon_output_filepath}\")\n",
        "\n",
        "\n",
        "        except json.JSONDecodeError as e:\n",
        "             print(f\"Error decoding JSON from {file}: {e}\")\n",
        "        except Exception as e:\n",
        "            print(f\"An unexpected error occurred processing {file}: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc() # Print full traceback for unexpected errors\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 7 files in ../../Data\\Historical Reddit\\Filtered Posts. Processing...\n",
            "Saved 691 ML token sets from filtered_r_bogleheads.json -> ../../Data\\Historical Reddit\\ML_Data\\ml_r_bogleheads.json\n",
            "Saved 691 FinBERT texts -> ../../Data\\Historical Reddit\\FinBERT_Data\\finbert_r_bogleheads.json\n",
            "Saved 691 Lexicon tokens -> ../../Data\\Historical Reddit\\Lexicon_Data\\lexicon_r_bogleheads.json\n",
            "Saved 22142 ML token sets from filtered_r_cryptocurrency.json -> ../../Data\\Historical Reddit\\ML_Data\\ml_r_cryptocurrency.json\n",
            "Saved 22142 FinBERT texts -> ../../Data\\Historical Reddit\\FinBERT_Data\\finbert_r_cryptocurrency.json\n",
            "Saved 22142 Lexicon tokens -> ../../Data\\Historical Reddit\\Lexicon_Data\\lexicon_r_cryptocurrency.json\n",
            "Saved 11471 ML token sets from filtered_r_investing.json -> ../../Data\\Historical Reddit\\ML_Data\\ml_r_investing.json\n",
            "Saved 11471 FinBERT texts -> ../../Data\\Historical Reddit\\FinBERT_Data\\finbert_r_investing.json\n",
            "Saved 11471 Lexicon tokens -> ../../Data\\Historical Reddit\\Lexicon_Data\\lexicon_r_investing.json\n",
            "Saved 2997 ML token sets from filtered_r_options.json -> ../../Data\\Historical Reddit\\ML_Data\\ml_r_options.json\n",
            "Saved 2997 FinBERT texts -> ../../Data\\Historical Reddit\\FinBERT_Data\\finbert_r_options.json\n",
            "Saved 2997 Lexicon tokens -> ../../Data\\Historical Reddit\\Lexicon_Data\\lexicon_r_options.json\n",
            "Saved 8007 ML token sets from filtered_r_stocks.json -> ../../Data\\Historical Reddit\\ML_Data\\ml_r_stocks.json\n",
            "Saved 8007 FinBERT texts -> ../../Data\\Historical Reddit\\FinBERT_Data\\finbert_r_stocks.json\n",
            "Saved 8007 Lexicon tokens -> ../../Data\\Historical Reddit\\Lexicon_Data\\lexicon_r_stocks.json\n",
            "Saved 3541 ML token sets from filtered_r_stock_market.json -> ../../Data\\Historical Reddit\\ML_Data\\ml_r_stock_market.json\n",
            "Saved 3541 FinBERT texts -> ../../Data\\Historical Reddit\\FinBERT_Data\\finbert_r_stock_market.json\n",
            "Saved 3541 Lexicon tokens -> ../../Data\\Historical Reddit\\Lexicon_Data\\lexicon_r_stock_market.json\n",
            "Saved 326 ML token sets from filtered_r_value_investing.json -> ../../Data\\Historical Reddit\\ML_Data\\ml_r_value_investing.json\n",
            "Saved 326 FinBERT texts -> ../../Data\\Historical Reddit\\FinBERT_Data\\finbert_r_value_investing.json\n",
            "Saved 326 Lexicon tokens -> ../../Data\\Historical Reddit\\Lexicon_Data\\lexicon_r_value_investing.json\n",
            "\n",
            "Processing complete.\n"
          ]
        }
      ],
      "source": [
        "process_all_files()\n",
        "print(\"\\nProcessing complete.\") # Optional: confirms when it's done\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
