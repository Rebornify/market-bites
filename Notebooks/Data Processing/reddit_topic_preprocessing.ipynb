{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBU3jmc5xgxl",
        "outputId": "b3a16569-9460-4989-bea2-5d3f241a113d"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import re\n",
        "import html\n",
        "import os\n",
        "import contractions\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import pos_tag\n",
        "import nltk\n",
        "\n",
        "# Ensure NLTK data is available (run this once if needed)\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "# --- Change this line ---\n",
        "except LookupError: # Changed from nltk.downloader.DownloadError\n",
        "    print(\"Downloading NLTK resource: punkt\")\n",
        "    nltk.download('punkt')\n",
        "try:\n",
        "    nltk.data.find('corpora/stopwords')\n",
        "# --- Change this line ---\n",
        "except LookupError: # Changed from nltk.downloader.DownloadError\n",
        "    print(\"Downloading NLTK resource: stopwords\")\n",
        "    nltk.download('stopwords')\n",
        "try:\n",
        "    nltk.data.find('corpora/wordnet')\n",
        "# --- Change this line ---\n",
        "except LookupError: # Changed from nltk.downloader.DownloadError\n",
        "    print(\"Downloading NLTK resource: wordnet\")\n",
        "    nltk.download('wordnet')\n",
        "try:\n",
        "    nltk.data.find('taggers/averaged_perceptron_tagger')\n",
        "# --- Change this line ---\n",
        "except LookupError: # Changed from nltk.downloader.DownloadError\n",
        "    print(\"Downloading NLTK resource: averaged_perceptron_tagger\")\n",
        "    nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Define the project root relative to the current notebook's directory\n",
        "project_root = '../../'\n",
        "\n",
        "# Define paths relative to the project root\n",
        "filtered_posts_dir = os.path.join(project_root, 'Data', 'Historical Reddit', 'Filtered Posts')\n",
        "lda_nmf_data_dir = os.path.join(project_root, 'Data', 'Historical Reddit', 'LDA_NMF_Data')\n",
        "bertopic_data_dir = os.path.join(project_root, 'Data', 'Historical Reddit', 'BERTopic_Data')\n",
        "\n",
        "# Ensure directories exist\n",
        "if not os.path.exists(lda_nmf_data_dir):\n",
        "    os.makedirs(lda_nmf_data_dir, exist_ok=True)\n",
        "if not os.path.exists(bertopic_data_dir):\n",
        "    os.makedirs(bertopic_data_dir, exist_ok=True)\n",
        "\n",
        "print(f\"Filtered posts directory: {filtered_posts_dir}\")\n",
        "print(f\"LDA/NMF Data directory: {lda_nmf_data_dir}\")\n",
        "print(f\"BERTopic Data directory: {bertopic_data_dir}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "htEXk9Aey4YD"
      },
      "outputs": [],
      "source": [
        "# Define the default stop words list\n",
        "default_stopwords = set(stopwords.words('english'))\n",
        "additional_stopwords = {\n",
        "        # Generic conversational noise (Sorted Alphabetically)\n",
        "        'also', 'anyone', 'back', 'bad', 'check', 'come', 'content', 'could', 'everyone', 'even',\n",
        "        'every', 'feel', 'find', 'get', 'give', 'go', 'good', 'great', 'group', 'guy',\n",
        "        'important', 'keep', 'know', 'let', 'like', 'look', 'lot', 'make', 'many', 'may',\n",
        "        'much', 'must', 'never', 'new', 'number', 'one', 'part', 'people', 'really',\n",
        "        'result', 'right', 'say', 'see', 'seem', 'something', 'still', 'thing', 'think',\n",
        "        'try', 'use', 'video', 'want', 'way', 'well', 'would',\n",
        "\n",
        "        # Reddit meta noise (Sorted Alphabetically)\n",
        "        'click', 'comment', 'disclaimer', 'discussion', 'edit', 'general', 'information',\n",
        "        'karma', 'link', 'list', 'meta', 'mod', 'op', 'poll', 'post', 'prior', 'read',\n",
        "        'reddit', 'rule', 'share', 'sub', 'subreddit', 'thread', 'topic', 'user', 'welcome',\n",
        "        'please', # 'please' kept separate as it's less descriptive\n",
        "\n",
        "        # Generic time words (Sorted Alphabetically)\n",
        "        'currently', 'daily', 'day', 'et', 'friday', 'january', 'last', 'month', 'monthly', 'pm',\n",
        "        'recent', 'since', 'time', 'today', 'tomorrow', 'week', 'year', 'yesterday',\n",
        "\n",
        "        # Common actions, verbs, nouns (Sorted Alphabetically)\n",
        "        'account', 'around', 'article', 'data', 'different', 'example', 'move', 'need', 'pay',\n",
        "        'project', 'start', 'take', 'work',\n",
        "\n",
        "        # Vague/common descriptors (Sorted Alphabetically)\n",
        "        'average', 'big', 'high', 'low', 'max', 'medium',\n",
        "\n",
        "        # Conversational noise (Sorted Alphabetically)\n",
        "        'answer', 'ask', 'help', 'hi', 'live', 'open', 'question', 'talk', 'tell', 'you',\n",
        "\n",
        "        # Artifacts (Sorted Alphabetically)\n",
        "        'com', 'dot', 'free', 'inc',\n",
        "\n",
        "        # Potentially add domain-specific noise words? (Be cautious here)\n",
        "        # 'stock', 'market', 'price', 'company', 'trading', 'shares', 'buy', 'sell', 'crypto' # KEEPING THESE OUT FOR NOW\n",
        "    }\n",
        "\n",
        "# Create custom stopwords set FOR LDA/NMF (includes negations if they are in default/additional)\n",
        "custom_stopwords = default_stopwords.copy()\n",
        "custom_stopwords.update(additional_stopwords)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MytBEAaR510f",
        "outputId": "89a32136-89d4-4cd3-b88b-4a0a28f6aedc"
      },
      "outputs": [],
      "source": [
        "def remove_markdown(text):\n",
        "    \"\"\"Removes common markdown formatting (but preserves newlines).\"\"\"\n",
        "    # NOTE: DO NOT remove newlines here anymore! It's done in initial_clean.\n",
        "    # text = re.sub(r'\\\\n+|\\n+', ' ', text)                   # <<-- REMOVED FROM HERE\n",
        "    text = re.sub(r'\\*\\*(.*?)\\*\\*', r'\\1', text)            # Remove bold markdown (**text**)\n",
        "    text = re.sub(r'__(.*?)__', r'\\1', text)                # Remove bold markdown (__text__)\n",
        "    text = re.sub(r'\\*(.*?)\\*', r'\\1', text)                # Remove italic markdown (*text*)\n",
        "    text = re.sub(r'_(.*?)_', r'\\1', text)                  # Remove italic markdown (_text_) - may affect underscores in words\n",
        "    text = re.sub(r'^\\s*#+\\s*(.*?)\\s*#*\\s*$', r'\\1', text, flags=re.MULTILINE) # Remove markdown headers (# Header)\n",
        "    text = re.sub(r'^\\s*>\\s?(.*)', r'\\1', text, flags=re.MULTILINE) # Remove markdown blockquotes (> quote)\n",
        "    text = re.sub(r'\\[(.*?)\\]\\(.*?\\)', r'\\1', text)         # Remove markdown links, keeping the link text ([text](url) -> text)\n",
        "    text = re.sub(r'`{1,3}(.*?)`{1,3}', r'\\1', text, flags=re.DOTALL) # Remove markdown code ticks/fences (`code` or ```code```)\n",
        "    text = re.sub(r'~~(.*?)~~', r'\\1', text)                # Remove markdown strikethrough (~~text~~)\n",
        "    text = re.sub(r'^\\s*[\\*\\-\\+]\\s+', '', text, flags=re.MULTILINE) # Remove unordered list markers (*, -, +) at line start\n",
        "    text = re.sub(r'^\\s*\\d+\\.\\s+', '', text, flags=re.MULTILINE) # Remove ordered list markers (1., 2.) at line start\n",
        "    text = re.sub(r'^\\s*[-*_]{3,}\\s*$', '', text, flags=re.MULTILINE) # Remove horizontal rules (---, ***, ___ )\n",
        "\n",
        "    # Do not strip here, final stripping happens in initial_clean\n",
        "    # return text.strip()\n",
        "    return text\n",
        "\n",
        "def replace_jargon(text, jargon_dict):\n",
        "    \"\"\"Replaces phrases from the jargon dict with their standardized form.\"\"\"\n",
        "    for phrase, replacement in jargon_dict.items():\n",
        "        # Use word boundaries to avoid partial matches within words\n",
        "        # Make the pattern case-insensitive\n",
        "        pattern = re.compile(r'\\b' + re.escape(phrase) + r'\\b', re.IGNORECASE)\n",
        "        text = pattern.sub(replacement, text)\n",
        "    return text\n",
        "\n",
        "def initial_clean(text):\n",
        "    \"\"\"Performs initial text cleaning common to most pipelines.\"\"\"\n",
        "    us_placeholder = \"__US_PLACEHOLDER__\"\n",
        "\n",
        "    # --- Original Cleaning Steps FIRST ---\n",
        "    text = re.sub(r'<.*?>', '', text)                       # Remove HTML tags\n",
        "    text = html.unescape(text)                              # Unescape HTML entities\n",
        "    text = html.unescape(text)\n",
        "    text = re.sub(r'\\b[/\\\\]?[ur]/\\w+\\b', '', text)           # Remove r/u/ references\n",
        "    text = remove_markdown(text)                            # Apply general markdown formatting removal\n",
        "    text = re.sub(r'^\\s*[|: -]+\\|?\\s*$\\n?', '', text, flags=re.MULTILINE) # Table separators\n",
        "    text = re.sub(r'^.*\\|(?:.*\\|)+.*$\\n?', '', text, flags=re.MULTILINE) # Table content\n",
        "    text = re.sub(r'http\\S+|www\\.\\S+', '', text)             # Remove URLs\n",
        "\n",
        "    # Step X: Replace U.S. variants AFTER markdown but BEFORE contractions\n",
        "    text = re.sub(r'U\\.S\\.', us_placeholder, text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r'\\bUS\\b', us_placeholder, text)\n",
        "\n",
        "    # Expand contractions (NOW SAFE for U.S. and placeholder)\n",
        "    try:\n",
        "        text = contractions.fix(text)\n",
        "    except Exception as e:\n",
        "        pass # Continue if contraction fixing fails\n",
        "\n",
        "    # --- Final Cleanup ---\n",
        "    text = re.sub(r'\\\\n+|\\n+', ' ', text)                   # Replace newlines with space\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()           # Normalize whitespace\n",
        "\n",
        "    # Step N: Restore \"U.S.\" from the placeholder\n",
        "    text = text.replace(us_placeholder, \"U.S.\") # Restore placeholder\n",
        "\n",
        "    return text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Map Treebank POS tags to WordNet POS tags\n",
        "def get_wordnet_pos(treebank_tag):\n",
        "    \"\"\"Maps NLTK POS tags to WordNet POS tags.\"\"\"\n",
        "    if treebank_tag.startswith('J'): return wordnet.ADJ\n",
        "    elif treebank_tag.startswith('V'): return wordnet.VERB\n",
        "    elif treebank_tag.startswith('N'): return wordnet.NOUN\n",
        "    elif treebank_tag.startswith('R'): return wordnet.ADV\n",
        "    else: return wordnet.NOUN # Default to noun\n",
        "\n",
        "def resolve_us_token(token, pos):\n",
        "    \"\"\"Convert 'U.S.' variants to 'united_states' if tagged as NNP/NNPS.\"\"\"\n",
        "    # Check POS tag and the lowercased token\n",
        "    if pos in [\"NNP\", \"NNPS\"] and token.lower() in [\"us\", \"u.s.\", \"u.s\"]:\n",
        "        return \"united_states\"\n",
        "    return token\n",
        "\n",
        "def tokenize_lemmatize_lowercase(text):\n",
        "    \"\"\"Tokenizes, POS tags, resolves U.S., lemmatizes, lowercases, and cleans quotes.\"\"\"\n",
        "    tokens = word_tokenize(text)\n",
        "    tagged = pos_tag(tokens)\n",
        "\n",
        "    processed_tokens = [\n",
        "        lemmatizer.lemmatize(resolve_us_token(token, pos), get_wordnet_pos(pos)).lower()\n",
        "        for token, pos in tagged\n",
        "    ]\n",
        "\n",
        "    # Clean leading/trailing quotes\n",
        "    # Note: Removed trailing quote strip based on previous tests/logic\n",
        "    cleaned_tokens = [token.lstrip(\"'\") for token in processed_tokens]\n",
        "    final_tokens = [token for token in cleaned_tokens if token] # Filter empty strings\n",
        "\n",
        "    return final_tokens\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def remove_stopwords_topic_model(tokens, stopwords_set):\n",
        "    \"\"\"Removes stopwords, single-character tokens, specified artifacts, and numeric tokens for LDA/NMF.\"\"\"\n",
        "    cleaned_tokens = []\n",
        "    # Use the more robust numeric pattern that handles various formats\n",
        "    numeric_pattern = re.compile(r'^\\s*[+-]?(\\d{1,3}(?:[.,]\\d{3})*|\\d+)(?:[.,]\\d+)?\\s*$')\n",
        "    # Reinstate the full set of artifacts we want removed\n",
        "    punctuation_artifacts = {\"''\", \"'s\", \"``\", \"--\", \"-\", \"\\\\\\\\-\", \"...\", \"`\", \"p.\"}\n",
        "\n",
        "    for token in tokens:\n",
        "        # 1. Skip if stopword\n",
        "        if token in stopwords_set:\n",
        "            continue\n",
        "\n",
        "        # 2. Skip if specific punctuation artifact\n",
        "        if token in punctuation_artifacts:\n",
        "            continue\n",
        "\n",
        "        # 3. Skip if purely numeric using fullmatch\n",
        "        if numeric_pattern.fullmatch(token):\n",
        "             continue\n",
        "\n",
        "        # 4. Keep token ONLY if it's longer than 1 character AND contains at least one word character\n",
        "        # Reinstates the stricter single-character filtering and general content check\n",
        "        if len(token) > 1 and re.search(r'\\w', token):\n",
        "             cleaned_tokens.append(token)\n",
        "\n",
        "    return cleaned_tokens\n",
        "\n",
        "def preprocess_for_bertopic(text):\n",
        "    \"\"\"Pipeline for generating clean text suitable for BERTopic.\"\"\"\n",
        "    # Only apply initial cleaning. Keep text structure.\n",
        "    cleaned_text = initial_clean(text)\n",
        "    # Optional: Minimal further cleanup if needed, but avoid tokenization/lemmatization here\n",
        "    cleaned_text = re.sub(r'[/]', ' ', cleaned_text) # Example: replace slashes\n",
        "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
        "    return cleaned_text\n",
        "\n",
        "def preprocess_for_lda_nmf(text, stopwords_set):\n",
        "    \"\"\"Pipeline for generating tokens for LDA/NMF models.\"\"\"\n",
        "    # 1. Initial clean (HTML, markdown, URLs, contractions)\n",
        "    cleaned_text = initial_clean(text)\n",
        "\n",
        "    # 2. (Optional) Replace jargon - skipping for now\n",
        "    jargon_replaced_text = cleaned_text # Use this line if skipping jargon replacement\n",
        "\n",
        "    # 3. Further cleanups specific to tokenization (e.g., slashes)\n",
        "    jargon_replaced_text = re.sub(r'[/]', ' ', jargon_replaced_text)\n",
        "    jargon_replaced_text = re.sub(r'\\s+', ' ', jargon_replaced_text).strip()\n",
        "\n",
        "    # 4. Tokenize, lemmatize, lowercase\n",
        "    lemmatized_tokens = tokenize_lemmatize_lowercase(jargon_replaced_text)\n",
        "\n",
        "    # 5. Skip financial token merging (if applicable)\n",
        "\n",
        "    # 6. Remove stopwords aggressively using the topic-model-specific filter\n",
        "    final_tokens = remove_stopwords_topic_model(lemmatized_tokens, stopwords_set)\n",
        "    return final_tokens\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_all_files():\n",
        "    if not os.path.exists(filtered_posts_dir):\n",
        "        print(f\"Error: Filtered posts directory does not exist: {filtered_posts_dir}\")\n",
        "        return\n",
        "\n",
        "    lda_nmf_data_dir = os.path.join(project_root, 'Data', 'Historical Reddit', 'LDA_NMF_Data')\n",
        "    bertopic_data_dir = os.path.join(project_root, 'Data', 'Historical Reddit', 'BERTopic_Data')\n",
        "\n",
        "    # Ensure output directories exist\n",
        "    os.makedirs(lda_nmf_data_dir, exist_ok=True)\n",
        "    os.makedirs(bertopic_data_dir, exist_ok=True)\n",
        "\n",
        "    print(f\"LDA/NMF Data directory: {lda_nmf_data_dir}\")\n",
        "    print(f\"BERTopic Data directory: {bertopic_data_dir}\")\n",
        "\n",
        "\n",
        "    files = [f for f in os.listdir(filtered_posts_dir) if f.endswith('.json') and f.startswith('filtered_r_')]\n",
        "    if not files:\n",
        "        print(f\"No 'filtered_r_*.json' files found in {filtered_posts_dir}\")\n",
        "        return\n",
        "\n",
        "    print(f\"\\nFound {len(files)} files in {filtered_posts_dir}. Processing for Topic Modeling...\")\n",
        "\n",
        "    for file in files:\n",
        "        input_filepath = os.path.join(filtered_posts_dir, file)\n",
        "        lda_nmf_output_filepath = os.path.join(lda_nmf_data_dir, file.replace(\"filtered_\", \"lda_nmf_\"))\n",
        "        bertopic_output_filepath = os.path.join(bertopic_data_dir, file.replace(\"filtered_\", \"bertopic_\"))\n",
        "\n",
        "        try:\n",
        "            with open(input_filepath, 'r', encoding='utf-8') as infile:\n",
        "                data = json.load(infile)\n",
        "\n",
        "            lda_nmf_output_data = []\n",
        "            bertopic_output_data = []\n",
        "\n",
        "            for post in data:\n",
        "                post_id = post.get(\"id\")\n",
        "                if not post_id:\n",
        "                    print(f\"Warning: Post in {file} missing 'id'. Skipping this post.\")\n",
        "                    continue\n",
        "\n",
        "                title = post.get(\"title\", \"\") or \"\"\n",
        "                selftext = post.get(\"selftext\", \"\") or \"\"\n",
        "                combined_text = f\"{title} {selftext}\".strip()\n",
        "\n",
        "                # --- Run the BERTopic Pipeline ---\n",
        "                text_for_bertopic = preprocess_for_bertopic(combined_text)\n",
        "                bertopic_output_data.append({\n",
        "                    \"id\": post_id,\n",
        "                    \"processed_text_bertopic\": text_for_bertopic\n",
        "                    # Optionally include original text or other metadata if needed later\n",
        "                    # \"original_title\": title,\n",
        "                    # \"original_selftext\": selftext\n",
        "                })\n",
        "\n",
        "                # --- Run the LDA/NMF Pipeline ---\n",
        "                lda_nmf_tokens = preprocess_for_lda_nmf(combined_text, custom_stopwords)\n",
        "                lda_nmf_output_data.append({\n",
        "                    \"id\": post_id,\n",
        "                    \"processed_tokens_lda_nmf\": lda_nmf_tokens\n",
        "                })\n",
        "\n",
        "            # --- Write Outputs ---\n",
        "            # Write LDA/NMF data\n",
        "            if lda_nmf_output_data:\n",
        "                with open(lda_nmf_output_filepath, 'w', encoding='utf-8') as outfile:\n",
        "                    json.dump(lda_nmf_output_data, outfile, ensure_ascii=False, indent=2)\n",
        "                print(f\"Saved {len(lda_nmf_output_data)} LDA/NMF token sets from {file} -> {lda_nmf_output_filepath}\")\n",
        "\n",
        "            # Write BERTopic data\n",
        "            if bertopic_output_data:\n",
        "                with open(bertopic_output_filepath, 'w', encoding='utf-8') as outfile:\n",
        "                   json.dump(bertopic_output_data, outfile, ensure_ascii=False, indent=2)\n",
        "                print(f\"Saved {len(bertopic_output_data)} BERTopic texts from {file} -> {bertopic_output_filepath}\")\n",
        "\n",
        "            if not lda_nmf_output_data and not bertopic_output_data:\n",
        "                 print(f\"No processable posts found in {file} (all might have been missing IDs).\")\n",
        "\n",
        "\n",
        "        except json.JSONDecodeError as e:\n",
        "             print(f\"Error decoding JSON from {file}: {e}\")\n",
        "        except Exception as e:\n",
        "            print(f\"An unexpected error occurred processing {file}: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "process_all_files()\n",
        "print(\"\\nProcessing complete.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # --- Test Cases ---\n",
        "# test_cases = [\n",
        "#     # Basic Cleaning\n",
        "#     (\"  Test &amp; example with   extra spaces. Don't forget!  \", \"Test & example with extra spaces. Do not forget!\"),\n",
        "#     (\"Visit www.example.com or https://test.org/path?q=1\", \"Visit or\"),\n",
        "#     (\"Here's some <b>bold</b> text and <a href='#'>a link</a>.\", \"Here is some bold text and a link.\"),\n",
        "#     (\"It&apos;s double encoded: &amp;amp;\", \"It is double encoded: &\"),\n",
        "\n",
        "#     # Markdown Specific\n",
        "#     (\"# Header\\n*italic* and **bold** ~~strike~~ `code`\", \"Header italic and bold strike code\"),\n",
        "#     (\"A line.\\n\\n---\\n\\nAnother line.\", \"A line. Another line.\"), # Horizontal rule\n",
        "#     (\"> Blockquote here\", \"Blockquote here\"),\n",
        "#     (\"Check [this link](http://example.com) and ![alt text](img.jpg)\", \"Check this link and\"),\n",
        "\n",
        "#     # Markdown Tables\n",
        "#     (\"Text before.\\n| Header 1 | Header 2 |\\n|---|---|\\n| Cell 1 | Cell 2 |\\nText after.\", \"Text before. Text after.\"),\n",
        "#     (\"Table at start:\\n| A | B |\\n|---|---|\\n| 1 | 2 |\", \"Table at start:\"),\n",
        "#     (\"| C | D |\\n|---|---|\\n| 3 | 4 |\\nEnd table.\", \"End table.\"),\n",
        "#     (\"No table here, just | pipes | in text.\", \"No table here, just | pipes | in text.\"), # Should not remove this\n",
        "#     (\"Text\\n\\n| Head |\\n|:--|\\n| Val |\\n\\nMore text\", \"Text More text\"), # Table with surrounding newlines\n",
        "\n",
        "#     # Edge Cases & Combinations\n",
        "#     (\"It's <br>broken &amp; messy www.site.com\\n\\n| T1 | T2 |\\n|--|--|\\n| V1 | V2 |\", \"It is broken & messy\"), # Combo\n",
        "#     (\"\", \"\"), # Empty string\n",
        "#     (\"  \", \"\"), # Only whitespace\n",
        "#     (\"`code` don't remove &lt;tag&gt;\", \"code do not remove <tag>\"), # Mixed entities/markdown/contractions\n",
        "#     (\"Line with / slash\", \"Line with / slash\"), # Slash preserved (assuming slash replacement is removed)\n",
        "\n",
        "#     # Test case similar to the problematic Bogleheads post (simplified)\n",
        "#     (\"First $100,000 saved!\\n\\n| Salary | Date |\\n|---|---|\\n| $32,000 | 12/31/2015 |\\n| $60,000 | 12/31/2016 |\\n\\nTotal Growth $27,000\", \"First $100,000 saved! Total Growth $27,000\") # Note: Symbols ($ ,) are removed later\n",
        "# ]\n",
        "\n",
        "# # --- Run Tests ---\n",
        "# print(\"--- Running initial_clean Tests ---\")\n",
        "# passed = 0\n",
        "# failed = 0\n",
        "# for i, (input_text, expected_output) in enumerate(test_cases):\n",
        "#     actual_output = initial_clean(input_text)\n",
        "#     if actual_output == expected_output:\n",
        "#         print(f\"[PASS] Test Case {i+1}\")\n",
        "#         passed += 1\n",
        "#     else:\n",
        "#         print(f\"[FAIL] Test Case {i+1}\")\n",
        "#         print(f\"  Input:    '{input_text}'\")\n",
        "#         print(f\"  Expected: '{expected_output}'\")\n",
        "#         print(f\"  Actual:   '{actual_output}'\")\n",
        "#         failed += 1\n",
        "# print(\"--- Test Summary ---\")\n",
        "# print(f\"Passed: {passed}\")\n",
        "# print(f\"Failed: {failed}\")\n",
        "# print(\"--- End Tests ---\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# lda_nmf_test_cases = [\n",
        "#     # Basic cases\n",
        "#     (\"He said 'hello'\", [\"hello\"]),          # Leading ' removed, 'hello' kept\n",
        "\n",
        "#     # Previous number tests (should still pass if number logic is correct)\n",
        "#     (\"Price is 15.000.000 dollars\", [\"price\", \"dollar\"]),\n",
        "#     (\"Gained +300.000 points\", [\"gained\", \"point\"]),\n",
        "#     (\"Increase of 1,500,000\", [\"increase\"]),\n",
        "#     (\"Profit: $50,000.50\", [\"profit\"]),\n",
        "#     (\"Down -50 points\", [\"point\"]),\n",
        "#     (\"Only 1 left\", [\"leave\"]),\n",
        "#     (\"Number 000\", [\"number\"]),\n",
        "#     (\"First $100,000 saved!\", [\"first\", \"save\"]),\n",
        "#     (\"Stock ABC went up by 5.5% to 123.456\", [\"stock\", \"abc\"]),\n",
        "#     (\"go 123 45.678 hello\", [\"hello\"]),\n",
        "\n",
        "#     # New tests for specific punctuation/symbols (with updated expectations)\n",
        "#     (\"A quick break -- then continue\", [\"quick\", \"break\", \"continue\"]), # Expect -- removed\n",
        "#     (\"Check item \\\\- it is important\", [\"check\", \"item\", \"important\"]), # Expect \\- removed\n",
        "#     (\"Check item - it is important\", [\"check\", \"item\", \"important\"]),      # Expect - removed\n",
        "#     (\"Wait ... what happened?\", [\"wait\", \"happen\"]),                       # Expect ... removed, happened -> happen\n",
        "#     (\"He said ''wait''\", [\"wait\"]),                                        # Expect `` removed (was '')\n",
        "#     (\"She mentioned \\\"stop\\\"\", [\"mention\", \"stop\"]),                       # Expect `` removed (was \"), mentioned -> mention\n",
        "#     (\"It's a test\", [\"test\"]),                                             # Expect 's removed\n",
        "#     (\"Symbols like : or ; or _\", [\"symbols\"]),\n",
        "\n",
        "#     # --- NEW TEST CASES FOR SINGLE LETTERS ---\n",
        "#     (\"Go to r/subreddit\", []),                 # Expect 'r' removed (assuming 'go', 'to' are stopwords)\n",
        "#     (\"Can u tell me?\", []),                         # Expect 'u' removed (assuming 'can', 'me' are stopwords)\n",
        "#     (\"See page p. 5\", [\"page\"]),                   # Expect 'p' removed (assuming '.' and '5' are removed) - 'see' kept if not stopword\n",
        "#     (\"Can u see p. 1 of r/stocks?\", []),    # Combine 'u', 'p', 'r' (assuming 'can', 'of', '.' '1' are removed/stopwords)\n",
        "#     (\"a b c d e f g h i j k l m n o p q r s t u v w x y z\", []), # Keep single letters ONLY if they aren't stopwords ('a', 'i', 'o' likely are, 'c', 'e' maybe?) AND not 'r', 'u', 'p'\n",
        "\n",
        "#     # --- NEW TEST CASES FOR U.S. / you.s ---\n",
        "#     (\"The U.S. economy is strong.\", [\"united_states\", \"economy\", \"strong\"]), # Expected resolution\n",
        "#     (\"He visited the U.S.\", [\"visit\", \"united_states\"]), # Expected resolution\n",
        "#     (\"Focus on u.s. stocks\", [\"focus\", \"united_states\", \"stock\"]), # Lowercase input\n",
        "#     (\"U.S. dollars are used\", [\"united_states\", \"dollar\"]), # With another word\n",
        "#     (\"Compare U.S. and China\", [\"compare\", \"united_states\", \"china\"]), # Plural context?\n",
        "#     (\"Is this related to us?\", [\"related\"]), # Pronoun 'us', should be removed by stopwords\n",
        "# ]\n",
        "\n",
        "# # --- Test Runner Code ---\n",
        "# print(\"--- Running preprocess_for_lda_nmf Number Tests ---\")\n",
        "# passed_lda = 0\n",
        "# failed_lda = 0\n",
        "# for i, (input_text, expected_tokens) in enumerate(lda_nmf_test_cases):\n",
        "#     # Pass the custom_stopwords set defined earlier in the notebook\n",
        "#     actual_tokens = preprocess_for_lda_nmf(input_text, custom_stopwords)\n",
        "#     # Sort for comparison consistency\n",
        "#     if sorted(actual_tokens) == sorted(expected_tokens):\n",
        "#         print(f\"[PASS] LDA/NMF Test Case {i+1}\")\n",
        "#         passed_lda += 1\n",
        "#     else:\n",
        "#         print(f\"[FAIL] LDA/NMF Test Case {i+1}\")\n",
        "#         print(f\"  Input:    '{input_text}'\")\n",
        "#         print(f\"  Expected: {sorted(expected_tokens)}\")\n",
        "#         print(f\"  Actual:   {sorted(actual_tokens)}\")\n",
        "#         failed_lda += 1\n",
        "\n",
        "# print(\"--- LDA/NMF Test Summary ---\")\n",
        "# print(f\"Passed: {passed_lda}\")\n",
        "# print(f\"Failed: {failed_lda}\")\n",
        "# print(\"--- End LDA/NMF Tests ---\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
