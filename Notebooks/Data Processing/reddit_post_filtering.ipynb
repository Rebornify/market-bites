{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lr-GF1ECt4PV"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import json\n",
        "import os\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive_base_path = '/content/drive/MyDrive/IS450 Project/Data/Historical Reddit/'\n",
        "input_dir = os.path.join(drive_base_path, 'Raw Submissions')\n",
        "output_dir = os.path.join(drive_base_path, 'Filtered Posts')\n"
      ],
      "metadata": {
        "id": "udfXcHd9u8-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_reddit_posts(input_file, output_file, min_score=10):\n",
        "    \"\"\"\n",
        "    Filters Reddit posts: only keeps posts with score greater than min_score,\n",
        "    and with meaningful selftext (non-empty, not \"[removed]\" or \"[deleted]\").\n",
        "    \"\"\"\n",
        "    filtered_posts = []\n",
        "    posts_read = 0\n",
        "    posts_kept = 0\n",
        "\n",
        "    print(f\"Filtering {input_file}...\")\n",
        "    if not os.path.exists(input_file):\n",
        "        print(f\"Error: Input file not found: {input_file}\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
        "            for line in f:\n",
        "                posts_read += 1\n",
        "                try:\n",
        "                    post = json.loads(line)  # Load each line as JSON\n",
        "                    score = post.get(\"score\", 0)\n",
        "                    selftext = post.get(\"selftext\", \"\").strip()\n",
        "\n",
        "                    if (score is not None and score > min_score and\n",
        "                        selftext and\n",
        "                        selftext.lower() not in [\"[removed]\", \"[deleted]\"]):\n",
        "\n",
        "                        filtered_posts.append({\n",
        "                            \"id\": post.get(\"id\", \"N/A\"),\n",
        "                            \"created_utc\": post.get(\"created_utc\", 0),\n",
        "                            \"score\": score,\n",
        "                            \"title\": post.get(\"title\", \"\"),\n",
        "                            \"selftext\": post.get(\"selftext\", \"\"), # Keep original selftext\n",
        "                            \"permalink\": f\"https://www.reddit.com{post.get('permalink', '')}\"\n",
        "                        })\n",
        "                        posts_kept += 1\n",
        "                except json.JSONDecodeError:\n",
        "                    print(f\"Warning: Skipping corrupted JSON line {posts_read} in {input_file}\")\n",
        "                    continue # Skip corrupted lines\n",
        "                except Exception as e:\n",
        "                    print(f\"Warning: Error processing line {posts_read} in {input_file}: {e}\")\n",
        "                    continue\n",
        "\n",
        "        output_dir = os.path.dirname(output_file)\n",
        "        if not os.path.exists(output_dir):\n",
        "            print(f\"Creating output directory: {output_dir}\")\n",
        "            os.makedirs(output_dir)\n",
        "\n",
        "        with open(output_file, \"w\", encoding=\"utf-8\") as f_out:\n",
        "            # Save as a JSON array (list of objects) instead of line-delimited\n",
        "            json.dump(filtered_posts, f_out, indent=2, ensure_ascii=False)\n",
        "\n",
        "        print(f\"âœ… Read {posts_read} posts. Filtered {posts_kept} meaningful posts saved to {output_file}\\n\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Input file not found during open: {input_file}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred while processing {input_file}: {e}\")\n"
      ],
      "metadata": {
        "id": "Bu85B9Sst98m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure output directory exists\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "    print(f\"Created output directory: {output_dir}\")\n",
        "\n",
        "# Add the base names of your raw submission files here\n",
        "files_to_process = [\n",
        "    \"stocks_submissions\",\n",
        "    \"StockMarket_submissions\",\n",
        "    \"ValueInvesting_submissions\",\n",
        "    \"investing_submissions\",\n",
        "    \"Bogleheads_submissions\",\n",
        "    \"options_submissions\",\n",
        "    \"CryptoCurrency_submissions\"\n",
        "]\n",
        "\n",
        "# You can adjust min_score per subreddit if needed, or use a default\n",
        "default_min_score = 10\n",
        "score_overrides = {\n",
        "    \"investing_submissions\": 50,\n",
        "    \"CryptoCurrency_submissions\": 50\n",
        "}\n",
        "\n",
        "print(f\"Starting filtering process...\")\n",
        "print(f\"Input directory: {input_dir}\")\n",
        "print(f\"Output directory: {output_dir}\")\n",
        "\n",
        "for base_filename in files_to_process:\n",
        "    input_file_path = None\n",
        "    possible_extensions = ['', '.json', '.ndjson', '.jsonl'] # Add more if needed\n",
        "    for ext in possible_extensions:\n",
        "        potential_path = os.path.join(input_dir, f\"{base_filename}{ext}\")\n",
        "        if os.path.exists(potential_path):\n",
        "            input_file_path = potential_path\n",
        "            break # Found the file\n",
        "\n",
        "    if input_file_path is None:\n",
        "        print(f\"Warning: Input file for '{base_filename}' not found in {input_dir}. Skipping.\")\n",
        "        continue\n",
        "\n",
        "    # Construct output path\n",
        "    output_file_path = os.path.join(output_dir, f\"filtered_{base_filename}.json\") # Standardize output to .json\n",
        "\n",
        "    # Determine minimum score\n",
        "    min_score = score_overrides.get(base_filename, default_min_score)\n",
        "\n",
        "    # Run the filter function\n",
        "    filter_reddit_posts(input_file_path, output_file_path, min_score=min_score)\n",
        "\n",
        "print(\"Filtering complete.\")\n"
      ],
      "metadata": {
        "id": "oNHEeKdluCbk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}