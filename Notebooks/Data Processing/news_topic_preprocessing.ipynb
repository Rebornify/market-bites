{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eVQCfFPH-q77"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "# Use standard NLTK resource IDs\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "85OO_9IbiVR6"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import html\n",
    "import os\n",
    "import contractions\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "import nltk\n",
    "import pandas as pd\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Define the root path of your project on Google Drive\n",
    "project_root = '/content/drive/MyDrive/IS450 Project'\n",
    "\n",
    "# Define paths relative to the project root\n",
    "filtered_news_dir = os.path.join(project_root, 'Data', 'Historical News', 'Supplementary', 'unique_financial_news.xlsx')\n",
    "lda_nmf_data_dir = os.path.join(project_root, 'Data', 'Historical News', 'LDA_NMF_Data')\n",
    "bertopic_data_dir = os.path.join(project_root, 'Data', 'Historical News', 'BERTopic_Data')\n",
    "\n",
    "# Ensure directories exist\n",
    "if not os.path.exists(lda_nmf_data_dir):\n",
    "    os.makedirs(lda_nmf_data_dir, exist_ok=True)\n",
    "if not os.path.exists(bertopic_data_dir):\n",
    "    os.makedirs(bertopic_data_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Filtered posts directory: {filtered_news_dir}\")\n",
    "print(f\"LDA/NMF Data directory: {lda_nmf_data_dir}\")\n",
    "print(f\"BERTopic Data directory: {bertopic_data_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fKFKHop78hS4"
   },
   "outputs": [],
   "source": [
    "# Define the default stop words list\n",
    "default_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "# Add domain-specific stopwords\n",
    "# additional_stopwords = {\"stock\", \"company\", \"inc\", \"said\", \"year\", \"new\", \"q\", \"u\", \"today\", \"like\"}\n",
    "# additional_stopwords = {}\n",
    "# custom_stopwords = default_stopwords.copy()\n",
    "# custom_stopwords.update(additional_stopwords)\n",
    "\n",
    "def initial_clean(text):\n",
    "    \"\"\"Performs initial text cleaning common to most pipelines.\"\"\"\n",
    "    text = re.sub(r'<.*?>', '', text)                       # Remove any remaining HTML tags (e.g., <b>, <i>) BEFORE unescaping\n",
    "\n",
    "    text = html.unescape(text)                              # Unescape HTML entities like &amp; -> &\n",
    "    text = html.unescape(text)                              # Double unescape just in case some entities were doubly encoded\n",
    "\n",
    "    text = re.sub(r'http\\S+|www\\.\\S+', '', text)             # Remove URLs (http/https or www.)\n",
    "    text = text.lower()\n",
    "    try:\n",
    "        text = contractions.fix(text)                       # Expand contractions (e.g., \"don't\" -> \"do not\")\n",
    "    except Exception as e:\n",
    "        # print(f\"Contraction fixing failed for text: {text[:100]}... Error: {e}\") # Optional: uncomment to log errors\n",
    "        pass                                                # Continue if contraction fixing fails for an edge case\n",
    "\n",
    "    text = re.sub(r\"\\b(\\w+)'s\\b\", r\"\\1\", text)     # e.g., \"Amazon's\" → \"Amazon\"\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)     # Remove all non-letter, non-space characters\n",
    "\n",
    "    # --- Final Cleanup ---\n",
    "    text = re.sub(r'\\\\n+|\\n+', ' ', text)                   # Replace literal '\\n' strings and actual newlines with a space *NOW*\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()                # Normalize whitespace (multiple spaces to one space, trim ends)\n",
    "    return text\n",
    "\n",
    "# === Helper functions for truncation detection ===\n",
    "def detect_truncated_title(title):\n",
    "    # Simple heuristic: check if title ends with ellipsis or is unusually short\n",
    "    return isinstance(title, str) and (title.strip().endswith(\"...\") or len(title.strip()) < 5)\n",
    "\n",
    "def detect_truncated_description(description):\n",
    "    # Same idea as above\n",
    "    return isinstance(description, str) and (description.strip().endswith(\"...\") or len(description.strip()) < 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QKNAU7PFmiTw"
   },
   "outputs": [],
   "source": [
    "# Map Treebank POS tags to WordNet POS tags\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    \"\"\"Maps NLTK POS tags to WordNet POS tags.\"\"\"\n",
    "    if treebank_tag.startswith('J'): return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'): return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'): return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'): return wordnet.ADV\n",
    "    else: return wordnet.NOUN # Default to noun\n",
    "\n",
    "def resolve_us_token(token, pos_tag):\n",
    "    \"\"\"Disambiguates 'us' (pronoun) vs 'US' (United States).\"\"\"\n",
    "    # Check the original token's case if needed, but POS tag is usually sufficient\n",
    "    if token.lower() == \"us\" and pos_tag == \"NNP\": # NNP = Proper noun, singular\n",
    "        return \"united_states\"\n",
    "    return token\n",
    "\n",
    "def tokenize_lemmatize_lowercase(text):\n",
    "    \"\"\"Tokenizes, POS tags, lemmatizes, lowercases text, and cleans leading/trailing quotes.\"\"\" # Updated docstring\n",
    "    tokens = word_tokenize(text)\n",
    "    tagged = pos_tag(tokens)\n",
    "    lemmatized = [\n",
    "        resolve_us_token(\n",
    "            lemmatizer.lemmatize(token, get_wordnet_pos(pos)),\n",
    "            pos\n",
    "        )\n",
    "        for token, pos in tagged\n",
    "    ]\n",
    "    # Lowercase AFTER lemmatization and US resolution\n",
    "    lemmatized_lower = [token.lower() for token in lemmatized]\n",
    "\n",
    "    # Define characters to strip (single and double quotes)\n",
    "    cleaned_tokens = [token.lstrip(\"'\") for token in lemmatized_lower]\n",
    "    # Filter out any empty strings that might result from stripping (e.g., if token was just \"'\")\n",
    "    final_tokens = [token for token in cleaned_tokens if token]\n",
    "\n",
    "    return final_tokens # Return the cleaned tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zYfNfhuGmzEz"
   },
   "outputs": [],
   "source": [
    "def remove_stopwords_topic_model(tokens, stopwords_set):\n",
    "    \"\"\"Removes stopwords, single-character tokens, specified artifacts, and numeric tokens for LDA/NMF.\"\"\"\n",
    "    cleaned_tokens = []\n",
    "    # Reinstate the full set of artifacts we want removed\n",
    "    punctuation_artifacts = {\"''\", \"'s\", \"``\", \"--\", \"-\", \"\\\\\\\\-\", \"...\", \"`\", \"p.\"}\n",
    "\n",
    "    for token in tokens:\n",
    "        if token and not token[0].isalnum():\n",
    "          token = token[1:]\n",
    "\n",
    "        # Skip if stopword\n",
    "        if token in stopwords_set:\n",
    "            continue\n",
    "\n",
    "        # Skip if specific punctuation artifact\n",
    "        if token in punctuation_artifacts:\n",
    "            continue\n",
    "\n",
    "        # Keep token ONLY if it's longer than 1 character AND contains at least one word character\n",
    "        # Reinstates the stricter single-character filtering and general content check\n",
    "        if len(token) > 1 and re.search(r'\\w', token):\n",
    "             cleaned_tokens.append(token)\n",
    "\n",
    "    return cleaned_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7sYM53elm5s_"
   },
   "outputs": [],
   "source": [
    "def preprocess_for_lda_nmf(text, stopwords_set):\n",
    "    cleaned_text = initial_clean(text)\n",
    "    lemmatized_tokens = tokenize_lemmatize_lowercase(cleaned_text)\n",
    "    final_tokens = remove_stopwords_topic_model(lemmatized_tokens, stopwords_set)\n",
    "    return final_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yKL49Ex6DjMV"
   },
   "outputs": [],
   "source": [
    "def preprocess_for_bertopic(text):\n",
    "    \"\"\"Pipeline for generating clean text suitable for BERTopic.\"\"\"\n",
    "    # Only apply initial cleaning. Keep text structure.\n",
    "    cleaned_text = initial_clean(text)\n",
    "    # Optional: Minimal further cleanup if needed, but avoid tokenization/lemmatization here\n",
    "    cleaned_text = re.sub(r'[/]', ' ', cleaned_text) # Example: replace slashes\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
    "    return cleaned_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LC77Kq3ZnKap"
   },
   "outputs": [],
   "source": [
    "# === Read input file ===\n",
    "df = pd.read_excel(filtered_news_dir)\n",
    "\n",
    "# === Process and store results ===\n",
    "lda_news = []\n",
    "bertopic_output_data = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    title = row.get(\"title\", \"\")\n",
    "    description = row.get(\"description\", \"\")\n",
    "\n",
    "    truncated_title = detect_truncated_title(title)\n",
    "    truncated_description = detect_truncated_description(description)\n",
    "\n",
    "    # Combine title + description before preprocessing\n",
    "    combined_text = f\"{title} {description}\"\n",
    "\n",
    "    # --- Run the BERTopic Pipeline ---\n",
    "    text_for_bertopic = preprocess_for_bertopic(combined_text)\n",
    "    bertopic_output_data.append({\n",
    "        \"id\": idx,\n",
    "        \"processed_text_bertopic\": text_for_bertopic,\n",
    "        \"truncated_title\": truncated_title,\n",
    "        \"truncated_description\": truncated_description\n",
    "    })\n",
    "\n",
    "    # --- Run the LDA/NMF Pipeline ---\n",
    "    processed_tokens = preprocess_for_lda_nmf(combined_text, default_stopwords)\n",
    "    lda_news.append({\n",
    "        \"id\": idx,\n",
    "        \"processed_text_lda\": processed_tokens,\n",
    "        \"truncated_title\": truncated_title,\n",
    "        \"truncated_description\": truncated_description\n",
    "    })\n",
    "\n",
    "# === Save to JSONL file ===\n",
    "lda_nmf_data_dir = os.path.join(lda_nmf_data_dir, \"lda_news.jsonl\")\n",
    "bertopic_data_dir = os.path.join(lda_nmf_data_dir, \"BERTopic_Data.jsonl\")\n",
    "\n",
    "with open(lda_nmf_data_dir, \"w\", encoding=\"utf-8\") as f_out:\n",
    "    for record in lda_news:\n",
    "        json.dump(record, f_out)\n",
    "        f_out.write(\"\\n\")  # Don't forget newline for JSONL format\n",
    "\n",
    "with open(bertopic_data_dir, \"w\", encoding=\"utf-8\") as f_out:\n",
    "    for record in bertopic_output_data:\n",
    "        json.dump(record, f_out)\n",
    "        f_out.write(\"\\n\")  # Don't forget newline for JSONL format\n",
    "\n",
    "print(f\"✅ Preprocessed data saved to: {lda_nmf_data_dir}\")\n",
    "print(f\"✅ Preprocessed data saved to: {bertopic_data_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vJZSEfUJ67gD"
   },
   "outputs": [],
   "source": [
    "########## Predict Topics for New News Article ################\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "!pip install nltk\n",
    "!pip install contractions\n",
    "!pip install gensim\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "import json\n",
    "import re\n",
    "import html\n",
    "import os\n",
    "import contractions\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from gensim.models import Nmf\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "# --- 0. Methods Needed: ---\n",
    "default_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "def initial_clean(text):\n",
    "    \"\"\"Performs initial text cleaning common to most pipelines.\"\"\"\n",
    "    text = re.sub(r'<.*?>', '', text)                       # Remove any remaining HTML tags (e.g., <b>, <i>) BEFORE unescaping\n",
    "\n",
    "    text = html.unescape(text)                              # Unescape HTML entities like &amp; -> &\n",
    "    text = html.unescape(text)                              # Double unescape just in case some entities were doubly encoded\n",
    "\n",
    "    text = re.sub(r'http\\S+|www\\.\\S+', '', text)             # Remove URLs (http/https or www.)\n",
    "    text = text.lower()\n",
    "    try:\n",
    "        text = contractions.fix(text)                       # Expand contractions (e.g., \"don't\" -> \"do not\")\n",
    "    except Exception as e:\n",
    "        # print(f\"Contraction fixing failed for text: {text[:100]}... Error: {e}\") # Optional: uncomment to log errors\n",
    "        pass                                                # Continue if contraction fixing fails for an edge case\n",
    "\n",
    "    text = re.sub(r\"\\b(\\w+)'s\\b\", r\"\\1\", text)     # e.g., \"Amazon's\" → \"Amazon\"\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)     # Remove all non-letter, non-space characters\n",
    "\n",
    "    # --- Final Cleanup ---\n",
    "    text = re.sub(r'\\\\n+|\\n+', ' ', text)                   # Replace literal '\\n' strings and actual newlines with a space *NOW*\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()                # Normalize whitespace (multiple spaces to one space, trim ends)\n",
    "    return text\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Map Treebank POS tags to WordNet POS tags\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    \"\"\"Maps NLTK POS tags to WordNet POS tags.\"\"\"\n",
    "    if treebank_tag.startswith('J'): return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'): return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'): return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'): return wordnet.ADV\n",
    "    else: return wordnet.NOUN # Default to noun\n",
    "\n",
    "def resolve_us_token(token, pos_tag):\n",
    "    \"\"\"Disambiguates 'us' (pronoun) vs 'US' (United States).\"\"\"\n",
    "    # Check the original token's case if needed, but POS tag is usually sufficient\n",
    "    if token.lower() == \"us\" and pos_tag == \"NNP\": # NNP = Proper noun, singular\n",
    "        return \"united_states\"\n",
    "    return token\n",
    "\n",
    "def tokenize_lemmatize_lowercase(text):\n",
    "    \"\"\"Tokenizes, POS tags, lemmatizes, lowercases text, and cleans leading/trailing quotes.\"\"\" # Updated docstring\n",
    "    tokens = word_tokenize(text)\n",
    "    tagged = pos_tag(tokens)\n",
    "    lemmatized = [\n",
    "        resolve_us_token(\n",
    "            lemmatizer.lemmatize(token, get_wordnet_pos(pos)),\n",
    "            pos\n",
    "        )\n",
    "        for token, pos in tagged\n",
    "    ]\n",
    "    # Lowercase AFTER lemmatization and US resolution\n",
    "    lemmatized_lower = [token.lower() for token in lemmatized]\n",
    "\n",
    "    # Define characters to strip (single and double quotes)\n",
    "    cleaned_tokens = [token.lstrip(\"'\") for token in lemmatized_lower]\n",
    "    # Filter out any empty strings that might result from stripping (e.g., if token was just \"'\")\n",
    "    final_tokens = [token for token in cleaned_tokens if token]\n",
    "\n",
    "    return final_tokens # Return the cleaned tokens\n",
    "\n",
    "def remove_stopwords_topic_model(tokens, stopwords_set):\n",
    "    \"\"\"Removes stopwords, single-character tokens, specified artifacts, and numeric tokens for LDA/NMF.\"\"\"\n",
    "    cleaned_tokens = []\n",
    "    # Reinstate the full set of artifacts we want removed\n",
    "    punctuation_artifacts = {\"''\", \"'s\", \"``\", \"--\", \"-\", \"\\\\\\\\-\", \"...\", \"`\", \"p.\"}\n",
    "\n",
    "    for token in tokens:\n",
    "        if token and not token[0].isalnum():\n",
    "          token = token[1:]\n",
    "\n",
    "        # Skip if stopword\n",
    "        if token in stopwords_set:\n",
    "            continue\n",
    "\n",
    "        # Skip if specific punctuation artifact\n",
    "        if token in punctuation_artifacts:\n",
    "            continue\n",
    "\n",
    "        # Keep token ONLY if it's longer than 1 character AND contains at least one word character\n",
    "        # Reinstates the stricter single-character filtering and general content check\n",
    "        if len(token) > 1 and re.search(r'\\w', token):\n",
    "             cleaned_tokens.append(token)\n",
    "\n",
    "    return cleaned_tokens\n",
    "\n",
    "def preprocess_for_lda_nmf(text, stopwords_set):\n",
    "    cleaned_text = initial_clean(text)\n",
    "    lemmatized_tokens = tokenize_lemmatize_lowercase(cleaned_text)\n",
    "    final_tokens = remove_stopwords_topic_model(lemmatized_tokens, stopwords_set)\n",
    "    return final_tokens\n",
    "\n",
    "# --- Add Topic Labels Dictionary ---\n",
    "topic_labels = {\n",
    "    1: \"Tesla & Electric Vehicles\",\n",
    "    2: \"Earnings Estimates & Surprises\",\n",
    "    3: \"Elon Musk & Twitter\",\n",
    "    4: \"Quarterly Financial Results\",\n",
    "    5: \"Banks, Interest Rates & Inflation\",\n",
    "    6: \"Stock Market Movements\",\n",
    "    7: \"Corporate Announcements\",\n",
    "    8: \"Yahoo Finance & Earnings Coverage\",\n",
    "    9: \"Big Tech & AI\",\n",
    "    10: \"Healthcare & Pharmaceuticals\",\n",
    "    11: \"Market Indices & Economic Data\",\n",
    "    12: \"Earnings Calls & Executive Commentary\",\n",
    "    13: \"Retail & Consumer Spending\",\n",
    "    14: \"Analyst Insights & Investment Ideas\",\n",
    "    15: \"Media & Streaming\",\n",
    "    16: \"Auto Industry & Labor Strikes\",\n",
    "    17: \"Costco & Wholesale Retail\",\n",
    "    18: \"Insider Trading & Share Activity\",\n",
    "    19: \"Aerospace & Aviation\",\n",
    "    20: \"E-commerce & Amazon\",\n",
    "    21: \"Social Media & Advertising\",\n",
    "    22: \"Dividends & Energy Stocks\",\n",
    "    23: \"Industrial Tech & Conglomerates\",\n",
    "    24: \"ETFs & Asset Management\",\n",
    "    25: \"AI & Semiconductors\"\n",
    "}\n",
    "# --- End of Topic Labels ---\n",
    "\n",
    "# --- 1. Define the new text and preprocess it ---\n",
    "title = \"Nvidia and Other Chip Stocks Fall as Tariff-Fueled Rout Continues\"\n",
    "content = \"Spencer Platt / Getty Images Traders work the floor of the New York Stock Exchange on Friday morning\\n\\nThe chips are down.\\n\\nChip stocks fell Friday, losing ground on the second trading day after President Donald Trump unveiled a set of global tariffs that sent stock markets dramatically lower—and the first since China announced its own retaliatory measures. The latest news offered investors yet another indication that trade war is here to stay, at least for now.\\n\\nThe PHLX Semiconductor Index (SOX), which tracks chip shares, was recently down about 7%. Four of its components, namely Marvell Technology (MRVL), Coherent (COHR), Entegris (ENTG) and Micron Technology (MU), were recently off more than 7% apiece.\\n\\n“Because many finished electronics goods and IT infrastructure goods are ultimately imported from many of [the countries affected by the new tariffs],” UBS analysts wrote Thursday, “this could have a profound negative impact on electronics demand.”\\n\\nNvidia (NVDA), one of last year’s stock-market darlings, was off more than 7% in recent trading.\\n\\nCiti analysts yesterday suggested that analog chipmakers like Analog Devices (ADI) could outperform the broader sector in a downturn; its shares were off some 6% in recent trading.\\n\\nThis week’s tariff announcements have hit stocks broadly. Today, all three major indexes were recently down roughly 4%, with the blue-chip Dow faring only a bit better. All 11 of the S&P 500’s sectors were recently in decline; its semiconductor and equipment subindex was down about 7%.\\n\\nChina’s new tariffs on US imports are set to take effect Thursday, according to that country’s finance ministry.\\n\\n\\n\\nRead the original article on Investopedia\"\n",
    "\n",
    "combined = f\"{title} {content}\"\n",
    "cleaned = preprocess_for_lda_nmf(combined, default_stopwords)\n",
    "print(cleaned)\n",
    "\n",
    "project_root = '/content/drive/MyDrive/IS450 Project'\n",
    "# --- 2. Load the saved models and dictionary ---\n",
    "model_path = os.path.join(project_root, \"outputs\", \"topic_modeling\", \"nmf_news\", 'gensim_nmf_tfidf.model')\n",
    "dict_path = os.path.join(project_root, \"outputs\", \"topic_modeling\", \"nmf_news\", 'gensim_dictionary.dict')\n",
    "tfidf_model_path = os.path.join(project_root, \"outputs\", \"topic_modeling\", \"nmf_news\", 'gensim_tfidf.model')\n",
    "\n",
    "print(f\"Loading NMF model from: {model_path}\")\n",
    "print(f\"Loading Dictionary from: {dict_path}\")\n",
    "print(f\"Loading TF-IDF model from: {tfidf_model_path}\")\n",
    "\n",
    "try:\n",
    "    loaded_nmf_model = Nmf.load(model_path)\n",
    "    loaded_dictionary = Dictionary.load(dict_path)\n",
    "    loaded_tfidf_model = TfidfModel.load(tfidf_model_path)\n",
    "    models_loaded = True\n",
    "    print(\"Models and dictionary loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"ERROR: One or more model/dictionary files not found.\")\n",
    "    models_loaded = False\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: An unexpected error occurred loading models: {e}\")\n",
    "    models_loaded = False\n",
    "\n",
    "# --- 3. Prepare the new text for the model ---\n",
    "if models_loaded:\n",
    "    # Convert the cleaned text to Bag-of-Words using the loaded dictionary\n",
    "    new_text_bow = loaded_dictionary.doc2bow(cleaned)\n",
    "\n",
    "    # Convert the BoW representation to TF-IDF using the loaded TF-IDF model\n",
    "    new_text_tfidf = loaded_tfidf_model[new_text_bow]\n",
    "\n",
    "    # --- 4. Apply the NMF model ---\n",
    "    topic_distribution = loaded_nmf_model[new_text_tfidf]\n",
    "\n",
    "    # --- 5. Extract and display the top topics ---\n",
    "    if topic_distribution: # Check if the model returned any topics\n",
    "        # Sort topics by probability (descending)\n",
    "        sorted_topics = sorted(topic_distribution, key=lambda item: item[1], reverse=True)\n",
    "\n",
    "        print(\"\\nPredicted Topic Distribution:\")\n",
    "        print(\"(Note: 'Probability' here refers to the NMF topic weight/contribution for this document)\")\n",
    "        for topic_idx, prob in sorted_topics:\n",
    "            # Convert 0-based index to 1-based for display\n",
    "            topic_num = topic_idx + 1\n",
    "            # --- Use topic labels dictionary ---\n",
    "            label = topic_labels.get(topic_num, \"Unknown Topic\") # Get label, default if not found\n",
    "            print(f\"  Topic: {label}: Probability = {prob:.4f}\")\n",
    "            # --- End of label use ---\n",
    "\n",
    "        # --- Display the top 2 topics ---\n",
    "        if len(sorted_topics) >= 1:\n",
    "            top_topic_idx, top_prob = sorted_topics[0]\n",
    "            top_topic_num = top_topic_idx + 1\n",
    "            # --- Use topic labels dictionary ---\n",
    "            top_label = topic_labels.get(top_topic_num, \"Unknown Topic\")\n",
    "            print(f\"\\nTop Topic: {top_label} (Weight/Probability: {top_prob:.4f})\")\n",
    "\n",
    "    else:\n",
    "        print(\"The NMF model did not assign any topics to this document (possibly due to all words being filtered out).\")\n",
    "\n",
    "else:\n",
    "    print(\"Skipping prediction because models could not be loaded.\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
