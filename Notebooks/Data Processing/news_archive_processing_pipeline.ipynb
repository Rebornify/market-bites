{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26411,
     "status": "ok",
     "timestamp": 1741783164303,
     "user": {
      "displayName": "Faith Ang",
      "userId": "09109116417858780852"
     },
     "user_tz": -480
    },
    "id": "FiYja6MBPeBq",
    "outputId": "15b0e7d9-9e5f-4d8c-d7ce-d21780bf57a8"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9257,
     "status": "ok",
     "timestamp": 1741783920339,
     "user": {
      "displayName": "Faith Ang",
      "userId": "09109116417858780852"
     },
     "user_tz": -480
    },
    "id": "ZTpJcVCvKXj1",
    "outputId": "1e8d66a7-e45d-4e15-a15e-a19b9aaee66c"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"hf://datasets/Shadow-Blade/financialNews/financialNews.csv\")\n",
    "\n",
    "# Define the output file path\n",
    "output_path = \"/content/drive/My Drive/IS450 Project/Data/Historical News/Raw/raw_financial_news.csv\"\n",
    "\n",
    "# Save as CSV\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Data saved at: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 221,
     "status": "ok",
     "timestamp": 1741789275039,
     "user": {
      "displayName": "Faith Ang",
      "userId": "09109116417858780852"
     },
     "user_tz": -480
    },
    "id": "urHGQfL-uxOr",
    "outputId": "d5878ce3-ec4f-45a0-fa14-d75a9355f2c8"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import html\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt_tab')\n",
    "except LookupError:\n",
    "    print(\"Downloading NLTK resource: punkt_tab\")\n",
    "    nltk.download('punkt_tab')\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    print(\"Downloading NLTK resource: stopwords\")\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "print(\"Imports complete and NLTK resources checked/downloaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the root path of your project on Google Drive\n",
    "project_root = '/content/drive/MyDrive/IS450 Project'\n",
    "\n",
    "# Define paths relative to the project root\n",
    "input_xlsx = os.path.join(project_root, 'Data/Historical News/Supplementary/unique_financial_news.xlsx')\n",
    "base_output_path = os.path.join(project_root, 'Data/Historical News')\n",
    "finbert_output_dir = os.path.join(base_output_path, \"FinBERT_Data\")\n",
    "ml_output_dir = os.path.join(base_output_path, \"ML_Data\")\n",
    "\n",
    "os.makedirs(finbert_output_dir, exist_ok=True)\n",
    "os.makedirs(ml_output_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Input file: {input_xlsx}\")\n",
    "print(f\"FinBERT Output directory: {finbert_output_dir}\")\n",
    "print(f\"ML Output directory: {ml_output_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_stopwords = set(stopwords.words('english'))\n",
    "stopwords_to_keep = {\"not\", \"no\", \"but\"} # Keep negation words\n",
    "custom_stopwords = default_stopwords - stopwords_to_keep\n",
    "\n",
    "print(f\"Custom stopwords set created. Total: {len(custom_stopwords)}\")\n",
    "# print(custom_stopwords) # Optional: Uncomment to verify\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_news_text(text):\n",
    "    \"\"\"\n",
    "    Clean news text by unescaping HTML, removing URLs/tags, and normalizing whitespace.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = html.unescape(text)\n",
    "    text = html.unescape(text)\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    text = re.sub(r'http\\\\S+|www\\\\.\\\\S+', '', text)\n",
    "    text = re.sub(r'\\\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def detect_truncated_description(description):\n",
    "    \"\"\"\n",
    "    Detect if the description is likely truncated using heuristics.\n",
    "    \"\"\"\n",
    "    if not isinstance(description, str) or len(description.strip()) == 0:\n",
    "        return False\n",
    "    description = description.strip()\n",
    "    if description.endswith(\"...\") or description.endswith(\"…\"):\n",
    "        return True\n",
    "    if not description or description[-1] not in {'.', '!', '?'}:\n",
    "        return True\n",
    "    if description:\n",
    "        try:\n",
    "            last_word = description.split()[-1]\n",
    "            last_word_cleaned = re.sub(r'[.!?]$', '', last_word)\n",
    "            if len(last_word_cleaned) < 3:\n",
    "                return True\n",
    "        except IndexError:\n",
    "             return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def detect_truncated_title(title):\n",
    "    \"\"\"\n",
    "    Check if the title appears truncated (checks for ellipsis).\n",
    "    \"\"\"\n",
    "    if not isinstance(title, str) or not title.strip():\n",
    "        return False\n",
    "    title_clean = title.strip()\n",
    "    if title_clean.endswith(\"...\") or title_clean.endswith(\"…\"):\n",
    "        return True\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_for_finbert_news(title, description):\n",
    "    \"\"\"\n",
    "    Combine title/description and apply basic cleaning for FinBERT.\n",
    "    \"\"\"\n",
    "    title_str = str(title) if title is not None else \"\"\n",
    "    desc_str = str(description) if description is not None else \"\"\n",
    "    combined = f\"{title_str} {desc_str}\".strip()\n",
    "    return clean_news_text(combined)\n",
    "\n",
    "\n",
    "def preprocess_for_traditional_ml_news(title, description):\n",
    "    \"\"\"\n",
    "    Combine title/description and clean aggressively for traditional ML.\n",
    "    \"\"\"\n",
    "    title_str = str(title) if title is not None else \"\"\n",
    "    desc_str = str(description) if description is not None else \"\"\n",
    "    combined = f\"{title_str} {desc_str}\".strip()\n",
    "    cleaned = clean_news_text(combined)\n",
    "    cleaned = cleaned.lower()\n",
    "    cleaned = re.sub(r'[/]', ' ', cleaned)\n",
    "    cleaned = re.sub(r'[^a-z0-9_\\\\s]', '', cleaned)\n",
    "    cleaned = re.sub(r'\\\\s+', ' ', cleaned).strip()\n",
    "    return cleaned\n",
    "\n",
    "def tokenize_and_remove_stopwords(text, stopwords_set):\n",
    "    \"\"\"Tokenizes text and removes stopwords.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [token for token in tokens if token not in stopwords_set and len(token) > 1]\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_news():\n",
    "    \"\"\"\n",
    "    Loads news data from Excel, processes it for FinBERT and ML pipelines,\n",
    "    and saves the results to separate JSON files.\n",
    "    \"\"\"\n",
    "    print(f\"Starting processing for: {input_xlsx}\")\n",
    "    try:\n",
    "        df = pd.read_excel(input_xlsx)\n",
    "        print(f\"Loaded {len(df)} rows from Excel.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERROR: Input file not found at {input_xlsx}\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Failed to load Excel file: {e}\")\n",
    "        return\n",
    "\n",
    "    ml_output_data = []\n",
    "    finbert_output_data = []\n",
    "    processed_count = 0\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        try:\n",
    "            record_id = int(idx) # Ensure ID is integer\n",
    "            title = row.get(\"title\", \"\")\n",
    "            description = row.get(\"description\", \"\")\n",
    "\n",
    "            finbert_text = preprocess_for_finbert_news(title, description)\n",
    "            finbert_output_data.append({\n",
    "                \"id\": record_id,\n",
    "                \"processed_text_finbert\": finbert_text\n",
    "            })\n",
    "\n",
    "            ml_text = preprocess_for_traditional_ml_news(title, description)\n",
    "            ml_tokens = tokenize_and_remove_stopwords(ml_text, custom_stopwords)\n",
    "            ml_output_data.append({\n",
    "                \"id\": record_id,\n",
    "                \"processed_tokens_ml\": ml_tokens\n",
    "            })\n",
    "\n",
    "            processed_count += 1\n",
    "            if processed_count % 10000 == 0: # Progress indicator every 10k records\n",
    "                 print(f\"Processed {processed_count} records...\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR processing row index {idx}: {e}\")\n",
    "            continue # Skip problematic rows\n",
    "\n",
    "    print(f\"Finished processing {processed_count} records.\")\n",
    "\n",
    "    finbert_output_file = os.path.join(finbert_output_dir, \"news_finbert.json\")\n",
    "    ml_output_file = os.path.join(ml_output_dir, \"news_ml.json\")\n",
    "\n",
    "    try:\n",
    "        with open(finbert_output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(finbert_output_data, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"Saved {len(finbert_output_data)} FinBERT records to {finbert_output_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR saving FinBERT data: {e}\")\n",
    "\n",
    "    try:\n",
    "        with open(ml_output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(ml_output_data, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"Saved {len(ml_output_data)} ML records to {ml_output_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR saving ML data: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_news()\n",
    "print(\"\\nProcessing attempt complete.\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
